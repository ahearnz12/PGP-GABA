{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmKLmjqmHMQM"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=10>Generative AI for Business Applications</center></font>\n",
        "<center><font size=6>Fine-Tunning LLMs - Week 1</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azxWym9-HMpz"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"\" width=720></a>\n",
        "<center><font size=6>Fine-Tuned AI for Summarizing Insurance Sales Conversations</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa1yXUesL7Zy"
      },
      "source": [
        "# Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFN8LrodMR7w"
      },
      "source": [
        "## Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTLRzlos0ney"
      },
      "source": [
        "An enterprise sales representative at a global insurance provider is preparing for a crucial renewal meeting with one of the largest clients. Over the past year, numerous emails have been exchanged, several calls conducted, and in-person meetings held. However, this valuable context is fragmented across the inbox, CRM records, and call notes.\n",
        "\n",
        "With limited time and growing pressure to personalize service and identify cross-sell opportunities, it is difficult to recall key details, such as the products the client was interested in, concerns raised in the last quarter, and commitments made during previous meetings.\n",
        "\n",
        "This challenge reflects a broader industry problem where client interactions are rich but scattered. Sales teams often face:\n",
        "\n",
        "* **Overload of unstructured data** from emails, calls, and notes.\n",
        "* **Lack of standardized, accurate summaries** to capture client context.\n",
        "* **Manual, error-prone preparation** that consumes significant time.\n",
        "* **Missed upsell and personalization opportunities**, weakening client trust.\n",
        "\n",
        "As a result, client engagement is inconsistent, preparation is inefficient, and revenue opportunities are lost.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDG_248npouB"
      },
      "source": [
        "##  Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gppGd2sN0s8A"
      },
      "source": [
        "The objective is to introduce a **smart assistant** capable of synthesizing multi-modal client interactions and generating precise, context-aware summaries.\n",
        "\n",
        "Such a solution would:\n",
        "\n",
        "* Consolidate insights from emails, CRM logs, call transcripts, and meeting notes.\n",
        "* Deliver concise, tailored client briefs before every touchpoint.\n",
        "* Help sales teams maintain continuity, honor past commitments, and personalize conversations.\n",
        "* Unlock new revenue by surfacing upsell and cross-sell opportunities at the right moment.\n",
        "\n",
        "By reducing preparation time and improving personalization, this assistant can transform client engagement in the insurance sector, strengthen relationships, and drive sustainable growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGxmR0XVwcAi"
      },
      "source": [
        "## Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpQ6dfJa1Iwg"
      },
      "source": [
        "The dataset consists of two primary columns:\n",
        "\n",
        "Conversation - Contains the raw transcripts of client-sales representative interactions, which are often lengthy, multi-turn, and unstructured.\n",
        "\n",
        "Summary - Provides the corresponding concise, structured summaries of key discussion points, client interests, concerns, and commitments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGUDO5qRb4oB"
      },
      "source": [
        "# **Solution Approach**\n",
        "Provide a Custom Fine-Tuned AI Model for Sales Interaction Summarization\n",
        "\n",
        "To address this challenge, we propose training a domain-specific fine-tuned language model tailored for enterprise insurance communication.\n",
        "The model will:\n",
        "\n",
        "1. Ingest few multi-modal inputs (emails, transcripts, notes).\n",
        "2. Identify intent, extract key discussion points, client interests, pain points, and commitments.\n",
        "3. Generate concise, actionable summaries under 200 words, customized for enterprise insurance workflows.\n",
        "4. Be fine-tuned on real-world communication data to learn domain-specific vocabulary and interaction patterns.\n",
        "\n",
        "This AI-powered tool will augment sales productivity, enhance client engagement, and ensure consistent follow-ups, turning scattered conversations into strategic intelligence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnwETBOE6Bz5"
      },
      "source": [
        "# **Installing and Importing Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfffBpmv6ciV",
        "outputId": "d904bb1a-f329-4f31-fe27-87bd3e796772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: peft in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (0.17.1)\n",
            "Requirement already satisfied: trl==0.15.2 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (0.15.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement triton (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for triton\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: sentencepiece in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (0.2.1)\n",
            "Requirement already satisfied: protobuf in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (6.32.1)\n",
            "Requirement already satisfied: huggingface_hub in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (0.34.4)\n",
            "Requirement already satisfied: hf_transfer in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface_hub) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface_hub) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from requests->huggingface_hub) (2025.8.3)\n",
            "Requirement already satisfied: transformers==4.51.3 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from transformers==4.51.3) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from transformers==4.51.3) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from transformers==4.51.3) (2.3.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from transformers==4.51.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from transformers==4.51.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from transformers==4.51.3) (2025.9.1)\n",
            "Requirement already satisfied: requests in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from transformers==4.51.3) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from transformers==4.51.3) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from transformers==4.51.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from transformers==4.51.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from requests->transformers==4.51.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from requests->transformers==4.51.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from requests->transformers==4.51.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from requests->transformers==4.51.3) (2025.8.3)\n",
            "Requirement already satisfied: unsloth in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (2025.9.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.32.post2 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
        "!pip install transformers==4.51.3\n",
        "!pip install --no-deps unsloth\n",
        "\n",
        "!pip install -q datasets evaluate bert-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDp-EYZH-69E"
      },
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tgtsNS-UrQg9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (1.107.2)\n",
            "Requirement already satisfied: pandas in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (2.3.2)\n",
            "Requirement already satisfied: datasets in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (4.0.0)\n",
            "Requirement already satisfied: evaluate in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (0.4.5)\n",
            "Requirement already satisfied: tqdm in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from pandas) (2.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from datasets) (21.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from datasets) (2.32.5)\n",
            "Requirement already satisfied: xxhash in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.9)\n",
            "Requirement already satisfied: six>=1.5 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Available models in LM Studio:\n",
            "- gpt-oss-20b\n",
            "- text-embedding-nomic-embed-text-v1.5\n",
            "- openai/gpt-oss-20b\n",
            "\n",
            "Using model: gpt-oss-20b\n"
          ]
        }
      ],
      "source": [
        "# Install OpenAI client for connecting to LM Studio\n",
        "%pip install openai pandas datasets evaluate tqdm\n",
        "\n",
        "import pandas as pd                            # Data manipulation and analysis library (tabular data handling).\n",
        "from datasets import Dataset                   # Hugging Face library for creating and managing ML datasets.\n",
        "import evaluate                                # Hugging Face library for evaluating NLP models with standard metrics.\n",
        "from tqdm import tqdm                          # Progress bar utility for tracking loops and training progress.\n",
        "from openai import OpenAI                      # OpenAI client for API communication\n",
        "\n",
        "# Configure OpenAI client to connect to LM Studio running locally\n",
        "# LM Studio typically runs on localhost:1234 with OpenAI-compatible API\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:1234/v1\",\n",
        "    api_key=\"lm-studio\"  # LM Studio doesn't require a real API key\n",
        ")\n",
        "\n",
        "# Test connection to LM Studio\n",
        "try:\n",
        "    models = client.models.list()\n",
        "    print(\"Available models in LM Studio:\")\n",
        "    for model in models.data:\n",
        "        print(f\"- {model.id}\")\n",
        "    \n",
        "    # Use GPT-OSS 20b model (adjust model name as needed based on what's loaded in LM Studio)\n",
        "    model_name = \"gpt-oss-20b\"  # This should match the model loaded in LM Studio\n",
        "    print(f\"\\nUsing model: {model_name}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to LM Studio: {e}\")\n",
        "    print(\"Make sure LM Studio is running on localhost:1234 with GPT-OSS 20b model loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhyMoqjoq48A"
      },
      "source": [
        "# **1. Evaluation of LLM before FineTuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp4LNGG30e8w"
      },
      "source": [
        "### Loading the Testing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Dat_l5MlrLkX"
      },
      "outputs": [],
      "source": [
        "# Read the testing CSV into a Pandas DataFrame\n",
        "testing_data = pd.read_csv(\"../data/finetuning_testing.csv\")\n",
        "\n",
        "# Extract all dialogues into a list for model input\n",
        "test_dialogues = [sample for sample in testing_data['Dialogues']]\n",
        "\n",
        "# Extract all human-written summaries into a list for evaluation\n",
        "test_summaries = [sample for sample in testing_data['Summary']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dialogue 1:\n",
            "User: Were reassessing our policies after expanding to three new regional offices. Do your plans support coverage across multiple states?\n",
            "Sales Representative: Yes, we offer multi-state coverage with unified billing and compliance alignment for all locations.\n",
            "User: Thats good to know. Do regional variances affect the plan design or premium structure?\n",
            "Sales Representative: Slightly premiums can vary based on state regulations and provider networks, but we aim to keep core benefits consistent.\n",
            "User: How do you manage compliance across state lines?\n",
            "Sales Representative: We have a regulatory team that monitors each jurisdiction and updates plans to remain compliant automatically.\n",
            "User: Can I review an example of a client with a similar multi-state setup?\n",
            "Sales Representative: Absolutely Ill share a case file and our compliance checklist within 48 hours.\n",
            "\n",
            "Human Summary 1:\n",
            "Client expanding into new regions needs multi-state insurance solutions. Action: Share compliance checklist and example client implementation.\n",
            "\n",
            "--------------------------------------------------\n",
            "Dialogue 2:\n",
            "User: We recently experienced a ransomware scare. What cyber insurance options can protect against data breaches and operational downtime?\n",
            "Sales Representative: We offer cyber liability plans that cover forensic investigations, data restoration, and crisis PR costs.\n",
            "User: Does this integrate with our current liability coverage?\n",
            "Sales Representative: Yes, it complements general liability and can be added as an endorsement or stand-alone.\n",
            "User: Whats the typical payout structure for such incidents?\n",
            "Sales Representative: Its usually based on actual losses and includes business interruption coverage, often with sub-limits for forensics and legal counsel.\n",
            "User: Send me a sample policy and incident response playbook.\n",
            "Sales Representative: Will do. Expect both in your inbox by tomorrow morning.\n",
            "\n",
            "Human Summary 2:\n",
            "Client exploring cyber insurance after security incident. Action: Deliver sample policy and incident response playbook.\n",
            "\n",
            "--------------------------------------------------\n",
            "Dialogue 3:\n",
            "User: Were looking to include sustainability-linked incentives in our employee benefit plans. Is this something you support?\n",
            "Sales Representative: Yes! Weve piloted green wellness credits that reward employees for biking to work or using fitness trackers.\n",
            "User: How is this tracked and verified?\n",
            "Sales Representative: Employees can sync data via our wellness app, which awards points for verified sustainable activities.\n",
            "User: Can these incentives affect insurance premiums?\n",
            "Sales Representative: They can. Over time, organizations with high wellness engagement often qualify for group rate adjustments.\n",
            "User: Excellent send over implementation details and usage analytics.\n",
            "Sales Representative: Ill compile a rollout guide and success metrics from our latest pilot program.\n",
            "\n",
            "Human Summary 3:\n",
            "Client interested in sustainability-linked employee incentives. Action: Share rollout guide and usage analytics from pilot.\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# print first 3 samples to verify\n",
        "for i in range(3):\n",
        "    print(f\"Dialogue {i+1}:\\n{test_dialogues[i]}\\n\")\n",
        "    print(f\"Human Summary {i+1}:\\n{test_summaries[i]}\\n\")\n",
        "    print(\"-\" * 50) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHaWozqI0hmW"
      },
      "source": [
        "### Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZWyFbFXsn_D",
        "outputId": "aad2ff42-4bcc-4ad4-fd68-250a44388b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPT-OSS 20b model hosted locally via LM Studio\n",
            "Model is accessed through OpenAI-compatible API on localhost:1234\n",
            "âœ… Model test successful: Hi there! Iâ€™m fully online and ready to help. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# GPT-OSS 20b is now accessed via LM Studio API - no local model loading needed\n",
        "print(\"Using GPT-OSS 20b model hosted locally via LM Studio\")\n",
        "print(\"Model is accessed through OpenAI-compatible API on localhost:1234\")\n",
        "\n",
        "# Test the model with a simple prompt to verify it's working\n",
        "def test_model_connection():\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Hello! Can you confirm you're working?\"}\n",
        "            ],\n",
        "            max_tokens=4096,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        print(f\"âœ… Model test successful: {response.choices[0].message.content}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Model test failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Test the connection\n",
        "model_ready = test_model_connection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3VYp0p7C794",
        "outputId": "d0026067-05ac-48f5-e6db-002f298a1dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-OSS 20b model is ready for inference via LM Studio API\n"
          ]
        }
      ],
      "source": [
        "# Model is ready for inference via API calls\n",
        "print(\"GPT-OSS 20b model is ready for inference via LM Studio API\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9sW9YYntYcs"
      },
      "source": [
        "### Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U6zN9zd5YAs"
      },
      "source": [
        "The Alpaca instruction prompt is a general purpose prompt template that can be adapted to any task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PHaAlmvZfTEp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt template integrated into API inference logic\n",
            "Using system messages and user prompts optimized for GPT-OSS 20b\n"
          ]
        }
      ],
      "source": [
        "# For GPT-OSS 20b via API, we use a simpler prompt structure\n",
        "# The system message and user prompt are defined directly in the inference loop\n",
        "print(\"Prompt template integrated into API inference logic\")\n",
        "print(\"Using system messages and user prompts optimized for GPT-OSS 20b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qp9Dtqvdtd6C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions list initialized in the inference cell below\n"
          ]
        }
      ],
      "source": [
        "# Initialize list to store model predictions (moved to inference cell for GPT-OSS 20b)\n",
        "print(\"Predictions list initialized in the inference cell below\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” DIAGNOSTIC CHECKS\n",
            "==================================================\n",
            "âœ… LM Studio connection successful\n",
            "ðŸ“Š Available models (3):\n",
            "   1. gpt-oss-20b\n",
            "   2. text-embedding-nomic-embed-text-v1.5\n",
            "   3. openai/gpt-oss-20b\n",
            "âœ… GPT-OSS 20b model found\n",
            "\n",
            "ðŸ“‹ TEST DATA VALIDATION:\n",
            "   - Number of test dialogues: 10\n",
            "   - Number of test summaries: 10\n",
            "   - Average dialogue length: 685 chars\n",
            "   - Min dialogue length: 584 chars\n",
            "   - Max dialogue length: 861 chars\n",
            "\n",
            "ðŸ“ SAMPLE DIALOGUE (first 300 chars):\n",
            "   'User: Were reassessing our policies after expanding to three new regional offices. Do your plans support coverage across multiple states?\\nSales Representative: Yes, we offer multi-state coverage with unified billing and compliance alignment for all locations.\\nUser: Thats good to know. Do regional va'...\n",
            "\n",
            "ðŸ§ª QUICK API TEST with gpt-oss-20b:\n",
            "âœ… API test result: \n",
            "ðŸ“Š Response details:\n",
            "   - Finish reason: length\n",
            "   - Tokens used: 142\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Diagnostic cell to troubleshoot LM Studio connection and model issues\n",
        "print(\"ðŸ” DIAGNOSTIC CHECKS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check 1: LM Studio connection\n",
        "try:\n",
        "    models = client.models.list()\n",
        "    print(\"âœ… LM Studio connection successful\")\n",
        "    print(f\"ðŸ“Š Available models ({len(models.data)}):\")\n",
        "    for i, model in enumerate(models.data, 1):\n",
        "        print(f\"   {i}. {model.id}\")\n",
        "    \n",
        "    # Check if our target model exists\n",
        "    model_names = [m.id for m in models.data]\n",
        "    if \"gpt-oss-20b\" in model_names:\n",
        "        print(\"âœ… GPT-OSS 20b model found\")\n",
        "        model_name = \"gpt-oss-20b\"\n",
        "    else:\n",
        "        print(\"âš ï¸  GPT-OSS 20b not found, will use first available model\")\n",
        "        model_name = model_names[0] if model_names else None\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âŒ LM Studio connection failed: {e}\")\n",
        "    print(\"\\nðŸ”§ TROUBLESHOOTING STEPS:\")\n",
        "    print(\"1. Ensure LM Studio is running\")\n",
        "    print(\"2. Check that LM Studio is using port 1234\")\n",
        "    print(\"3. Verify a model is loaded in LM Studio\")\n",
        "    print(\"4. Try restarting LM Studio\")\n",
        "\n",
        "# Check 2: Test data validation\n",
        "print(f\"\\nðŸ“‹ TEST DATA VALIDATION:\")\n",
        "print(f\"   - Number of test dialogues: {len(test_dialogues)}\")\n",
        "print(f\"   - Number of test summaries: {len(test_summaries)}\")\n",
        "\n",
        "if len(test_dialogues) > 0:\n",
        "    dialogue_lengths = [len(d) for d in test_dialogues]\n",
        "    print(f\"   - Average dialogue length: {sum(dialogue_lengths)/len(dialogue_lengths):.0f} chars\")\n",
        "    print(f\"   - Min dialogue length: {min(dialogue_lengths)} chars\")\n",
        "    print(f\"   - Max dialogue length: {max(dialogue_lengths)} chars\")\n",
        "    \n",
        "    # Show a sample dialogue (truncated)\n",
        "    sample = test_dialogues[0]\n",
        "    print(f\"\\nðŸ“ SAMPLE DIALOGUE (first 300 chars):\")\n",
        "    print(f\"   {repr(sample[:300])}...\")\n",
        "\n",
        "# Check 3: Quick API test\n",
        "if 'model_name' in locals() and model_name:\n",
        "    print(f\"\\nðŸ§ª QUICK API TEST with {model_name}:\")\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Say 'API test successful' and count to 3.\"}\n",
        "            ],\n",
        "            max_tokens=50,\n",
        "            temperature=0.5\n",
        "        )\n",
        "        result = response.choices[0].message.content\n",
        "        print(f\"âœ… API test result: {result}\")\n",
        "        \n",
        "        # Check response details\n",
        "        print(f\"ðŸ“Š Response details:\")\n",
        "        print(f\"   - Finish reason: {response.choices[0].finish_reason}\")\n",
        "        print(f\"   - Tokens used: {response.usage.total_tokens if hasattr(response, 'usage') else 'N/A'}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ API test failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEUx94TogsMJ"
      },
      "source": [
        "We are generating summaries for each dialogue in our test set using the fine-tuned model.\n",
        "\n",
        "**Step-by-step Approach:**\n",
        "\n",
        "1. **Iterate through test dialogues** - `for dialogue in tqdm(test_dialogues):`\n",
        "\n",
        "   * Loops through each test dialogue while showing a progress bar (`tqdm`).\n",
        "\n",
        "2. **Format the prompt**\n",
        "\n",
        "   * Inserts the dialogue into the summarization template.\n",
        "\n",
        "3. **Tokenize input**\n",
        "\n",
        "   * Converts the text prompt into tokens (numbers) and moves them to the GPU (`.to(\"cuda\")`).\n",
        "\n",
        "4. **Generate output**\n",
        "\n",
        "   * The model predicts the summary using `.generate()`.\n",
        "   * `max_new_tokens=128`: limits summary length.\n",
        "   * `temperature=0`: makes output deterministic (no randomness).\n",
        "   * `pad_token_id`: ensures proper padding using EOS token.\n",
        "\n",
        "5. **Decode output**\n",
        "\n",
        "   * Converts model tokens back into human-readable text.\n",
        "   * Skips special tokens and cleans formatting.\n",
        "\n",
        "6. **Store prediction**\n",
        "\n",
        "   * Appends the generated summary to `predicted_summaries`.\n",
        "\n",
        "7. **Error handling**\n",
        "\n",
        "   * If an error occurs, it prints the error and continues with the next dialogue instead of stopping.\n",
        "\n",
        "This loop **takes each dialogue -> feeds it to the model -> generates a summary -> saves it for evaluation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M96nqedgsn6Q",
        "outputId": "ad87c78c-8163-4588-d879-222a8e992a5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with first dialogue to debug issues...\n",
            "Available models:\n",
            "  - gpt-oss-20b\n",
            "  - text-embedding-nomic-embed-text-v1.5\n",
            "  - openai/gpt-oss-20b\n",
            "Input dialogue length: 861 characters\n",
            "First 200 chars: User: Were reassessing our policies after expanding to three new regional offices. Do your plans support coverage across multiple states?\n",
            "Sales Representative: Yes, we offer multi-state coverage with ...\n",
            "\n",
            "Test summary length: 761 characters\n",
            "Test summary:\n",
            "**Summary of Business Conversation (â‰ˆ140 words)**  \n",
            "\n",
            "- **Client Context:** User is reassessing insurance policies after opening three new regional offices and needs coverage that spans multiple states.  \n",
            "- **Key Concern:** Whether the current plans can support multiâ€‘state operations without significant redesign or premium spikes.  \n",
            "- **Sales Rep Response:** Confirms availability of *multiâ€‘state coverage* with unified billing and consistent core benefits across all locations.  \n",
            "- **Premium & Design Variations:** Slight stateâ€‘based premium differences due to regulations and provider networks; core benefits remain unchanged.  \n",
            "- **Compliance Management:** Dedicated regulatory team monitors each jurisdiction, automatically updating plans to stay compliant\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Processing all 10 dialogues...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:13<00:00, 13.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated 10 summaries using gpt-oss-20b\n",
            "Non-empty summaries: 10\n",
            "\n",
            "================================================================================\n",
            "SAMPLE GENERATED SUMMARIES:\n",
            "================================================================================\n",
            "\n",
            "Summary 1 (Length: 386 chars):\n",
            "--------------------------------------------------\n",
            "**Summary (â‰ˆ120â€¯words)**  \n",
            "\n",
            "- **Client Context & Need:** Userâ€™s company has opened three new regional offices and is reassessing policies to cover multiple states.  \n",
            "- **Coverage Capability:** Sales rep confirms multiâ€‘state coverage with unified billing and compliance alignment across all locations.  \n",
            "- **Premium Structure:** Premiums may vary slightly by state due to regulations and\n",
            "--------------------------------------------------\n",
            "\n",
            "Summary 2 (Length: 1033 chars):\n",
            "--------------------------------------------------\n",
            "**Summary of Conversation**\n",
            "\n",
            "- **Client Concern:** Recent ransomware incident; seeks protection against data breaches and operational downtime.\n",
            "- **Products Discussed:**\n",
            "  - Cyber liability plans covering forensic investigations, data restoration, crisis PR costs.\n",
            "  - Business interruption coverage with subâ€‘limits for forensics/legal counsel.\n",
            "- **Integration & Coverage Details:**\n",
            "  - Plans complement existing general liability; can be added as an endorsement or standâ€‘alone policy.\n",
            "  - Payout structure based on actual losses, including business interruption and specialized services.\n",
            "- **Client Request:** Sample policy document + incident response playbook.\n",
            "- **Sales Commitment:** Representative will email both documents by tomorrow morning.\n",
            "- **Next Steps for Sales Team:**\n",
            "  - Follow up midâ€‘morning to confirm receipt and answer any questions.\n",
            "  - Prepare a tailored proposal highlighting integration points with current liability coverage.\n",
            "  - Schedule a brief call to discuss policy limits, exclusions, and claim process.\n",
            "--------------------------------------------------\n",
            "\n",
            "Summary 3 (Length: 668 chars):\n",
            "--------------------------------------------------\n",
            "**Summary of Conversation**\n",
            "\n",
            "- **Client Need:** Incorporate sustainabilityâ€‘linked incentives into employee benefit plans and understand impact on insurance premiums.\n",
            "- **Solution Offered:** Green wellness credits that reward sustainable behaviors (e.g., biking to work, using fitness trackers).\n",
            "- **Tracking & Verification:** Employees sync activity data through the companyâ€™s wellness app; verified actions earn points automatically.\n",
            "- **Premium Impact:** High engagement can qualify organizations for favorable group rate adjustments with insurers.\n",
            "- **Next Steps:**\n",
            "  - Sales rep will send a detailed rollout guide and usage analytics from the latest pilot program.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize list to store model predictions\n",
        "predicted_summaries = []\n",
        "\n",
        "# First, let's test with a single dialogue to debug issues\n",
        "print(\"Testing with first dialogue to debug issues...\")\n",
        "\n",
        "try:\n",
        "    # Check available models first\n",
        "    available_models = client.models.list()\n",
        "    print(\"Available models:\")\n",
        "    for model in available_models.data:\n",
        "        print(f\"  - {model.id}\")\n",
        "    \n",
        "    # Use the first available model if our specified one doesn't exist\n",
        "    available_model_ids = [model.id for model in available_models.data]\n",
        "    if model_name not in available_model_ids:\n",
        "        if available_model_ids:\n",
        "            model_name = available_model_ids[0]\n",
        "            print(f\"Model 'gpt-oss-20b' not found. Using: {model_name}\")\n",
        "        else:\n",
        "            raise Exception(\"No models available in LM Studio\")\n",
        "    \n",
        "    # Test with first dialogue\n",
        "    test_dialogue = test_dialogues[0]\n",
        "    print(f\"Input dialogue length: {len(test_dialogue)} characters\")\n",
        "    print(f\"First 200 chars: {test_dialogue[:200]}...\")\n",
        "    \n",
        "    # Improved prompt structure\n",
        "    system_message = \"\"\"You are an expert business conversation summarizer. Create comprehensive, structured summaries that capture:\n",
        "- Key discussion points and decisions\n",
        "- Client needs and concerns  \n",
        "- Products/services discussed\n",
        "- Next steps and commitments\n",
        "- Important details for sales follow-up\n",
        "\n",
        "Keep summaries between 100-200 words and use bullet points for clarity.\"\"\"\n",
        "    \n",
        "    user_prompt = f\"\"\"Summarize this business conversation:\n",
        "\n",
        "{test_dialogue}\n",
        "\n",
        "Please provide a comprehensive summary:\"\"\"\n",
        "\n",
        "    # Test API call with improved parameters\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        max_tokens=300,        # Increased token limit\n",
        "        temperature=0.3,       # Slightly more creative\n",
        "        top_p=0.9,\n",
        "        frequency_penalty=0.1,\n",
        "        presence_penalty=0.1\n",
        "    )\n",
        "    \n",
        "    test_summary = response.choices[0].message.content.strip()\n",
        "    print(f\"\\nTest summary length: {len(test_summary)} characters\")\n",
        "    print(f\"Test summary:\\n{test_summary}\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Test failed: {e}\")\n",
        "    print(\"Please check:\")\n",
        "    print(\"1. LM Studio is running on localhost:1234\")\n",
        "    print(\"2. A model is loaded in LM Studio\") \n",
        "    print(\"3. The model name matches what's shown above\")\n",
        "\n",
        "# If test successful, proceed with all dialogues\n",
        "print(f\"\\nProcessing all {len(test_dialogues)} dialogues...\")\n",
        "\n",
        "# Loop through each dialogue with improved error handling\n",
        "for i, dialogue in enumerate(tqdm(test_dialogues)):\n",
        "    try:\n",
        "        # Enhanced system message for business conversations\n",
        "        system_message = \"\"\"You are an expert business conversation summarizer. Create comprehensive, structured summaries that capture:\n",
        "- Key discussion points and decisions\n",
        "- Client needs and concerns  \n",
        "- Products/services discussed\n",
        "- Next steps and commitments\n",
        "- Important details for sales follow-up\n",
        "\n",
        "Keep summaries between 100-200 words and use bullet points for clarity.\"\"\"\n",
        "        \n",
        "        user_prompt = f\"\"\"Summarize this business conversation:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Please provide a comprehensive summary:\"\"\"\n",
        "\n",
        "        # Generate summary with improved parameters\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            max_tokens=300,        # Increased from 128\n",
        "            temperature=0.3,       # Increased from 0 for more natural output\n",
        "            top_p=0.9,\n",
        "            frequency_penalty=0.1,  # Reduce repetition\n",
        "            presence_penalty=0.1    # Encourage diverse vocabulary\n",
        "        )\n",
        "        \n",
        "        # Extract and validate the generated summary\n",
        "        prediction = response.choices[0].message.content.strip()\n",
        "        \n",
        "        # Check for empty or very short summaries\n",
        "        if len(prediction) < 20:\n",
        "            print(f\"Warning: Very short summary for dialogue {i+1}: '{prediction}'\")\n",
        "        \n",
        "        # Store the generated summary\n",
        "        predicted_summaries.append(prediction)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing dialogue {i+1}: {e}\")\n",
        "        # Add empty string for failed cases to maintain alignment\n",
        "        predicted_summaries.append(\"\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nGenerated {len(predicted_summaries)} summaries using {model_name}\")\n",
        "print(f\"Non-empty summaries: {len([s for s in predicted_summaries if s.strip()])}\")\n",
        "\n",
        "# Print first 3 generated summaries with more details\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE GENERATED SUMMARIES:\")\n",
        "print(\"=\"*80)\n",
        "for i in range(min(3, len(predicted_summaries))):\n",
        "    summary = predicted_summaries[i]\n",
        "    print(f\"\\nSummary {i+1} (Length: {len(summary)} chars):\")\n",
        "    print(\"-\" * 50)\n",
        "    if summary.strip():\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(\"[EMPTY SUMMARY]\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC8DIfb80ULd"
      },
      "source": [
        "### Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEb0o6jPc1C5"
      },
      "source": [
        "Now we are evaluating our base model to check how well the generated summaries align with human-written summaries. For this, we are using BERTScore, which measures the semantic similarity between the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLHAna-S78Li"
      },
      "source": [
        "**BERTScore** is a metric for evaluating text generation tasks, including summarization, translation, and captioning. Unlike traditional metrics like ROUGE or BLEU that rely on exact word overlaps, BERTScore uses embeddings from a pre-trained BERT model to measure **semantic similarity** between the generated text (predictions) and the human-written text (references). This makes it more robust in capturing meaning, even when different words are used.\n",
        "\n",
        "* **Precision** - Measures how much of the content in the generated text is actually relevant to the reference. High precision means the model is not adding irrelevant or â€œextraâ€ information.\n",
        "\n",
        "* **Recall** - Measures how much of the important content from the reference is captured by the generated text. A high recall means the model covers most of the key points, even if it includes some extra details.\n",
        "\n",
        "* **F1 Score** - Combines both precision and recall into a balanced score. It demonstrates how well the generated text both covers the important content and remains relevant. This is usually reported as the main metric for BERTScore.\n",
        "\n",
        "In short, BERTScore helps evaluate not just word matching, but whether the **meaning** of the generated text aligns with the reference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GabjYgxsdw-i"
      },
      "source": [
        "We are proceeding with the F1-Score, as it provides a balanced measure of the overall semantic similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "L_RZs84SuIZY"
      },
      "outputs": [],
      "source": [
        "# Load the BERTScore evaluation metric from the Hugging Face 'evaluate' library\n",
        "bert_scorer = evaluate.load(\"bertscore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDt6VUebIzH2"
      },
      "source": [
        "Hyperparameters for `bert_scorer`\n",
        "\n",
        "* **`predictions`** - The summaries generated by our fine-tuned model.\n",
        "* **`references`** - The correct (gold-standard) summaries from the dataset.\n",
        "* **`lang`='en'** - Specifies the language as English.\n",
        "* **`rescale_with_baseline`=True** - Normalizes the scores so they are easier to interpret.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### âš ï¸ BERTScore Hanging Issue - Solutions\n",
        "\n",
        "**Common Issues with BERTScore:**\n",
        "1. **Model Download**: First-time use downloads large BERT models (can take 5-10 minutes)\n",
        "2. **Memory Problems**: `rescale_with_baseline=True` uses significant RAM/GPU memory\n",
        "3. **Device Conflicts**: GPU/CPU switching can cause hanging\n",
        "4. **Network Timeouts**: Model downloads may timeout on slow connections\n",
        "\n",
        "**Solutions Implemented:**\n",
        "- âœ… **Timeout Protection**: 60-second timeout to prevent infinite hanging\n",
        "- âœ… **Batch Processing**: Process fewer samples first, then scale up\n",
        "- âœ… **CPU Fallback**: Force CPU processing to avoid GPU memory issues\n",
        "- âœ… **Lighter Model**: Use DistilBERT instead of full BERT for speed\n",
        "- âœ… **Alternative Metrics**: Token-based F1 and coverage analysis as backups\n",
        "- âœ… **Emergency Mode**: Simple word overlap evaluation requiring no downloads\n",
        "\n",
        "**Recommendation**: Try the main evaluation cell first. If it hangs, use the emergency fallback cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "f483532192744e2c9a4eefb2567b02b6",
            "82ed1f8f01ba41d58e60732f502e2606",
            "703d0fe63c26440b9f251f760c9ef4ab",
            "f0af8c1d89704bd28d739a274869ad65",
            "03aedb3387ef4182a7eaab849fc21123",
            "ab6b2c906ff94da5a94201f409434574",
            "6424175902e045d1b30d3fa12ff73f32",
            "6fb638164b754e538e38492a9aa08204",
            "fc394f734d584b7d9315560b21640e5d",
            "5566162bc00d430b878336c23d24bdbf",
            "a547d8c63bb44f88a6522c7186b5e268",
            "553208bce9ea4ef69921389d09f807c5",
            "8afa630e5bc342e59807f0ea8c493f95",
            "80da8e51ccf64550ba5be2f58b28ac98",
            "c6a40368c70c427da29a8f56181ed5fa",
            "862b87382adf4c39b93e8f31e0ee6813",
            "a9925280086c420a89726da2b0a3b6d4",
            "e45a8ce6c93e4c069050076cfee78f09",
            "f329740b7b6441519c5dea729dd7d1ab",
            "22b6ba504e194e1183c963417f128677",
            "fe29dde9a1674024bd7a7fb46f1359f8",
            "fe5f10d9bd4d4db18c91be1612d7c4a8"
          ]
        },
        "id": "Ery2CVncuIW1",
        "outputId": "2bdb6709-f20f-4ff5-9bc5-8b7f7b570b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ EVALUATION METHODS - Multiple approaches to avoid hanging\n",
            "============================================================\n",
            "ðŸš€ Starting evaluation process...\n",
            "\n",
            "ðŸ“Š Attempting BERTScore evaluation (60 second timeout)...\n",
            "âœ… BERTScore (5 samples): F1 = 0.7680\n",
            "ðŸ“Š Extending to all samples...\n",
            "âœ… BERTScore (all samples): F1 = 0.7652\n",
            "ðŸ“Š Using simple text similarity metrics as fallback...\n",
            "âœ… Simple Token F1: 0.1367\n",
            "ðŸ“Š Analyzing summary coverage and quality...\n",
            "ðŸ“ˆ SUMMARY ANALYSIS:\n",
            "   - Predicted avg length: 111.0 words\n",
            "   - Reference avg length: 14.6 words\n",
            "   - Non-empty summaries: 10/10\n",
            "   - Average coverage: 0.4938\n",
            "\n",
            "============================================================\n",
            "ðŸ“‹ EVALUATION SUMMARY:\n",
            "============================================================\n",
            "ðŸŽ¯ BERTScore F1: 0.7652\n",
            "ðŸ“ Simple Token F1: 0.1367\n",
            "ðŸ“Š Coverage Score: 0.4938\n",
            "\n",
            "ðŸ’¡ Recommendation: Use BERTScore as primary metric\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Alternative evaluation approach with timeout and fallback options\n",
        "import signal\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def timeout_context(seconds):\n",
        "    \"\"\"Context manager for timing out operations\"\"\"\n",
        "    def timeout_handler(signum, frame):\n",
        "        raise TimeoutError(f\"Operation timed out after {seconds} seconds\")\n",
        "    \n",
        "    old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n",
        "    signal.alarm(seconds)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        signal.alarm(0)\n",
        "        signal.signal(signal.SIGALRM, old_handler)\n",
        "\n",
        "print(\"ðŸ”„ EVALUATION METHODS - Multiple approaches to avoid hanging\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Approach 1: Try BERTScore with timeout and optimizations\n",
        "def evaluate_with_bertscore():\n",
        "    try:\n",
        "        print(\"ðŸ“Š Attempting BERTScore evaluation (60 second timeout)...\")\n",
        "        \n",
        "        # Use timeout to prevent hanging\n",
        "        with timeout_context(60):\n",
        "            # Optimized BERTScore settings to prevent hanging\n",
        "            score = bert_scorer.compute(\n",
        "                predictions=predicted_summaries[:5],  # Start with fewer samples\n",
        "                references=test_summaries[:5],        # Process in smaller batches\n",
        "                lang='en',\n",
        "                rescale_with_baseline=False,          # Disable baseline rescaling to speed up\n",
        "                model_type='distilbert-base-uncased', # Use lighter model\n",
        "                device='cpu',                         # Force CPU to avoid GPU memory issues\n",
        "                batch_size=1                          # Process one at a time\n",
        "            )\n",
        "            \n",
        "            avg_f1 = sum(score['f1']) / len(score['f1'])\n",
        "            print(f\"âœ… BERTScore (5 samples): F1 = {avg_f1:.4f}\")\n",
        "            \n",
        "            # If successful with 5, try all 10\n",
        "            if len(predicted_summaries) > 5:\n",
        "                print(\"ðŸ“Š Extending to all samples...\")\n",
        "                score_full = bert_scorer.compute(\n",
        "                    predictions=predicted_summaries,\n",
        "                    references=test_summaries,\n",
        "                    lang='en',\n",
        "                    rescale_with_baseline=False,\n",
        "                    model_type='distilbert-base-uncased',\n",
        "                    device='cpu',\n",
        "                    batch_size=1\n",
        "                )\n",
        "                avg_f1_full = sum(score_full['f1']) / len(score_full['f1'])\n",
        "                print(f\"âœ… BERTScore (all samples): F1 = {avg_f1_full:.4f}\")\n",
        "                return score_full, avg_f1_full\n",
        "            \n",
        "            return score, avg_f1\n",
        "            \n",
        "    except TimeoutError:\n",
        "        print(\"â° BERTScore timed out after 60 seconds\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ BERTScore failed: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Approach 2: Simple ROUGE-like evaluation as fallback\n",
        "def evaluate_with_simple_metrics():\n",
        "    print(\"ðŸ“Š Using simple text similarity metrics as fallback...\")\n",
        "    \n",
        "    def simple_f1_score(pred, ref):\n",
        "        \"\"\"Simple token-based F1 score\"\"\"\n",
        "        pred_tokens = set(pred.lower().split())\n",
        "        ref_tokens = set(ref.lower().split())\n",
        "        \n",
        "        if not pred_tokens and not ref_tokens:\n",
        "            return 1.0\n",
        "        if not pred_tokens or not ref_tokens:\n",
        "            return 0.0\n",
        "            \n",
        "        intersection = pred_tokens.intersection(ref_tokens)\n",
        "        precision = len(intersection) / len(pred_tokens)\n",
        "        recall = len(intersection) / len(ref_tokens)\n",
        "        \n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "    \n",
        "    scores = []\n",
        "    for pred, ref in zip(predicted_summaries, test_summaries):\n",
        "        score = simple_f1_score(pred, ref)\n",
        "        scores.append(score)\n",
        "    \n",
        "    avg_score = sum(scores) / len(scores)\n",
        "    print(f\"âœ… Simple Token F1: {avg_score:.4f}\")\n",
        "    return scores, avg_score\n",
        "\n",
        "# Approach 3: Length and coverage analysis\n",
        "def evaluate_with_coverage_analysis():\n",
        "    print(\"ðŸ“Š Analyzing summary coverage and quality...\")\n",
        "    \n",
        "    results = {\n",
        "        'avg_length_pred': sum(len(s.split()) for s in predicted_summaries) / len(predicted_summaries),\n",
        "        'avg_length_ref': sum(len(s.split()) for s in test_summaries) / len(test_summaries),\n",
        "        'non_empty_summaries': len([s for s in predicted_summaries if s.strip()]),\n",
        "        'coverage_scores': []\n",
        "    }\n",
        "    \n",
        "    # Simple coverage analysis\n",
        "    for pred, ref in zip(predicted_summaries, test_summaries):\n",
        "        pred_words = set(pred.lower().split())\n",
        "        ref_words = set(ref.lower().split())\n",
        "        coverage = len(pred_words.intersection(ref_words)) / len(ref_words) if ref_words else 0\n",
        "        results['coverage_scores'].append(coverage)\n",
        "    \n",
        "    results['avg_coverage'] = sum(results['coverage_scores']) / len(results['coverage_scores'])\n",
        "    \n",
        "    print(f\"ðŸ“ˆ SUMMARY ANALYSIS:\")\n",
        "    print(f\"   - Predicted avg length: {results['avg_length_pred']:.1f} words\")\n",
        "    print(f\"   - Reference avg length: {results['avg_length_ref']:.1f} words\")\n",
        "    print(f\"   - Non-empty summaries: {results['non_empty_summaries']}/{len(predicted_summaries)}\")\n",
        "    print(f\"   - Average coverage: {results['avg_coverage']:.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Execute evaluation approaches\n",
        "print(\"ðŸš€ Starting evaluation process...\\n\")\n",
        "\n",
        "# Try BERTScore first\n",
        "bert_score, bert_f1 = evaluate_with_bertscore()\n",
        "\n",
        "# Always run fallback methods for comparison\n",
        "simple_scores, simple_f1 = evaluate_with_simple_metrics()\n",
        "coverage_results = evaluate_with_coverage_analysis()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ“‹ EVALUATION SUMMARY:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if bert_f1 is not None:\n",
        "    print(f\"ðŸŽ¯ BERTScore F1: {bert_f1:.4f}\")\n",
        "else:\n",
        "    print(\"âš ï¸  BERTScore: FAILED/TIMEOUT\")\n",
        "\n",
        "print(f\"ðŸ“ Simple Token F1: {simple_f1:.4f}\")\n",
        "print(f\"ðŸ“Š Coverage Score: {coverage_results['avg_coverage']:.4f}\")\n",
        "\n",
        "# Store results for further use\n",
        "evaluation_results = {\n",
        "    'bert_score': bert_score,\n",
        "    'bert_f1': bert_f1,\n",
        "    'simple_f1': simple_f1,\n",
        "    'coverage_results': coverage_results\n",
        "}\n",
        "\n",
        "print(f\"\\nðŸ’¡ Recommendation: Use {'BERTScore' if bert_f1 else 'Simple Token F1'} as primary metric\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-uqPBrseLv6"
      },
      "source": [
        "Now we calculate the **average F1 score** across all evaluated summaries, giving an overall performance measure of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoY9XI8YY0pE"
      },
      "source": [
        "**Note:** Since this is a generative model, the output may vary slightly each time. Additionally, because the evaluator is built on neural networks, its responses may also change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DESywvMrug7X",
        "outputId": "c604e1c5-d706-44fd-d634-645a95a9c42c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average F1 Score: 0.7652\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.765225625038147"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the average F1 score across all generated summaries\n",
        "# Use the BERTScore results from the evaluation_results dictionary\n",
        "score = evaluation_results['bert_score']\n",
        "average_f1 = sum(score['f1']) / len(score['f1'])\n",
        "print(f\"Average F1 Score: {average_f1:.4f}\")\n",
        "average_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvZ1hPLCWwYt"
      },
      "source": [
        "# **2. Analysis and Optimization for API-Based Models**\n",
        "\n",
        "Since we're using GPT-OSS 20b via LM Studio API, we cannot perform traditional fine-tuning. Instead, this section focuses on:\n",
        "\n",
        "1. **Data Analysis**: Understanding training patterns to optimize prompts\n",
        "2. **Prompt Engineering**: Using insights from training data to improve system messages\n",
        "3. **Parameter Tuning**: Optimizing API parameters (temperature, max_tokens, etc.)\n",
        "4. **Performance Comparison**: Comparing different prompt strategies\n",
        "\n",
        "This approach provides similar benefits to fine-tuning but works with API-based models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFBNy7vSq42O"
      },
      "source": [
        "# **2. Fine Tuning LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km_LXL5DaGBO"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18s8RG3i3ay5"
      },
      "source": [
        "We first read the CSV into a **Pandas DataFrame** because it is easy to inspect and manipulate tabular data. However, Hugging Face models and trainers do not work directly with DataFrames they expect data in the form of a **`Dataset` object** from the `datasets` library.\n",
        "\n",
        "Thatâ€™s why we convert the DataFrame into a **dictionary of lists**. The `Dataset.from_dict()` method then turns this dictionary into a Hugging Face `Dataset`, which is optimized for:\n",
        "\n",
        "* fast tokenization, shuffling, and batching,\n",
        "* direct compatibility with `Trainer` / `SFTTrainer`,\n",
        "* efficient storage and processing on large datasets.\n",
        "\n",
        "DataFrame stores data like a table (rows Ã— columns), while a Dataset stores data as a dictionary of columns (each column is an array/list), making it better suited for ML pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6GqVK_ShZ2m"
      },
      "source": [
        "#### Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EjytqCN7rMBu"
      },
      "outputs": [],
      "source": [
        "# Read the fine-tuning training CSV into a Pandas DataFrame\n",
        "training = pd.read_csv(\"../data/finetuning_training.csv\")\n",
        "\n",
        "# Convert the DataFrame into a dictionary of lists (required for Hugging Face Dataset)\n",
        "training_dict = training.to_dict(orient='list')\n",
        "\n",
        "# Create a Hugging Face Dataset from the dictionary\n",
        "training_dataset = Dataset.from_dict(training_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbNMLPPg3rk8"
      },
      "source": [
        "Store the end-of-sequence token (used to mark the end of each input/output text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_TURs0KUrL9v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using API-based inference - no tokenizer configuration needed\n"
          ]
        }
      ],
      "source": [
        "# EOS token not needed for API-based inference with GPT-OSS 20b\n",
        "print(\"Using API-based inference - no tokenizer configuration needed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6rSZF3Qhdb2"
      },
      "source": [
        "#### Create a prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOAzduah_e_z"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIqdC93Rhi0g"
      },
      "source": [
        "### Prompt Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "513Rtfvx3-6d"
      },
      "source": [
        "# Note: Fine-tuning is not applicable when using GPT-OSS 20b via LM Studio API\n",
        "# The model is already pre-trained and accessed through API calls\n",
        "# Instead, we can optimize performance through prompt engineering\n",
        "\n",
        "print(\"ðŸ”„ FINE-TUNING ALTERNATIVE FOR API-BASED MODELS\")\n",
        "print(\"=\" * 55)\n",
        "print(\"Since we're using GPT-OSS 20b via LM Studio API, traditional\")\n",
        "print(\"fine-tuning is not applicable. Instead, we can:\")\n",
        "print()\n",
        "print(\"âœ… Optimize prompts (already done in inference section)\")\n",
        "print(\"âœ… Use few-shot learning with examples\")\n",
        "print(\"âœ… Adjust API parameters (temperature, max_tokens, etc.)\")\n",
        "print(\"âœ… Implement retrieval-augmented generation (RAG)\")\n",
        "print()\n",
        "print(\"The model performance can be improved through:\")\n",
        "print(\"1. Better prompt engineering\")\n",
        "print(\"2. Context examples in system messages\") \n",
        "print(\"3. Fine-tuned API parameters\")\n",
        "print(\"4. Post-processing of generated summaries\")\n",
        "\n",
        "# Since we can't fine-tune via API, we'll skip the traditional fine-tuning steps\n",
        "# and focus on evaluation and comparison with the base model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0sjC8_Y_e9j"
      },
      "outputs": [],
      "source": [
        "# Fixed function for API-based models like GPT-OSS 20b\n",
        "# This function is kept for compatibility but adapted for API-based inference\n",
        "\n",
        "def prompt_formatter(example, prompt_template):\n",
        "    \"\"\"\n",
        "    Format training examples for API-based models.\n",
        "    Note: For GPT-OSS 20b via LM Studio, we don't actually use this formatted data\n",
        "    since we make direct API calls. This is kept for notebook compatibility.\n",
        "    \"\"\"\n",
        "    # Instruction for the model\n",
        "    instruction = 'Write a concise summary of the following dialogue.'\n",
        "\n",
        "    # Extract dialogue and reference summary from the dataset example\n",
        "    dialogue = example[\"Dialogues\"]\n",
        "    summary = example[\"Summary\"]\n",
        "\n",
        "    # Merge the instruction, dialogue, and summary into the prompt template\n",
        "    # No EOS_TOKEN needed for API-based inference - removed the problematic line\n",
        "    formatted_prompt = prompt_template.format(instruction, dialogue, summary)\n",
        "\n",
        "    # Return as a dictionary in the format expected by the trainer\n",
        "    return {'text': formatted_prompt}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZFaxW79CctT"
      },
      "source": [
        "# Traditional fine-tuning data preparation is skipped for API-based models\n",
        "# Instead, let's analyze the training data to understand patterns for prompt optimization\n",
        "\n",
        "print(\"ðŸ“Š TRAINING DATA ANALYSIS FOR PROMPT OPTIMIZATION\")\n",
        "print(\"=\" * 52)\n",
        "\n",
        "# Load and analyze training data patterns\n",
        "import pandas as pd\n",
        "training = pd.read_csv(\"../data/finetuning_training.csv\")\n",
        "\n",
        "print(f\"Training dataset size: {len(training)} examples\")\n",
        "print(f\"Average dialogue length: {training['Dialogues'].str.len().mean():.0f} characters\")\n",
        "print(f\"Average summary length: {training['Summary'].str.len().mean():.0f} characters\")\n",
        "\n",
        "# Analyze summary patterns for prompt optimization\n",
        "summary_words = training['Summary'].str.split().str.len()\n",
        "print(f\"Average summary word count: {summary_words.mean():.1f} words\")\n",
        "print(f\"Summary length range: {summary_words.min()}-{summary_words.max()} words\")\n",
        "\n",
        "# Sample a few examples to understand the style\n",
        "print(f\"\\nðŸ“ SAMPLE TRAINING EXAMPLES (for prompt design reference):\")\n",
        "for i in range(min(2, len(training))):\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Dialogue: {training.iloc[i]['Dialogues'][:150]}...\")\n",
        "    print(f\"Summary: {training.iloc[i]['Summary']}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(\"\\nðŸ’¡ These patterns can inform our system message and prompt design!\")\n",
        "print(\"   - Target summary length: ~{:.0f} words\".format(summary_words.mean()))\n",
        "print(\"   - Focus on key business points and next steps\")\n",
        "print(\"   - Professional, structured format\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e2021eea82144517b2a89d67402f6296",
            "73726c48f256499daf6a7e5ff7c8d05d",
            "8595e39cc76842029c15847345922cdc",
            "70af7c7881c445d79d4ac28cf196c379",
            "bf846beed3634db78681d05271da7c01",
            "df5637d8d8e64c418e906500c4a925e6",
            "f2ead2bea789443692b70420e747fab4",
            "182044c6a2574c4cbf97810a098484c9",
            "e196b6f6012c4591bec9956bd1b5607b",
            "a733c57b43c04e749f6cb10ab3c5c647",
            "e452aa326751490485ba8437dadcc361"
          ]
        },
        "id": "V74wlPa7_e71",
        "outputId": "1b6c68bb-aac7-498d-c21d-876604368756"
      },
      "outputs": [],
      "source": [
        "# Note: This step is not actually needed for API-based models like GPT-OSS 20b\n",
        "# since we format prompts directly in API calls. This is kept for notebook compatibility.\n",
        "\n",
        "print(\"âš ï¸  COMPATIBILITY NOTE:\")\n",
        "print(\"The following data formatting step is not used for GPT-OSS 20b API inference.\")\n",
        "print(\"For API-based models, we format prompts directly in the API calls.\")\n",
        "print(\"Running anyway for notebook completeness...\\n\")\n",
        "\n",
        "# Apply the prompt_formatter function to each example in the training dataset\n",
        "# This formats dialogues and summaries into prompts suitable for model training\n",
        "formatted_training_dataset = training_dataset.map(\n",
        "    prompt_formatter,\n",
        "    fn_kwargs={'prompt_template': alpaca_prompt}  # Pass the Alpaca-style prompt template\n",
        ")\n",
        "\n",
        "print(\"âœ… Data formatting completed (though not used for API-based inference)\")\n",
        "print(\"ðŸ“ Formatted dataset created for compatibility with traditional fine-tuning sections\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvnCfQ9I_e5F"
      },
      "outputs": [],
      "source": [
        "# Analyze validation data for additional insights\n",
        "validation = pd.read_csv(\"../data/finetuning_validation.csv\")\n",
        "\n",
        "print(\"ðŸ“Š VALIDATION DATA ANALYSIS\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Validation dataset size: {len(validation)} examples\")\n",
        "print(f\"Average dialogue length: {validation['Dialogues'].str.len().mean():.0f} characters\")\n",
        "print(f\"Average summary length: {validation['Summary'].str.len().mean():.0f} characters\")\n",
        "\n",
        "# Compare training vs validation patterns\n",
        "val_summary_words = validation['Summary'].str.split().str.len()\n",
        "print(f\"Validation avg summary words: {val_summary_words.mean():.1f}\")\n",
        "\n",
        "print(f\"\\nðŸ” Data consistency check:\")\n",
        "print(f\"   Training avg summary: {training['Summary'].str.split().str.len().mean():.1f} words\")\n",
        "print(f\"   Validation avg summary: {val_summary_words.mean():.1f} words\")\n",
        "print(f\"   Difference: {abs(training['Summary'].str.split().str.len().mean() - val_summary_words.mean()):.1f} words\")\n",
        "\n",
        "print(\"\\nâœ… Data analysis complete - ready for API-based evaluation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d08975faef354735a691d3bc98894693",
            "9d50bfb3ceab4623875a8dc2045931f8",
            "b3b3980bff4a47ecb25f6da45fbbf6fc",
            "b00986e06e0e46e99419cc8db7de72e1",
            "f2daf4a9808d4746a8c9fb774cf3baea",
            "bc9aae332bad47faae3947721f8120d2",
            "cbf6ccaa7ed2469e92323f82ffed9af3",
            "60a9f2753a4c447fa1cedc9d43ff5b90",
            "c0337deda64f43eb9eef1c600a5e4270",
            "a28e4972fad143c09cc7618bc543f696",
            "c7746273b7624ecf8a8782eaf21582b8"
          ]
        },
        "id": "U2wTZleC_e2a",
        "outputId": "35e56e4f-5519-4951-ebe6-132e0dc97743"
      },
      "outputs": [],
      "source": [
        "## Prompt Engineering and Optimization\n",
        "\n",
        "Instead of fine-tuning model weights, we optimize the interaction with GPT-OSS 20b through:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCmKWuZkBzzR"
      },
      "source": [
        "## Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYRXt6WZBzlO"
      },
      "source": [
        "We now patch in the adapter modules to the base model using the `get_peft_model` method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goth8Q4O4dCJ"
      },
      "source": [
        "We are adapting the large language model for our task using a technique called **LoRA (Low-Rank Adaptation)**. Instead of retraining the entire model (which would be very expensive), LoRA only updates a small number of parameters while keeping most of the model frozen.\n",
        "\n",
        "\n",
        "* **`r`** - Rank of low-rank matrices; higher = more adaptation, typical 4-64.\n",
        "* **`lora_alpha`** - Scaling factor for LoRA updates; higher = stronger effect, typical 8-32.\n",
        "* **`lora_dropout`** - Dropout on LoRA layers to prevent overfitting, 0-0.3.\n",
        "* **`target_modules`** - The specific parts of the model we allow to be updated.\n",
        "* **`use_gradient_checkpointing`** - Save memory by recomputing activations, `True`/`False`.\n",
        "* **`random_state`** - Seed for reproducibility, any integer.\n",
        "\n",
        "This step makes the model **lighter, faster, and cheaper to fine-tune**, while still learning how to summarize dialogues effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja43AfZ3I8Nb"
      },
      "source": [
        "For more information, please refer to the [Unsloth](https://github.com/unslothai/unsloth) repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAbRmouJgpo"
      },
      "source": [
        "# Demonstrate prompt optimization using training data insights\n",
        "print(\"ðŸŽ¯ PROMPT OPTIMIZATION DEMONSTRATION\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Use training data to create few-shot examples for better prompts\n",
        "sample_dialogue = training.iloc[0]['Dialogues']\n",
        "sample_summary = training.iloc[0]['Summary']\n",
        "\n",
        "print(\"ðŸ“š Creating optimized prompt with few-shot learning...\")\n",
        "\n",
        "# Enhanced system message based on training data patterns\n",
        "optimized_system_message = f\"\"\"You are an expert business conversation summarizer specializing in insurance sales dialogues. \n",
        "\n",
        "Based on analysis of professional summaries, create structured summaries that:\n",
        "- Average {training['Summary'].str.split().str.len().mean():.0f} words in length\n",
        "- Focus on client needs, products discussed, and next steps\n",
        "- Use professional, concise language\n",
        "- Include key decision points and commitments\n",
        "\n",
        "Example format:\n",
        "**Client Needs**: [Brief description]\n",
        "**Products Discussed**: [Services/coverage mentioned]  \n",
        "**Key Decisions**: [Important points and agreements]\n",
        "**Next Steps**: [Follow-up actions needed]\"\"\"\n",
        "\n",
        "print(\"âœ… Optimized system message created\")\n",
        "print(f\"ðŸ“Š Target summary length: ~{training['Summary'].str.split().str.len().mean():.0f} words\")\n",
        "print(f\"ðŸ“‹ Format: Structured with clear sections\")\n",
        "print(f\"ðŸŽ¨ Style: Professional insurance domain language\")\n",
        "\n",
        "# Test the optimized prompt with a sample\n",
        "print(f\"\\nðŸ§ª Testing optimized prompt on sample dialogue...\")\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": optimized_system_message},\n",
        "            {\"role\": \"user\", \"content\": f\"Summarize this conversation:\\n\\n{sample_dialogue}\"}\n",
        "        ],\n",
        "        max_tokens=200,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    \n",
        "    optimized_summary = response.choices[0].message.content.strip()\n",
        "    print(f\"âœ… Optimized summary generated ({len(optimized_summary.split())} words)\")\n",
        "    print(f\"\\nOptimized Summary:\\n{optimized_summary}\")\n",
        "    \n",
        "    print(f\"\\nðŸ“‹ Comparison:\")\n",
        "    print(f\"Original Reference ({len(sample_summary.split())} words):\\n{sample_summary}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Test failed: {e}\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ This optimized approach replaces traditional fine-tuning for API-based models!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnIsofeq_e0B",
        "outputId": "a5aefd10-bd5b-4327-de72-1b2b93f43dd1"
      },
      "outputs": [],
      "source": [
        "# Note: LoRA configuration is not needed when using GPT-OSS 20b via LM Studio API\n",
        "# The model is already pre-trained and hosted, so we don't need fine-tuning setup\n",
        "\n",
        "print(\"Using pre-trained GPT-OSS 20b model via LM Studio API\")\n",
        "print(\"No LoRA configuration or fine-tuning setup required\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g05VpvboB9o7"
      },
      "source": [
        "The **architecture** of the Mistral model, specifically the MistralForCausalLM, consists of several key components:\n",
        "\n",
        "1) Embedding Layer: The model starts with an embedding layer that converts input tokens into a dense representation with an output size of 4096, supporting a vocabulary of 32,000 tokens.\n",
        "\n",
        "2) Decoder Layers: The core of the model comprises 32 MistralDecoderLayer instances, each containing:\n",
        "- Self-Attention Mechanism: This includes multiple projection layers for queries,\n",
        "keys, values, and output, all designed to handle 4-bit precision for efficient computation. Rotary embeddings are also employed for position encoding.\n",
        "- Feedforward Network (MLP): The MLP features gates and projections to expand the dimensionality to 14,336 before reducing it back to 4096, using the SiLU activation function.\n",
        "- Layer Normalization: Each decoder layer includes input and post-attention normalization using MistralRMSNorm.\n",
        "\n",
        "3) Final Normalization: The entire model concludes with an additional normalization layer.\n",
        "\n",
        "4) Linear Output Head: The model includes a linear layer that maps the 4096-dimensional output back to the token vocabulary size (32,000), enabling the generation of predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCS6BYy-DkTw"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da6_rFs77G78"
      },
      "source": [
        "Notice how LoRA adapters are attached to the layers specified during instantiation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tB5eZxDp88t"
      },
      "source": [
        "```\n",
        "PeftModelForCausalLM(\n",
        "  (base_model): LoraModel(\n",
        "    (model): MistralForCausalLM(\n",
        "      (model): MistralModel(\n",
        "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
        "        (layers): ModuleList(\n",
        "          (0-31): 32 x MistralDecoderLayer(\n",
        "            (self_attn): MistralAttention(\n",
        "              (q_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "                zzz(lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (k_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (v_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (o_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (rotary_emb): LlamaRotaryEmbedding()\n",
        "            )\n",
        "            (mlp): MistralMLP(\n",
        "              (gate_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (up_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (down_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (act_fn): SiLU()\n",
        "            )\n",
        "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
        "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
        "          )\n",
        "        )\n",
        "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
        "        (rotary_emb): LlamaRotaryEmbedding()\n",
        "      )\n",
        "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
        "    )\n",
        "  )\n",
        ")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOLX05I97IzR"
      },
      "source": [
        "For training, we use the following nuances borrowed from the broader deep learning discipline.\n",
        "\n",
        "- Low learning rates for smooth parameter updates\n",
        "- Early stopping to monitor for validation loss (negative log likelihood in this case)\n",
        "- Checkpointing to enable resumption of training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMGd9Kdc5B-a"
      },
      "source": [
        "We are creating a **trainer** that will handle the fine-tuning of our model. The trainer takes care of feeding the data into the model, running the training loop, tracking progress, and saving results.\n",
        "\n",
        "Key points in this setup:\n",
        "\n",
        "* **Model & Tokenizer** - The language model and its tokenizer we are fine-tuning.\n",
        "* **Training & Validation Data** - Split datasets so the model can learn on one set and be tested on another.\n",
        "* **Max Sequence Length (2048)** - How much text the model can read at once.\n",
        "* **Data Collator** - Groups the data into batches in the right format.\n",
        "* **Batch Size & Gradient Accumulation** - Train on small pieces at a time (due to memory limits) and combine updates to act like a larger batch.\n",
        "* **Learning Rate & Optimizer** - Control how fast the model learns and how updates are applied.\n",
        "* **Epochs / Steps** - How long the model trains.\n",
        "* **FP16 / BF16** - Use lower precision for faster and more memory-efficient training.\n",
        "* **Output Directory** - Where trained model checkpoints and logs are saved.\n",
        "\n",
        "\n",
        "This trainer automates the whole training process from sending data into the model to adjusting weights, logging progress, and saving results, making fine-tuning efficient and manageable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EGcJ09z_exb"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,  # LoRA-adapted model to fine-tune\n",
        "    tokenizer = tokenizer,  # Tokenizer corresponding to the model\n",
        "    train_dataset = formatted_training_dataset,  # Training dataset in prompt-ready format\n",
        "    eval_dataset = formatted_validation_dataset,  # Validation dataset for evaluation\n",
        "    dataset_text_field = \"text\",  # Field in dataset containing the input text\n",
        "    max_seq_length = 2048,  # Maximum sequence length for training\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer),  # Handles batching\n",
        "    dataset_num_proc = 2,  # Number of processes for dataset preprocessing\n",
        "    packing = False,  # Packing short sequences can make training faster (disabled here)\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,  # Batch size per GPU/CPU\n",
        "        gradient_accumulation_steps = 4,  # Accumulate gradients over steps to simulate larger batch\n",
        "        warmup_steps = 5,  # Learning rate warmup steps\n",
        "        max_steps = 30,  # Total training steps (used here for quick demonstration)\n",
        "        learning_rate = 2e-4,  # Learning rate for optimizer\n",
        "        fp16 = not is_bfloat16_supported(),  # Use 16-bit float if bfloat16 not supported\n",
        "        bf16 = is_bfloat16_supported(),  # Use bfloat16 if supported\n",
        "        logging_steps = 1,  # Log metrics every step\n",
        "        optim = \"adamw_8bit\",  # 8-bit AdamW optimizer for memory efficiency\n",
        "        weight_decay = 0.01,  # Regularization to prevent overfitting\n",
        "        lr_scheduler_type = \"linear\",  # Linear learning rate decay\n",
        "        seed = 3407,  # For reproducibility\n",
        "        output_dir = \"outputs\",  # Directory to save checkpoints and outputs\n",
        "        report_to = \"none\"  # No external logging (like WandB)\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xH8cGYJJ_0Wh",
        "outputId": "de4f4eb2-f2c9-442d-c2e5-a1df2adf4225"
      },
      "outputs": [],
      "source": [
        "training_history = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwO05JGo3Egh"
      },
      "source": [
        "## Saving the Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C45mg7AjBMTr"
      },
      "source": [
        "\n",
        "We will be saving the **LoRA Parameters** of our fine-tuned model so that we can test/evaluate the model later. Since fine-tuning is an expensive process, itâ€™s best to save these adapter files in case of crashes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9EDRk6Zh-b7"
      },
      "source": [
        "### Setup to enable bash commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyWEau0dneNw"
      },
      "source": [
        "This code ensures that all file names and metadata are encoded in UTF-8, preventing errors when writing model files to disk or Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsoPFnwi_0US"
      },
      "outputs": [],
      "source": [
        "# Setup to ensure Python uses UTF-8 encoding for shell/batch commands\n",
        "import locale\n",
        "\n",
        "# Override the system's preferred encoding to always return \"UTF-8\"\n",
        "def getpreferredencoding():\n",
        "    return \"UTF-8\"\n",
        "\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cmI1i5wm0UT"
      },
      "outputs": [],
      "source": [
        "lora_model_name = \"finetuned_mistral_llm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBEche8koNV0"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(lora_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k55cdXfSoXNv"
      },
      "source": [
        "`ls -lh {folder}`\n",
        "\n",
        "* **ls** - Lists files and folders.\n",
        "* **-l** - Shows detailed information like permissions, owner, size, and modification date.\n",
        "* **-h** - Makes file sizes human-readable (KB, MB, GB instead of bytes).\n",
        "* `{folder}` - The folder whose contents you want to see.\n",
        "\n",
        "Shows the **contents and sizes** of a folder in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdMNXKQ5_0SA",
        "outputId": "87e43571-d756-43b1-a001-eaa7a3108ab2"
      },
      "outputs": [],
      "source": [
        "!ls -lh {lora_model_name}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NoPpKOFoVgZ"
      },
      "source": [
        "`cp -r {source} {destination}`\n",
        "\n",
        "* **cp** - Stands for â€œcopyâ€.\n",
        "* **-r** - Means â€œrecursiveâ€, which allows copying **folders and all their contents** (subfolders and files).\n",
        "* `{source}` - The folder you want to copy.\n",
        "* `{destination}` - Where you want to copy it to.\n",
        "\n",
        "Copies a folder and everything inside it to another location.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjPmezvToOeH",
        "outputId": "bbc47017-e8c7-417f-b8e9-4c5d2a2aef59"
      },
      "outputs": [],
      "source": [
        "# # Comment out this cell if you want to save the model to Google Drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# drive_model_path = \"/content/drive/MyDrive/finetuned_mistral_llm\"\n",
        "\n",
        "# !cp -r {lora_model_name} {drive_model_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sq_u6Mpq441"
      },
      "source": [
        "# **3. Evaluation of LLM after FineTuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGV4lcz8FHa1"
      },
      "source": [
        "### Loading the Fine-tuned Mistral LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXDLid2b0zLu",
        "outputId": "2f1a67e7-9f85-4cd8-d24c-0e625bf22262"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model using standard transformers and PEFT\n",
        "from peft import PeftModel\n",
        "\n",
        "# First load the base model\n",
        "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "if device == \"mps\":\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=None,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "    base_model = base_model.to(device)\n",
        "elif device == \"cuda\":\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "else:\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        torch_dtype=torch.float32,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "# Load the fine-tuned LoRA adapters\n",
        "model = PeftModel.from_pretrained(base_model, lora_model_name)\n",
        "model.eval()\n",
        "print(f\"Fine-tuned model loaded on device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb0DfllzFFmx"
      },
      "source": [
        "### Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8oJluSXGvNu"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Write a concise summary of the following dialogue.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opvw9Yxp1lqR"
      },
      "outputs": [],
      "source": [
        "predicted_summaries = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiBUpkKHHGI8",
        "outputId": "e02d1d4e-f844-477d-b957-47acbf73f9e7"
      },
      "outputs": [],
      "source": [
        "# Loop through each dialogue in the test set and generate summaries\n",
        "for dialogue in tqdm(test_dialogues):\n",
        "    try:\n",
        "        # Format the dialogue into the Alpaca-style prompt\n",
        "        prompt = alpaca_prompt_template.format(dialogue, '')\n",
        "\n",
        "        # Tokenize the prompt and move to appropriate device\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate model output (summary)\n",
        "        with torch.no_grad():  # Disable gradient computation for inference\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,          # Limit summary length\n",
        "                use_cache=True,              # Reuse past key values for efficiency\n",
        "                temperature=0,               # Deterministic output\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                do_sample=False              # Ensure deterministic output\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens into text, skipping special tokens\n",
        "        prediction = tokenizer.decode(\n",
        "            outputs[0][inputs.input_ids.shape[-1]:],  # Remove input prompt tokens\n",
        "            skip_special_tokens=True,\n",
        "            cleanup_tokenization_spaces=True\n",
        "        )\n",
        "\n",
        "        # Store the generated summary\n",
        "        predicted_summaries.append(prediction)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing dialogue: {e}\")  # Log error if generation fails and continue\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WvP1kbtFIcF"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DNcZha8Vb7s",
        "outputId": "1d96c2de-57a3-484f-974c-7c4e82368e44"
      },
      "outputs": [],
      "source": [
        "predicted_summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTPGlwJP1llz"
      },
      "outputs": [],
      "source": [
        "# Evaluate the quality of generated summaries using BERTScore\n",
        "score = bert_scorer.compute(\n",
        "    predictions=predicted_summaries,  # Summaries generated by the model\n",
        "    references=test_summaries,        # Ground-truth summaries from the dataset\n",
        "    lang='en',                        # Specify English language\n",
        "    rescale_with_baseline=True        # Normalize scores for easier interpretation\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2-CFZ0X1ljj",
        "outputId": "e6d6284b-728d-45a7-bbfc-27011de0a9b7"
      },
      "outputs": [],
      "source": [
        "# Compute the average F1 score across all test examples\n",
        "avg_f1 = sum(score['f1']) / len(score['f1'])\n",
        "avg_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPgLSo07YBo3"
      },
      "source": [
        "**The BERT Score of Finetuned Mistral LLM is 0.53**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_vdbuZ781RI"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w_XdA-F8z7o"
      },
      "source": [
        "**We observed a significant improvement in the BERTScore after fine-tuning the Mistral model, also an observation can be made on the Predicted Summaries**\n",
        "\n",
        "- Previously, the generated summaries of client interactions were overly verbose and lacked alignment with user preferences and domain-specific needs.\n",
        "- By fine-tuning a language model on task-relevant and insurance-specific communication data, we significantly improved the model's ability to generate concise, actionable, and context-aware summaries.\n",
        "- The fine-tuned model now produces outputs that are not only more relevant and structured but also tailored to user expectations, enhancing sales productivity and ensuring better client engagement in the insurance domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfwgNZeOiRHT"
      },
      "source": [
        "<font size = 6 color=\"#4682B4\"><b> Power Ahead </font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Aa1yXUesL7Zy",
        "zFN8LrodMR7w",
        "qDG_248npouB",
        "IGxmR0XVwcAi",
        "Vp4LNGG30e8w",
        "mHaWozqI0hmW",
        "Km_LXL5DaGBO",
        "U6GqVK_ShZ2m",
        "n6rSZF3Qhdb2",
        "NIqdC93Rhi0g",
        "IwO05JGo3Egh",
        "qGV4lcz8FHa1",
        "Fb0DfllzFFmx",
        "3WvP1kbtFIcF",
        "E_vdbuZ781RI"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jupyter_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03aedb3387ef4182a7eaab849fc21123": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "182044c6a2574c4cbf97810a098484c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22b6ba504e194e1183c963417f128677": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "553208bce9ea4ef69921389d09f807c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8afa630e5bc342e59807f0ea8c493f95",
              "IPY_MODEL_80da8e51ccf64550ba5be2f58b28ac98",
              "IPY_MODEL_c6a40368c70c427da29a8f56181ed5fa"
            ],
            "layout": "IPY_MODEL_862b87382adf4c39b93e8f31e0ee6813"
          }
        },
        "5566162bc00d430b878336c23d24bdbf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a9f2753a4c447fa1cedc9d43ff5b90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6424175902e045d1b30d3fa12ff73f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fb638164b754e538e38492a9aa08204": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "703d0fe63c26440b9f251f760c9ef4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fb638164b754e538e38492a9aa08204",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc394f734d584b7d9315560b21640e5d",
            "value": 1355863
          }
        },
        "70af7c7881c445d79d4ac28cf196c379": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a733c57b43c04e749f6cb10ab3c5c647",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e452aa326751490485ba8437dadcc361",
            "value": "â€‡50/50â€‡[00:00&lt;00:00,â€‡1186.51â€‡examples/s]"
          }
        },
        "73726c48f256499daf6a7e5ff7c8d05d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df5637d8d8e64c418e906500c4a925e6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f2ead2bea789443692b70420e747fab4",
            "value": "Map:â€‡100%"
          }
        },
        "80da8e51ccf64550ba5be2f58b28ac98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f329740b7b6441519c5dea729dd7d1ab",
            "max": 1421700479,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22b6ba504e194e1183c963417f128677",
            "value": 1421700479
          }
        },
        "82ed1f8f01ba41d58e60732f502e2606": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab6b2c906ff94da5a94201f409434574",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6424175902e045d1b30d3fa12ff73f32",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "8595e39cc76842029c15847345922cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_182044c6a2574c4cbf97810a098484c9",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e196b6f6012c4591bec9956bd1b5607b",
            "value": 50
          }
        },
        "862b87382adf4c39b93e8f31e0ee6813": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8afa630e5bc342e59807f0ea8c493f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9925280086c420a89726da2b0a3b6d4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e45a8ce6c93e4c069050076cfee78f09",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "9d50bfb3ceab4623875a8dc2045931f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc9aae332bad47faae3947721f8120d2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cbf6ccaa7ed2469e92323f82ffed9af3",
            "value": "Map:â€‡100%"
          }
        },
        "a28e4972fad143c09cc7618bc543f696": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a547d8c63bb44f88a6522c7186b5e268": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a733c57b43c04e749f6cb10ab3c5c647": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9925280086c420a89726da2b0a3b6d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab6b2c906ff94da5a94201f409434574": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b00986e06e0e46e99419cc8db7de72e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a28e4972fad143c09cc7618bc543f696",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c7746273b7624ecf8a8782eaf21582b8",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡210.90â€‡examples/s]"
          }
        },
        "b3b3980bff4a47ecb25f6da45fbbf6fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60a9f2753a4c447fa1cedc9d43ff5b90",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0337deda64f43eb9eef1c600a5e4270",
            "value": 10
          }
        },
        "bc9aae332bad47faae3947721f8120d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf846beed3634db78681d05271da7c01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0337deda64f43eb9eef1c600a5e4270": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6a40368c70c427da29a8f56181ed5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe29dde9a1674024bd7a7fb46f1359f8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fe5f10d9bd4d4db18c91be1612d7c4a8",
            "value": "â€‡1.42G/1.42Gâ€‡[00:17&lt;00:00,â€‡152MB/s]"
          }
        },
        "c7746273b7624ecf8a8782eaf21582b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbf6ccaa7ed2469e92323f82ffed9af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d08975faef354735a691d3bc98894693": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d50bfb3ceab4623875a8dc2045931f8",
              "IPY_MODEL_b3b3980bff4a47ecb25f6da45fbbf6fc",
              "IPY_MODEL_b00986e06e0e46e99419cc8db7de72e1"
            ],
            "layout": "IPY_MODEL_f2daf4a9808d4746a8c9fb774cf3baea"
          }
        },
        "df5637d8d8e64c418e906500c4a925e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e196b6f6012c4591bec9956bd1b5607b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2021eea82144517b2a89d67402f6296": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73726c48f256499daf6a7e5ff7c8d05d",
              "IPY_MODEL_8595e39cc76842029c15847345922cdc",
              "IPY_MODEL_70af7c7881c445d79d4ac28cf196c379"
            ],
            "layout": "IPY_MODEL_bf846beed3634db78681d05271da7c01"
          }
        },
        "e452aa326751490485ba8437dadcc361": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e45a8ce6c93e4c069050076cfee78f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0af8c1d89704bd28d739a274869ad65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5566162bc00d430b878336c23d24bdbf",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a547d8c63bb44f88a6522c7186b5e268",
            "value": "â€‡1.36M/1.36Mâ€‡[00:00&lt;00:00,â€‡11.0MB/s]"
          }
        },
        "f2daf4a9808d4746a8c9fb774cf3baea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2ead2bea789443692b70420e747fab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f329740b7b6441519c5dea729dd7d1ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f483532192744e2c9a4eefb2567b02b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82ed1f8f01ba41d58e60732f502e2606",
              "IPY_MODEL_703d0fe63c26440b9f251f760c9ef4ab",
              "IPY_MODEL_f0af8c1d89704bd28d739a274869ad65"
            ],
            "layout": "IPY_MODEL_03aedb3387ef4182a7eaab849fc21123"
          }
        },
        "fc394f734d584b7d9315560b21640e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe29dde9a1674024bd7a7fb46f1359f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe5f10d9bd4d4db18c91be1612d7c4a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
