{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166b912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perimeter of the triangle with sides 5 , 7 and 10 is: 22\n"
     ]
    }
   ],
   "source": [
    "# generate simple code that calcs the perimeter of a triangle using defined lengths, no functions\n",
    "a = 5\n",
    "b = 7\n",
    "c = 10\n",
    "perimeter = a + b + c\n",
    "print(\"The perimeter of the triangle with sides\", a, \",\", b, \"and\", c, \"is:\", perimeter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf655b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 numbers in the Fibonacci series are: [0, 1, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# Write a python program that prints the first 5 numbers in the fibonacci series.\n",
    "fibonacci = [0, 1]\n",
    "for i in range(2, 5):\n",
    "    next_number = fibonacci[i - 1] + fibonacci[i - 2]\n",
    "    fibonacci.append(next_number)\n",
    "print(\"The first 5 numbers in the Fibonacci series are:\", fibonacci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c80d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are given two integers as input. Write a program to perform the following arithmetic operations on the integers and print the results: \n",
    "# a. Addition\n",
    "# b. Subtraction\n",
    "# c. Multiplication\n",
    "# d. Division\n",
    "a = 10\n",
    "b = 5\n",
    "addition = a + b\n",
    "subtraction = a - b\n",
    "multiplication = a * b\n",
    "division = a / b\n",
    "print(\"Addition:\", addition)\n",
    "print(\"Subtraction:\", subtraction)\n",
    "print(\"Multiplication:\", multiplication)\n",
    "print(\"Division:\", division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09484b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hypotenuse of the triangle with sides 3.0 and 4.0 is: 5.0\n"
     ]
    }
   ],
   "source": [
    "# To do: Write a program to calculate the hypotenuse2 of a right triangle using the lengths of the other two sides. The lengths should be floating-point numbers.\n",
    "import math\n",
    "a = 3.0\n",
    "b = 4.0\n",
    "hypotenuse = math.sqrt(a**2 + b**2)\n",
    "print(\"The hypotenuse of the triangle with sides\", a, \"and\", b, \"is:\", hypotenuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "530f5eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area of the circle with radius 5.0 is: 78.53981633974483\n",
      "The area of the circle with radius 5.0 rounded to 2 decimal places is: 78.54\n",
      "The area of the circle with radius 5.0 truncated to 2 decimal places is: 78.53\n",
      "The area of the circle with radius 5.0 rounded to 2 decimal places is: 78.54\n",
      "78.54\n"
     ]
    }
   ],
   "source": [
    "# Write a program to calculate the area of a circle using the given radius. The radius should be a floating-point number.\n",
    "import math\n",
    "radius = 5.0\n",
    "area = math.pi * radius**2\n",
    "print(\"The area of the circle with radius\", radius, \"is:\", area)\n",
    "# apply rounding to 2 decimal places\n",
    "area_rounded = round(area, 2)\n",
    "print(\"The area of the circle with radius\", radius, \"rounded to 2 decimal places is:\", area_rounded)\n",
    "# truncate to 2 decimal places\n",
    "area_truncated = int(area * 100) / 100.0\n",
    "print(\"The area of the circle with radius\", radius, \"truncated to 2 decimal places is:\", area_truncated)\n",
    "# use f-string formatting to print the area rounded to 2 decimal places\n",
    "print(f\"The area of the circle with radius {radius} rounded to 2 decimal places is: {area:.2f}\")\n",
    "# Write a program to calculate the factorial of a number using a loop.\n",
    "print(f\"{area:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2df7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = int(input())\n",
    "b = int(input())\n",
    "\n",
    "# Write your code below\n",
    "\n",
    "if a > b:\n",
    "    print(\"First number is greater\")\n",
    "elif a <= b:\n",
    "    print(\"First number is NOT greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2918a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplication table for 1:\n",
      "1 x 1 = 1\n",
      "1 x 2 = 2\n",
      "1 x 3 = 3\n",
      "1 x 4 = 4\n",
      "1 x 5 = 5\n",
      "1 x 6 = 6\n",
      "1 x 7 = 7\n",
      "1 x 8 = 8\n",
      "1 x 9 = 9\n",
      "1 x 10 = 10\n"
     ]
    }
   ],
   "source": [
    "# You are given a number as input. Write a program to print the multiplication table of the number from 1 to 10.\n",
    "number = int(input(\"Enter a number to print its multiplication table: \"))\n",
    "print(f\"Multiplication table for {number}:\")\n",
    "for i in range(1, 11):\n",
    "    print(f\"{number} x {i} = {number * i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff0f509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'b', 'c'}\n",
      "Repeating letters: abc\n"
     ]
    }
   ],
   "source": [
    "# if a letter is repeating for 3 times continously then print that letter else ignore and the letter should be unique, no functions\n",
    "input1 = \"aaaabbbcccdd\"\n",
    "# Write your code below\n",
    "repeating_letters = set()\n",
    "for i in range(len(input1) - 2):\n",
    "    if input1[i] == input1[i + 1] == input1[i + 2]:\n",
    "        repeating_letters.add(input1[i])\n",
    "print(repeating_letters)\n",
    "if repeating_letters:\n",
    "    print(\"Repeating letters:\", ''.join(repeating_letters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d07f604c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 4 3 2 1 \n",
      "4 3 2 1 \n",
      "3 2 1 \n",
      "2 1 \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Algorithm to print reverse number pattern\n",
    "n = 5  # You can change this value to print a different pattern size\n",
    "\n",
    "# Loop for each row\n",
    "for i in range(n):\n",
    "    # For the first n-1 rows, print decreasing numbers from (n-i) to 1\n",
    "    if i < n - 1:\n",
    "        for j in range(n - i, 0, -1):\n",
    "            print(j, end=\" \")\n",
    "        print()  # Move to next line after each row\n",
    "    # For the last row, just print 1\n",
    "    else:\n",
    "        print(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46aa82df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "list_1 = [1, 2, 3, 4, 5]\n",
    "list_comp = [i for i in list_1]\n",
    "print(list_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4ce983d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x: (x+2)*5/2)(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1eb4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     3.14\n",
       "1       pi\n",
       "2    2.718\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bucket = [3.14, 'pi', 2.718]\n",
    "\n",
    "series = pd.Series(bucket)\n",
    "\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a dictionary with keys as strings and values as integers\n",
    "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "# convert the dictionary to a pandas Series\n",
    "series_from_dict = pd.Series(my_dict)\n",
    "print(series_from_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5841b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col1  col2\n",
      "0     1     3\n",
      "1     2     4\n",
      "2     3     5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "items = {'col1': [1,2,3], 'col2': [3,4,5]}\n",
    "\n",
    "df_from_dict = pd.DataFrame(items)\n",
    "\n",
    "print(df_from_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee188811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a csv file into a pandas dataframe\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a boolean expression to include only rows where the age is greater than 23\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/stockdata.csv')\n",
    "age_filtered = df[df['age'] > 23]\n",
    "\n",
    "result = df[age_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fcce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a numpy array with values [100,150, 200,250, 300]\n",
    "import numpy as np\n",
    "array = np.array([100, 150, 200, 250, 300])\n",
    "# append a new value 350 to the array\n",
    "array = np.append(array, 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375572f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the following code:\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Input\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X,y = diabetes['data'],diabetes['target']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1],)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss=MeanSquaredError())\n",
    "\n",
    "model.fit(x=X_train, y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45da107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layered Perceptron (MLP) for Customer Cancellation Prediction\n",
    "# Best choice for structured subscription data\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Example: Subscription features might include:\n",
    "# - Age, income, subscription_type, monthly_spend, support_tickets, \n",
    "# - usage_frequency, contract_length, payment_method, etc.\n",
    "\n",
    "# Multi-layered Perceptron Architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(n_features,)),  # Input layer\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),   # Hidden layer 1\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),   # Hidden layer 2\n",
    "    Dropout(0.1),\n",
    "    Dense(1, activation='sigmoid')  # Output: probability of cancellation\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "print(\"MLP is ideal for subscription data because:\")\n",
    "print(\"1. Handles tabular/structured data excellently\")\n",
    "print(\"2. Captures complex feature interactions\") \n",
    "print(\"3. No sequential processing needed\")\n",
    "print(\"4. Perfect for binary classification tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3acc53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for Sales Forecasting - Best choice for time series data\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# LSTM Architecture for Sales Forecasting\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(timesteps, n_features)),  # LSTM layer 1\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=True),  # LSTM layer 2\n",
    "    Dropout(0.2),\n",
    "    LSTM(50),  # Final LSTM layer\n",
    "    Dropout(0.2),\n",
    "    Dense(25),  # Dense layer\n",
    "    Dense(1)   # Output: predicted sales value\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',  # Mean Squared Error for regression\n",
    "    metrics=['mae']  # Mean Absolute Error\n",
    ")\n",
    "\n",
    "print(\"LSTM is ideal for sales forecasting because:\")\n",
    "print(\"1. Captures temporal dependencies in sales data\")\n",
    "print(\"2. Learns seasonal and cyclical patterns\")\n",
    "print(\"3. Handles long-term trends over the past year\")\n",
    "print(\"4. Remembers important patterns from distant past\")\n",
    "print(\"5. Perfect for sequential time series prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136068fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive LSTM for Daily Sales Forecasting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Data preparation for time series\n",
    "def prepare_sales_data(data, lookback_window=30):\n",
    "    \"\"\"\n",
    "    Prepare daily sales data for LSTM training\n",
    "    lookback_window: number of previous days to use for prediction\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(lookback_window, len(scaled_data)):\n",
    "        X.append(scaled_data[i-lookback_window:i, 0])\n",
    "        y.append(scaled_data[i, 0])\n",
    "    \n",
    "    return np.array(X), np.array(y), scaler\n",
    "\n",
    "# Advanced LSTM Architecture for Sales Forecasting\n",
    "def create_sales_lstm_model(lookback_window=30):\n",
    "    model = Sequential([\n",
    "        # First LSTM layer - captures short-term patterns\n",
    "        LSTM(100, return_sequences=True, input_shape=(lookback_window, 1)),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Second LSTM layer - captures medium-term patterns  \n",
    "        LSTM(100, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Third LSTM layer - captures long-term trends\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Dense layers for final prediction\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1)  # Single output for next day's sales\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "print(\"LSTM Sales Forecasting Model Features:\")\n",
    "print(\"1. Uses 30-day lookback window for context\")\n",
    "print(\"2. Multi-layer LSTM captures different time patterns:\")\n",
    "print(\"   - Short-term: daily fluctuations\")\n",
    "print(\"   - Medium-term: weekly patterns\") \n",
    "print(\"   - Long-term: seasonal trends\")\n",
    "print(\"3. Dropout layers prevent overfitting\")\n",
    "print(\"4. Can forecast multiple days ahead\")\n",
    "print(\"5. Handles irregular patterns and trend changes\")\n",
    "\n",
    "# Model can be extended for multi-step forecasting\n",
    "print(\"\\nFor next month forecasting:\")\n",
    "print(\"- Train on daily data from past year (365 days)\")\n",
    "print(\"- Use sliding window approach\")\n",
    "print(\"- Can predict 30 days ahead iteratively\")\n",
    "print(\"- Include external features (holidays, promotions)\")\n",
    "\n",
    "# Create the model\n",
    "# model = create_sales_lstm_model(lookback_window=30)\n",
    "# print(f\"\\nModel created with {model.count_params()} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f589f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Model Configuration:\n",
      "========================================\n",
      "Vocabulary size: 20,000 words\n",
      "Embedding dimension: 500 neurons\n",
      "\n",
      "Answer: Each word embedding will be 500-dimensional\n",
      "Each word is represented by a vector of size: 500\n",
      "\n",
      "Word2Vec Model Structure:\n",
      "- Total embedding matrix shape: (20,000 Ã— 500)\n",
      "- Each word gets exactly 500 features/dimensions\n",
      "- Total parameters in embedding layer: 10,000,000\n",
      "\n",
      "Example:\n",
      "word 'king' â†’ [500-dimensional vector]\n",
      "word 'queen' â†’ [500-dimensional vector]\n",
      "word 'apple' â†’ [500-dimensional vector]\n",
      "\n",
      "Key Point: The number of neurons (500) directly determines\n",
      "the size of each word's embedding vector, regardless of vocabulary size.\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec Embedding Dimensions Explanation\n",
    "print(\"Word2Vec Model Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "vocabulary_size = 20000\n",
    "embedding_dimension = 500  # This is determined by the \"neurons\" parameter\n",
    "\n",
    "print(f\"Vocabulary size: {vocabulary_size:,} words\")\n",
    "print(f\"Embedding dimension: {embedding_dimension} neurons\")\n",
    "\n",
    "print(\"\\nAnswer: Each word embedding will be 500-dimensional\")\n",
    "print(f\"Each word is represented by a vector of size: {embedding_dimension}\")\n",
    "\n",
    "print(\"\\nWord2Vec Model Structure:\")\n",
    "print(f\"- Total embedding matrix shape: ({vocabulary_size:,} Ã— {embedding_dimension})\")\n",
    "print(f\"- Each word gets exactly {embedding_dimension} features/dimensions\")\n",
    "print(f\"- Total parameters in embedding layer: {vocabulary_size * embedding_dimension:,}\")\n",
    "\n",
    "print(\"\\nExample:\")\n",
    "print(f\"word 'king' â†’ [{embedding_dimension}-dimensional vector]\")\n",
    "print(f\"word 'queen' â†’ [{embedding_dimension}-dimensional vector]\") \n",
    "print(f\"word 'apple' â†’ [{embedding_dimension}-dimensional vector]\")\n",
    "\n",
    "print(f\"\\nKey Point: The number of neurons ({embedding_dimension}) directly determines\")\n",
    "print(\"the size of each word's embedding vector, regardless of vocabulary size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW vs Skip-gram Training Objectives Comparison\n",
    "print(\"Sentence: 'The quick brown fox jumps over the lazy dog.'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\".split()\n",
    "print(f\"Words: {sentence}\")\n",
    "print(f\"Total words: {len(sentence)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CBOW (Continuous Bag of Words)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING OBJECTIVE: Predict CENTER word from CONTEXT words\")\n",
    "print(\"Direction: Context â†’ Target\")\n",
    "\n",
    "# Example with window size = 2\n",
    "window_size = 2\n",
    "print(f\"\\nUsing window size = {window_size}\")\n",
    "print(\"Examples of CBOW training pairs:\")\n",
    "\n",
    "for i in range(window_size, len(sentence) - window_size):\n",
    "    target = sentence[i]\n",
    "    context = []\n",
    "    \n",
    "    # Get context words (before and after target)\n",
    "    for j in range(i - window_size, i + window_size + 1):\n",
    "        if j != i:  # Skip the target word itself\n",
    "            context.append(sentence[j])\n",
    "    \n",
    "    print(f\"Context: {context} â†’ Target: '{target}'\")\n",
    "    if i == window_size + 1:  # Show only first 2 examples\n",
    "        break\n",
    "\n",
    "print(\"\\nCBOW Process:\")\n",
    "print(\"1. Input: Multiple context words\")\n",
    "print(\"2. Average their embeddings\")\n",
    "print(\"3. Predict the center word\")\n",
    "print(\"4. Update all word vectors based on prediction error\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SKIP-GRAM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING OBJECTIVE: Predict CONTEXT words from CENTER word\")\n",
    "print(\"Direction: Target â†’ Context\")\n",
    "\n",
    "print(f\"\\nUsing window size = {window_size}\")\n",
    "print(\"Examples of Skip-gram training pairs:\")\n",
    "\n",
    "for i in range(window_size, len(sentence) - window_size):\n",
    "    target = sentence[i]\n",
    "    \n",
    "    # Generate multiple training pairs for each context word\n",
    "    for j in range(i - window_size, i + window_size + 1):\n",
    "        if j != i:  # Skip the target word itself\n",
    "            context_word = sentence[j]\n",
    "            print(f\"Target: '{target}' â†’ Context: '{context_word}'\")\n",
    "    \n",
    "    if i == window_size:  # Show only first word's examples\n",
    "        break\n",
    "\n",
    "print(\"\\nSkip-gram Process:\")\n",
    "print(\"1. Input: Single center word\")\n",
    "print(\"2. Use its embedding\")\n",
    "print(\"3. Predict each surrounding context word\")\n",
    "print(\"4. Update word vectors for each prediction\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY DIFFERENCES SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison = {\n",
    "    \"Aspect\": [\"Input\", \"Output\", \"Training pairs per window\", \"Architecture\", \"Performance\"],\n",
    "    \"CBOW\": [\n",
    "        \"Multiple context words\", \n",
    "        \"Single center word\",\n",
    "        \"1 pair per window\",\n",
    "        \"Many-to-one\",\n",
    "        \"Better with frequent words\"\n",
    "    ],\n",
    "    \"Skip-gram\": [\n",
    "        \"Single center word\",\n",
    "        \"Multiple context words\", \n",
    "        \"Multiple pairs per window\",\n",
    "        \"One-to-many\",\n",
    "        \"Better with rare words\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for i, aspect in enumerate(comparison[\"Aspect\"]):\n",
    "    print(f\"{aspect:25} | CBOW: {comparison['CBOW'][i]:25} | Skip-gram: {comparison['Skip-gram'][i]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPUTATIONAL DIFFERENCES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CBOW:\")\n",
    "print(\"- Fewer training examples (1 per window)\")\n",
    "print(\"- Faster training\")\n",
    "print(\"- Smooths over distributional information\")\n",
    "print(\"- Good for frequent words\")\n",
    "\n",
    "print(\"\\nSkip-gram:\")\n",
    "print(\"- More training examples (2Ã—window_size per center word)\")\n",
    "print(\"- Slower training\")\n",
    "print(\"- Preserves distributional information better\")\n",
    "print(\"- Good for rare words and larger datasets\")\n",
    "\n",
    "print(f\"\\nFor our sentence with window_size={window_size}:\")\n",
    "valid_positions = len(sentence) - 2 * window_size\n",
    "cbow_pairs = valid_positions\n",
    "skipgram_pairs = valid_positions * (2 * window_size)\n",
    "print(f\"CBOW training pairs: {cbow_pairs}\")\n",
    "print(f\"Skip-gram training pairs: {skipgram_pairs}\")\n",
    "print(f\"Skip-gram has {skipgram_pairs // cbow_pairs}x more training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c8814d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENTION SCORES IN TRANSFORMER MODELS\n",
      "==================================================\n",
      "Question: Which of the following sets of scores can be attention scores for a word in a sentence?\n",
      "\n",
      "Options:\n",
      "A) [0.15, 0.50, -0.35]\n",
      "B) [0.15, 0.55, 0.4]\n",
      "C) [0.15, 0.55, 0.3]\n",
      "D) [1.45, 2.34, 0.88]\n",
      "\n",
      "==================================================\n",
      "ATTENTION SCORES PROPERTIES\n",
      "==================================================\n",
      "Key Requirements for Attention Scores:\n",
      "1. All values must be NON-NEGATIVE (â‰¥ 0)\n",
      "2. All values must sum to EXACTLY 1.0\n",
      "3. They represent probability distributions\n",
      "4. Each score shows how much attention to pay to each position\n",
      "\n",
      "Analyzing each option:\n",
      "------------------------------\n",
      "A) [0.15, 0.5, -0.35]\n",
      "   Sum: 0.30000000000000004\n",
      "   Has negative values: True\n",
      "   Valid: False\n",
      "\n",
      "B) [0.15, 0.55, 0.4]\n",
      "   Sum: 1.1\n",
      "   Has negative values: False\n",
      "   Valid: False\n",
      "\n",
      "C) [0.15, 0.55, 0.3]\n",
      "   Sum: 1.0\n",
      "   Has negative values: False\n",
      "   Valid: True\n",
      "\n",
      "D) [1.45, 2.34, 0.88]\n",
      "   Sum: 4.67\n",
      "   Has negative values: False\n",
      "   Valid: False\n",
      "\n",
      "==================================================\n",
      "ANSWER ANALYSIS\n",
      "==================================================\n",
      "CORRECT ANSWER: C) [0.15, 0.55, 0.3]\n",
      "\n",
      "Reasoning:\n",
      "âœ“ All values are non-negative\n",
      "âœ“ Sum equals 1.0 exactly (0.15 + 0.55 + 0.3 = 1.0)\n",
      "âœ“ Represents valid probability distribution\n",
      "\n",
      "Why other options are wrong:\n",
      "A) Contains negative value (-0.35) - impossible for attention\n",
      "B) Sum = 1.1 (not 1.0) - violates probability constraint\n",
      "D) Sum = 4.67 (not 1.0) - violates probability constraint\n",
      "\n",
      "==================================================\n",
      "ATTENTION MECHANISM CONTEXT\n",
      "==================================================\n",
      "In Transformer models:\n",
      "â€¢ Attention scores are computed using softmax function\n",
      "â€¢ Softmax ensures all outputs are positive and sum to 1\n",
      "â€¢ Each score represents 'how much to focus' on each word\n",
      "â€¢ Higher scores = more attention to that position\n",
      "â€¢ Lower scores = less attention to that position\n",
      "\n",
      "Example interpretation of option C:\n",
      "Word 1: 15% attention\n",
      "Word 2: 55% attention\n",
      "Word 3: 30% attention\n",
      "Total: 100% attention\n"
     ]
    }
   ],
   "source": [
    "# Attention Scores Analysis - Quiz Question\n",
    "print(\"ATTENTION SCORES IN TRANSFORMER MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Question: Which of the following sets of scores can be attention scores for a word in a sentence?\")\n",
    "print(\"\\nOptions:\")\n",
    "print(\"A) [0.15, 0.50, -0.35]\")\n",
    "print(\"B) [0.15, 0.55, 0.4]\") \n",
    "print(\"C) [0.15, 0.55, 0.3]\")\n",
    "print(\"D) [1.45, 2.34, 0.88]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ATTENTION SCORES PROPERTIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Key Requirements for Attention Scores:\")\n",
    "print(\"1. All values must be NON-NEGATIVE (â‰¥ 0)\")\n",
    "print(\"2. All values must sum to EXACTLY 1.0\")\n",
    "print(\"3. They represent probability distributions\")\n",
    "print(\"4. Each score shows how much attention to pay to each position\")\n",
    "\n",
    "print(\"\\nAnalyzing each option:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Option A\n",
    "option_a = [0.15, 0.50, -0.35]\n",
    "sum_a = sum(option_a)\n",
    "has_negative_a = any(x < 0 for x in option_a)\n",
    "print(f\"A) {option_a}\")\n",
    "print(f\"   Sum: {sum_a}\")\n",
    "print(f\"   Has negative values: {has_negative_a}\")\n",
    "print(f\"   Valid: {not has_negative_a and abs(sum_a - 1.0) < 0.001}\")\n",
    "\n",
    "# Option B  \n",
    "option_b = [0.15, 0.55, 0.4]\n",
    "sum_b = sum(option_b)\n",
    "has_negative_b = any(x < 0 for x in option_b)\n",
    "print(f\"\\nB) {option_b}\")\n",
    "print(f\"   Sum: {sum_b}\")\n",
    "print(f\"   Has negative values: {has_negative_b}\")\n",
    "print(f\"   Valid: {not has_negative_b and abs(sum_b - 1.0) < 0.001}\")\n",
    "\n",
    "# Option C\n",
    "option_c = [0.15, 0.55, 0.3]\n",
    "sum_c = sum(option_c)\n",
    "has_negative_c = any(x < 0 for x in option_c)\n",
    "print(f\"\\nC) {option_c}\")\n",
    "print(f\"   Sum: {sum_c}\")\n",
    "print(f\"   Has negative values: {has_negative_c}\")\n",
    "print(f\"   Valid: {not has_negative_c and abs(sum_c - 1.0) < 0.001}\")\n",
    "\n",
    "# Option D\n",
    "option_d = [1.45, 2.34, 0.88]\n",
    "sum_d = sum(option_d)\n",
    "has_negative_d = any(x < 0 for x in option_d)\n",
    "print(f\"\\nD) {option_d}\")\n",
    "print(f\"   Sum: {sum_d}\")\n",
    "print(f\"   Has negative values: {has_negative_d}\")\n",
    "print(f\"   Valid: {not has_negative_d and abs(sum_d - 1.0) < 0.001}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ANSWER ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"CORRECT ANSWER: C) [0.15, 0.55, 0.3]\")\n",
    "print(\"\\nReasoning:\")\n",
    "print(\"âœ“ All values are non-negative\")\n",
    "print(\"âœ“ Sum equals 1.0 exactly (0.15 + 0.55 + 0.3 = 1.0)\")\n",
    "print(\"âœ“ Represents valid probability distribution\")\n",
    "\n",
    "print(\"\\nWhy other options are wrong:\")\n",
    "print(\"A) Contains negative value (-0.35) - impossible for attention\")\n",
    "print(\"B) Sum = 1.1 (not 1.0) - violates probability constraint\") \n",
    "print(\"D) Sum = 4.67 (not 1.0) - violates probability constraint\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ATTENTION MECHANISM CONTEXT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"In Transformer models:\")\n",
    "print(\"â€¢ Attention scores are computed using softmax function\")\n",
    "print(\"â€¢ Softmax ensures all outputs are positive and sum to 1\")\n",
    "print(\"â€¢ Each score represents 'how much to focus' on each word\")\n",
    "print(\"â€¢ Higher scores = more attention to that position\")\n",
    "print(\"â€¢ Lower scores = less attention to that position\")\n",
    "\n",
    "print(f\"\\nExample interpretation of option C:\")\n",
    "print(f\"Word 1: {option_c[0]*100:.0f}% attention\")\n",
    "print(f\"Word 2: {option_c[1]*100:.0f}% attention\") \n",
    "print(f\"Word 3: {option_c[2]*100:.0f}% attention\")\n",
    "print(f\"Total: {sum(option_c)*100:.0f}% attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13adb2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Components - Information Flow Analysis\n",
    "print(\"TRANSFORMER ENCODER BLOCK COMPONENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Question: Which component improves the flow of information through the Encoder?\")\n",
    "print(\"\\nTypical Encoder Block Components:\")\n",
    "print(\"1. Multi-Head Self-Attention\")\n",
    "print(\"2. Layer Normalization\")\n",
    "print(\"3. Feed-Forward Neural Network (FFN)\")\n",
    "print(\"4. Residual Connections (Skip Connections)\")\n",
    "print(\"5. Dropout\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"COMPONENT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "components = {\n",
    "    \"Multi-Head Self-Attention\": {\n",
    "        \"purpose\": \"Captures relationships between tokens\",\n",
    "        \"info_flow\": \"Enables parallel processing of all positions\",\n",
    "        \"key_benefit\": \"Allows model to focus on relevant parts\"\n",
    "    },\n",
    "    \"Layer Normalization\": {\n",
    "        \"purpose\": \"Normalizes inputs to each sub-layer\",\n",
    "        \"info_flow\": \"Stabilizes training and gradients\",\n",
    "        \"key_benefit\": \"Prevents vanishing/exploding gradients\"\n",
    "    },\n",
    "    \"Feed-Forward Network\": {\n",
    "        \"purpose\": \"Applies non-linear transformations\",\n",
    "        \"info_flow\": \"Processes information independently per position\",\n",
    "        \"key_benefit\": \"Adds computational capacity and non-linearity\"\n",
    "    },\n",
    "    \"Residual Connections\": {\n",
    "        \"purpose\": \"Connects input directly to output\",\n",
    "        \"info_flow\": \"DRAMATICALLY IMPROVES INFORMATION FLOW\",\n",
    "        \"key_benefit\": \"Solves vanishing gradient problem in deep networks\"\n",
    "    },\n",
    "    \"Dropout\": {\n",
    "        \"purpose\": \"Regularization technique\",\n",
    "        \"info_flow\": \"Prevents overfitting during training\",\n",
    "        \"key_benefit\": \"Improves generalization\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for component, details in components.items():\n",
    "    print(f\"\\n{component}:\")\n",
    "    print(f\"  Purpose: {details['purpose']}\")\n",
    "    print(f\"  Info Flow: {details['info_flow']}\")\n",
    "    print(f\"  Key Benefit: {details['key_benefit']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ANSWER: RESIDUAL CONNECTIONS (SKIP CONNECTIONS)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Why Residual Connections improve information flow:\")\n",
    "print(\"âœ“ Allow gradients to flow directly backwards through the network\")\n",
    "print(\"âœ“ Prevent vanishing gradient problem in deep networks\")\n",
    "print(\"âœ“ Enable training of very deep transformer models\")\n",
    "print(\"âœ“ Provide direct paths for information to bypass layers\")\n",
    "print(\"âœ“ Make it easier for the model to learn identity mappings\")\n",
    "\n",
    "print(\"\\nMathematical Representation:\")\n",
    "print(\"Without residual: output = F(x)\")\n",
    "print(\"With residual: output = F(x) + x\")\n",
    "print(\"Where F(x) is the sub-layer function and x is the input\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INFORMATION FLOW COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Without Residual Connections:\")\n",
    "print(\"Input â†’ Layer1 â†’ Layer2 â†’ Layer3 â†’ ... â†’ LayerN â†’ Output\")\n",
    "print(\"â€¢ Information must pass through every layer\")\n",
    "print(\"â€¢ Gradients can diminish through many layers\")\n",
    "print(\"â€¢ Deep networks become hard to train\")\n",
    "\n",
    "print(\"\\nWith Residual Connections:\")\n",
    "print(\"Input â†’ Layer1(+Input) â†’ Layer2(+prev) â†’ Layer3(+prev) â†’ ... â†’ Output\")\n",
    "print(\"â€¢ Information has direct paths through the network\")\n",
    "print(\"â€¢ Gradients can flow directly to earlier layers\")\n",
    "print(\"â€¢ Enables training of very deep networks (100+ layers)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TRANSFORMER ENCODER ARCHITECTURE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Standard Encoder Block Structure:\")\n",
    "print(\"1. Input Embeddings + Positional Encoding\")\n",
    "print(\"2. Multi-Head Self-Attention\")\n",
    "print(\"3. Add & Norm (Residual Connection + Layer Norm)\")\n",
    "print(\"4. Feed-Forward Network\") \n",
    "print(\"5. Add & Norm (Residual Connection + Layer Norm)\")\n",
    "print(\"6. Output to next encoder layer\")\n",
    "\n",
    "print(\"\\nThe 'Add & Norm' steps are where residual connections occur:\")\n",
    "print(\"â€¢ 'Add' = Residual Connection (x + sublayer(x))\")\n",
    "print(\"â€¢ 'Norm' = Layer Normalization\")\n",
    "\n",
    "print(\"\\nThis architecture allows Transformers to have 6, 12, or even 24+ layers\")\n",
    "print(\"while maintaining stable training and good information flow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530447a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03548a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD2VEC vs BAG OF WORDS COMPARISON\n",
      "============================================================\n",
      "Question: What is a significant advantage of using Word2Vec over traditional Bag of Words?\n",
      "\n",
      "============================================================\n",
      "BAG OF WORDS (BOW) REPRESENTATION\n",
      "============================================================\n",
      "Example sentences:\n",
      "1. 'The king rules the kingdom'\n",
      "2. 'The queen governs the nation'\n",
      "3. 'I love programming in Python'\n",
      "4. 'Python is a great programming language'\n",
      "\n",
      "Vocabulary: ['a', 'governs', 'great', 'i', 'in', 'is', 'king', 'kingdom', 'language', 'love', 'nation', 'programming', 'python', 'queen', 'rules', 'the']\n",
      "Vocabulary size: 16\n",
      "\n",
      "Bag of Words Representation:\n",
      "Each sentence becomes a vector counting word occurrences\n",
      "Sentence 1: [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 2]\n",
      "Sentence 2: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2]\n",
      "Sentence 3: [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n",
      "Sentence 4: [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
      "\n",
      "============================================================\n",
      "BOW LIMITATIONS\n",
      "============================================================\n",
      "1. LOSS OF SEMANTIC MEANING:\n",
      "   â€¢ 'king' and 'queen' are treated as completely different\n",
      "   â€¢ No understanding that they're both royal titles\n",
      "   â€¢ Words with similar meanings have no relationship\n",
      "\n",
      "2. CURSE OF DIMENSIONALITY:\n",
      "   â€¢ Vector size = vocabulary size (can be 100,000+ words)\n",
      "   â€¢ Very sparse vectors (mostly zeros)\n",
      "   â€¢ High computational cost\n",
      "\n",
      "3. NO WORD ORDER:\n",
      "   â€¢ 'king rules kingdom' = 'kingdom king rules'\n",
      "   â€¢ Loses syntactic information\n",
      "\n",
      "4. NO CONTEXT UNDERSTANDING:\n",
      "   â€¢ 'bank' (financial) vs 'bank' (river) treated identically\n",
      "   â€¢ Cannot distinguish word meanings in different contexts\n",
      "\n",
      "============================================================\n",
      "WORD2VEC ADVANTAGES\n",
      "============================================================\n",
      "ðŸŽ¯ SIGNIFICANT ADVANTAGE: SEMANTIC RELATIONSHIPS\n",
      "\n",
      "Word2Vec captures semantic meaning through dense vector representations:\n",
      "\n",
      "1. SEMANTIC SIMILARITY:\n",
      "   â€¢ Similar words have similar vectors\n",
      "   â€¢ vector('king') â‰ˆ vector('queen')\n",
      "   â€¢ vector('programming') â‰ˆ vector('coding')\n",
      "\n",
      "2. MATHEMATICAL WORD RELATIONSHIPS:\n",
      "   â€¢ Famous example: king - man + woman â‰ˆ queen\n",
      "   â€¢ Paris - France + Italy â‰ˆ Rome\n",
      "   â€¢ walk - walked + running â‰ˆ ran\n",
      "\n",
      "3. DENSE REPRESENTATIONS:\n",
      "   â€¢ Typical dimension: 100-300 (vs 10,000+ for BOW)\n",
      "   â€¢ Every dimension has meaningful information\n",
      "   â€¢ Much more efficient storage and computation\n",
      "\n",
      "4. CONTEXT AWARENESS:\n",
      "   â€¢ Words appearing in similar contexts get similar vectors\n",
      "   â€¢ Captures distributional hypothesis: 'words that occur in\n",
      "     similar contexts tend to have similar meanings'\n",
      "\n",
      "============================================================\n",
      "PRACTICAL COMPARISON\n",
      "============================================================\n",
      "Example: Measuring similarity between 'king' and 'queen'\n",
      "\n",
      "Bag of Words:\n",
      "king:  [0, 0, 1, 0, 0, 0, 0, 0, 0, ...]  # Only 'king' position = 1\n",
      "queen: [0, 0, 0, 0, 0, 0, 1, 0, 0, ...]  # Only 'queen' position = 1\n",
      "Similarity: 0.0 (completely different)\n",
      "\n",
      "Word2Vec:\n",
      "king:  [0.2, -0.1, 0.8, 0.3, -0.4, ...]  # Dense meaningful vector\n",
      "queen: [0.1, -0.2, 0.7, 0.4, -0.3, ...]  # Similar dense vector\n",
      "Similarity: 0.85 (highly similar)\n",
      "\n",
      "============================================================\n",
      "KEY ADVANTAGES SUMMARY\n",
      "============================================================\n",
      "1. Captures semantic relationships between words\n",
      "2. Enables mathematical operations on word meanings\n",
      "3. Dense representations (efficient storage/computation)\n",
      "4. Handles synonyms and related words effectively\n",
      "5. Better generalization to unseen word combinations\n",
      "6. Enables transfer learning across different tasks\n",
      "7. Captures analogical relationships\n",
      "8. Reduces dimensionality while preserving meaning\n",
      "\n",
      "ðŸ† MOST SIGNIFICANT ADVANTAGE:\n",
      "Word2Vec learns distributed representations that capture\n",
      "semantic relationships, allowing the model to understand\n",
      "that 'king' and 'queen' are related concepts, while BOW\n",
      "treats them as completely independent tokens.\n",
      "\n",
      "============================================================\n",
      "REAL-WORLD IMPACT\n",
      "============================================================\n",
      "Applications where Word2Vec excels over BOW:\n",
      "â€¢ Search engines: Understanding query intent\n",
      "â€¢ Recommendation systems: Finding similar content\n",
      "â€¢ Machine translation: Preserving meaning across languages\n",
      "â€¢ Sentiment analysis: Understanding nuanced emotions\n",
      "â€¢ Document classification: Better feature representations\n",
      "â€¢ Chatbots: Understanding user intent variations\n",
      "\n",
      "Example:\n",
      "User query: 'cheap flights'\n",
      "BOW: Only matches documents with exact words 'cheap' and 'flights'\n",
      "Word2Vec: Also matches 'inexpensive airfare', 'budget airlines', etc.\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec vs Bag of Words - Significant Advantages\n",
    "print(\"WORD2VEC vs BAG OF WORDS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Question: What is a significant advantage of using Word2Vec over traditional Bag of Words?\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BAG OF WORDS (BOW) REPRESENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example sentences for demonstration\n",
    "sentences = [\n",
    "    \"The king rules the kingdom\",\n",
    "    \"The queen governs the nation\",\n",
    "    \"I love programming in Python\",\n",
    "    \"Python is a great programming language\"\n",
    "]\n",
    "\n",
    "print(\"Example sentences:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. '{sentence}'\")\n",
    "\n",
    "# Create vocabulary for BOW\n",
    "vocabulary = set()\n",
    "for sentence in sentences:\n",
    "    words = sentence.lower().split()\n",
    "    vocabulary.update(words)\n",
    "\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "print(f\"\\nVocabulary: {vocabulary}\")\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "print(\"\\nBag of Words Representation:\")\n",
    "print(\"Each sentence becomes a vector counting word occurrences\")\n",
    "\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    words = sentence.lower().split()\n",
    "    bow_vector = []\n",
    "    for vocab_word in vocabulary:\n",
    "        count = words.count(vocab_word)\n",
    "        bow_vector.append(count)\n",
    "    print(f\"Sentence {i}: {bow_vector}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BOW LIMITATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"1. LOSS OF SEMANTIC MEANING:\")\n",
    "print(\"   â€¢ 'king' and 'queen' are treated as completely different\")\n",
    "print(\"   â€¢ No understanding that they're both royal titles\")\n",
    "print(\"   â€¢ Words with similar meanings have no relationship\")\n",
    "\n",
    "print(\"\\n2. CURSE OF DIMENSIONALITY:\")\n",
    "print(\"   â€¢ Vector size = vocabulary size (can be 100,000+ words)\")\n",
    "print(\"   â€¢ Very sparse vectors (mostly zeros)\")\n",
    "print(\"   â€¢ High computational cost\")\n",
    "\n",
    "print(\"\\n3. NO WORD ORDER:\")\n",
    "print(\"   â€¢ 'king rules kingdom' = 'kingdom king rules'\")\n",
    "print(\"   â€¢ Loses syntactic information\")\n",
    "\n",
    "print(\"\\n4. NO CONTEXT UNDERSTANDING:\")\n",
    "print(\"   â€¢ 'bank' (financial) vs 'bank' (river) treated identically\")\n",
    "print(\"   â€¢ Cannot distinguish word meanings in different contexts\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WORD2VEC ADVANTAGES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"ðŸŽ¯ SIGNIFICANT ADVANTAGE: SEMANTIC RELATIONSHIPS\")\n",
    "print(\"\\nWord2Vec captures semantic meaning through dense vector representations:\")\n",
    "\n",
    "print(\"\\n1. SEMANTIC SIMILARITY:\")\n",
    "print(\"   â€¢ Similar words have similar vectors\")\n",
    "print(\"   â€¢ vector('king') â‰ˆ vector('queen')\")\n",
    "print(\"   â€¢ vector('programming') â‰ˆ vector('coding')\")\n",
    "\n",
    "print(\"\\n2. MATHEMATICAL WORD RELATIONSHIPS:\")\n",
    "print(\"   â€¢ Famous example: king - man + woman â‰ˆ queen\")\n",
    "print(\"   â€¢ Paris - France + Italy â‰ˆ Rome\")\n",
    "print(\"   â€¢ walk - walked + running â‰ˆ ran\")\n",
    "\n",
    "print(\"\\n3. DENSE REPRESENTATIONS:\")\n",
    "print(\"   â€¢ Typical dimension: 100-300 (vs 10,000+ for BOW)\")\n",
    "print(\"   â€¢ Every dimension has meaningful information\")\n",
    "print(\"   â€¢ Much more efficient storage and computation\")\n",
    "\n",
    "print(\"\\n4. CONTEXT AWARENESS:\")\n",
    "print(\"   â€¢ Words appearing in similar contexts get similar vectors\")\n",
    "print(\"   â€¢ Captures distributional hypothesis: 'words that occur in\")\n",
    "print(\"     similar contexts tend to have similar meanings'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PRACTICAL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulated example showing the difference\n",
    "print(\"Example: Measuring similarity between 'king' and 'queen'\")\n",
    "print(\"\\nBag of Words:\")\n",
    "print(\"king:  [0, 0, 1, 0, 0, 0, 0, 0, 0, ...]  # Only 'king' position = 1\")\n",
    "print(\"queen: [0, 0, 0, 0, 0, 0, 1, 0, 0, ...]  # Only 'queen' position = 1\")\n",
    "print(\"Similarity: 0.0 (completely different)\")\n",
    "\n",
    "print(\"\\nWord2Vec:\")\n",
    "print(\"king:  [0.2, -0.1, 0.8, 0.3, -0.4, ...]  # Dense meaningful vector\")\n",
    "print(\"queen: [0.1, -0.2, 0.7, 0.4, -0.3, ...]  # Similar dense vector\")\n",
    "print(\"Similarity: 0.85 (highly similar)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY ADVANTAGES SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "advantages = [\n",
    "    \"Captures semantic relationships between words\",\n",
    "    \"Enables mathematical operations on word meanings\",\n",
    "    \"Dense representations (efficient storage/computation)\",\n",
    "    \"Handles synonyms and related words effectively\",\n",
    "    \"Better generalization to unseen word combinations\",\n",
    "    \"Enables transfer learning across different tasks\",\n",
    "    \"Captures analogical relationships\",\n",
    "    \"Reduces dimensionality while preserving meaning\"\n",
    "]\n",
    "\n",
    "for i, advantage in enumerate(advantages, 1):\n",
    "    print(f\"{i}. {advantage}\")\n",
    "\n",
    "print(f\"\\nðŸ† MOST SIGNIFICANT ADVANTAGE:\")\n",
    "print(\"Word2Vec learns distributed representations that capture\")\n",
    "print(\"semantic relationships, allowing the model to understand\")\n",
    "print(\"that 'king' and 'queen' are related concepts, while BOW\")\n",
    "print(\"treats them as completely independent tokens.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REAL-WORLD IMPACT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Applications where Word2Vec excels over BOW:\")\n",
    "print(\"â€¢ Search engines: Understanding query intent\")\n",
    "print(\"â€¢ Recommendation systems: Finding similar content\")\n",
    "print(\"â€¢ Machine translation: Preserving meaning across languages\")\n",
    "print(\"â€¢ Sentiment analysis: Understanding nuanced emotions\")\n",
    "print(\"â€¢ Document classification: Better feature representations\")\n",
    "print(\"â€¢ Chatbots: Understanding user intent variations\")\n",
    "\n",
    "print(\"\\nExample:\")\n",
    "print(\"User query: 'cheap flights'\")\n",
    "print(\"BOW: Only matches documents with exact words 'cheap' and 'flights'\")\n",
    "print(\"Word2Vec: Also matches 'inexpensive airfare', 'budget airlines', etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explaining Word2Vec Averaging Code Statement\n",
    "print(\"WORD2VEC FEATURE VECTOR AVERAGING EXPLANATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Code Statement:\")\n",
    "print(\"if len(words_in_vocab) != 0:\")\n",
    "print(\"    feature_vector /= len(words_in_vocab)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONTEXT: DOCUMENT/SENTENCE REPRESENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"This code is typically used when converting a document or sentence\")\n",
    "print(\"into a single vector representation using Word2Vec embeddings.\")\n",
    "\n",
    "print(\"\\nProcess Overview:\")\n",
    "print(\"1. Start with a document containing multiple words\")\n",
    "print(\"2. Get Word2Vec embedding for each word\")\n",
    "print(\"3. Sum all word vectors together\")\n",
    "print(\"4. Divide by number of words (AVERAGING)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LINE-BY-LINE BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"if len(words_in_vocab) != 0:\")\n",
    "print(\"â”œâ”€ Check if we found any words in the vocabulary\")\n",
    "print(\"â”œâ”€ Prevents division by zero error\")\n",
    "print(\"â””â”€ Only proceed if at least one word was found\")\n",
    "\n",
    "print(\"\\nfeature_vector /= len(words_in_vocab)\")\n",
    "print(\"â”œâ”€ This is shorthand for: feature_vector = feature_vector / len(words_in_vocab)\")\n",
    "print(\"â”œâ”€ Divides the summed vector by the number of words\")\n",
    "print(\"â””â”€ Converts sum into average (mean) vector\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PRACTICAL EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate the process\n",
    "import numpy as np\n",
    "\n",
    "print(\"Example: Converting sentence 'I love programming' to single vector\")\n",
    "print(\"\\nStep 1: Individual word vectors (simulated 3D vectors)\")\n",
    "word_vectors = {\n",
    "    'i': np.array([0.1, 0.2, 0.3]),\n",
    "    'love': np.array([0.5, 0.6, 0.2]),  \n",
    "    'programming': np.array([0.3, 0.1, 0.8])\n",
    "}\n",
    "\n",
    "sentence = \"I love programming\"\n",
    "words = sentence.lower().split()\n",
    "print(f\"Words: {words}\")\n",
    "\n",
    "print(\"\\nStep 2: Word vectors\")\n",
    "for word in words:\n",
    "    if word in word_vectors:\n",
    "        print(f\"'{word}': {word_vectors[word]}\")\n",
    "\n",
    "print(\"\\nStep 3: Sum all vectors\")\n",
    "feature_vector = np.zeros(3)\n",
    "words_in_vocab = []\n",
    "\n",
    "for word in words:\n",
    "    if word in word_vectors:  # Check if word exists in vocabulary\n",
    "        feature_vector += word_vectors[word]\n",
    "        words_in_vocab.append(word)\n",
    "        print(f\"After adding '{word}': {feature_vector}\")\n",
    "\n",
    "print(f\"\\nStep 4: Check vocabulary coverage\")\n",
    "print(f\"words_in_vocab = {words_in_vocab}\")\n",
    "print(f\"len(words_in_vocab) = {len(words_in_vocab)}\")\n",
    "\n",
    "print(f\"\\nStep 5: Apply the averaging code\")\n",
    "print(f\"Before averaging: feature_vector = {feature_vector}\")\n",
    "\n",
    "if len(words_in_vocab) != 0:\n",
    "    feature_vector /= len(words_in_vocab)\n",
    "    print(f\"After averaging: feature_vector = {feature_vector}\")\n",
    "    print(f\"(Divided by {len(words_in_vocab)} words)\")\n",
    "else:\n",
    "    print(\"No words found in vocabulary - would skip averaging\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHY AVERAGING IS IMPORTANT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"1. FAIR COMPARISON:\")\n",
    "print(\"   â€¢ Documents with different lengths can be compared\")\n",
    "print(\"   â€¢ Prevents longer documents from having artificially larger vectors\")\n",
    "\n",
    "print(\"\\n2. SEMANTIC REPRESENTATION:\")\n",
    "print(\"   â€¢ Average captures the overall meaning of the document\")\n",
    "print(\"   â€¢ Each dimension represents average 'amount' of that semantic feature\")\n",
    "\n",
    "print(\"\\n3. NORMALIZED SCALE:\")\n",
    "print(\"   â€¢ Keeps vector magnitudes reasonable\")\n",
    "print(\"   â€¢ Prevents numerical overflow in calculations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON: SUM vs AVERAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Demonstrate difference\n",
    "doc1_words = ['good', 'movie']\n",
    "doc2_words = ['good', 'excellent', 'amazing', 'movie']\n",
    "\n",
    "# Simulated vectors\n",
    "vectors = {\n",
    "    'good': np.array([0.8, 0.2]),\n",
    "    'excellent': np.array([0.9, 0.1]), \n",
    "    'amazing': np.array([0.9, 0.0]),\n",
    "    'movie': np.array([0.1, 0.8])\n",
    "}\n",
    "\n",
    "print(\"Document 1: 'good movie'\")\n",
    "print(\"Document 2: 'good excellent amazing movie'\")\n",
    "\n",
    "for doc_num, words in enumerate([doc1_words, doc2_words], 1):\n",
    "    sum_vector = np.zeros(2)\n",
    "    count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vectors:\n",
    "            sum_vector += vectors[word]\n",
    "            count += 1\n",
    "    \n",
    "    avg_vector = sum_vector / count if count > 0 else sum_vector\n",
    "    \n",
    "    print(f\"\\nDocument {doc_num}:\")\n",
    "    print(f\"  Sum vector: {sum_vector}\")\n",
    "    print(f\"  Average vector: {avg_vector}\")\n",
    "    print(f\"  Words count: {count}\")\n",
    "\n",
    "print(\"\\nWithout averaging:\")\n",
    "print(\"â€¢ Document 2 would have much larger values\")\n",
    "print(\"â€¢ Similarity comparisons would be biased toward longer documents\")\n",
    "\n",
    "print(\"\\nWith averaging:\")\n",
    "print(\"â€¢ Both documents have comparable scales\")\n",
    "print(\"â€¢ Fair comparison of semantic content\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ERROR PREVENTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"The 'if len(words_in_vocab) != 0:' check prevents:\")\n",
    "print(\"1. Division by zero error\")\n",
    "print(\"2. Runtime crashes\")\n",
    "print(\"3. Invalid vector results\")\n",
    "\n",
    "print(\"\\nScenarios where this matters:\")\n",
    "print(\"â€¢ Document contains only out-of-vocabulary words\")\n",
    "print(\"â€¢ Empty document\")\n",
    "print(\"â€¢ Document with only punctuation/stopwords\")\n",
    "\n",
    "print(\"\\nExample of what happens without the check:\")\n",
    "try:\n",
    "    empty_vector = np.array([1.0, 2.0, 3.0])\n",
    "    empty_vector /= 0  # This would cause an error\n",
    "except Exception as e:\n",
    "    print(f\"Error without check: {e}\")\n",
    "\n",
    "print(\"\\nWith the check:\")\n",
    "words_in_vocab_empty = []\n",
    "if len(words_in_vocab_empty) != 0:\n",
    "    print(\"Would perform averaging\")\n",
    "else:\n",
    "    print(\"âœ“ Safely skipped averaging - no words found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4518bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENTION MECHANISM IN CUSTOMER CHURN PREDICTION\n",
      "============================================================\n",
      "Question: You are building a model to predict customer churn for a telecom company.\n",
      "The model will take as input a customer's recent call data and predict whether\n",
      "the customer is likely to churn or not. You decide to use an attention-based model\n",
      "to help the model focus on the most important parts of the customer's call data.\n",
      "\n",
      "Which of the following statements best describes the role of the attention mechanism in this model?\n",
      "\n",
      "============================================================\n",
      "ANSWER ANALYSIS\n",
      "============================================================\n",
      "CORRECT ANSWER: A) The attention mechanism is used to weight different parts\n",
      "of the input data, so that the most important parts are given more influence\n",
      "in the prediction\n",
      "\n",
      "============================================================\n",
      "WHY OPTION A IS CORRECT\n",
      "============================================================\n",
      "1. CORE FUNCTION OF ATTENTION:\n",
      "   â€¢ Attention mechanisms assign different weights to different input features\n",
      "   â€¢ Higher weights = more importance in final prediction\n",
      "   â€¢ Lower weights = less influence on prediction\n",
      "\n",
      "2. CUSTOMER CHURN CONTEXT:\n",
      "   â€¢ Call data contains many features (duration, frequency, time, etc.)\n",
      "   â€¢ Some features are more predictive of churn than others\n",
      "   â€¢ Attention helps identify which features matter most\n",
      "\n",
      "3. WEIGHTING MECHANISM:\n",
      "   â€¢ Each input feature gets an attention score (0 to 1)\n",
      "   â€¢ Scores determine how much each feature contributes\n",
      "   â€¢ Model learns which patterns indicate churn risk\n",
      "\n",
      "============================================================\n",
      "PRACTICAL EXAMPLE: CUSTOMER CALL DATA\n",
      "============================================================\n",
      "Example: Customer call data features and their attention weights\n",
      "\n",
      "Customer Call Features:\n",
      "Feature                   | Attention Weight | Importance\n",
      "-----------------------------------------------------------------\n",
      "Call duration average     | 0.05            | LOW\n",
      "Call frequency per week   | 0.15            | MEDIUM\n",
      "Late night calls count    | 0.08            | LOW\n",
      "Customer service calls    | 0.25            | HIGH\n",
      "Missed call percentage    | 0.20            | HIGH\n",
      "International calls count | 0.03            | LOW\n",
      "Weekend call ratio        | 0.04            | LOW\n",
      "Bill payment delays       | 0.20            | HIGH\n",
      "\n",
      "Total attention weights: 1.00 (should sum to 1.0)\n",
      "\n",
      "============================================================\n",
      "HOW ATTENTION IMPROVES CHURN PREDICTION\n",
      "============================================================\n",
      "Without Attention:\n",
      "â€¢ All features treated equally\n",
      "â€¢ Model might focus on irrelevant patterns\n",
      "â€¢ Less accurate predictions\n",
      "\n",
      "With Attention:\n",
      "â€¢ Model learns that 'Customer service calls' (0.25) is very important\n",
      "â€¢ 'Bill payment delays' (0.20) also highly weighted\n",
      "â€¢ 'International calls' (0.03) given low importance\n",
      "â€¢ More accurate and interpretable predictions\n",
      "\n",
      "============================================================\n",
      "WHY OTHER OPTIONS ARE WRONG\n",
      "============================================================\n",
      "B) 'Measure similarity between different parts of input data'\n",
      "   âŒ Wrong: Attention weights importance, not similarity\n",
      "   âŒ This describes similarity metrics, not attention mechanism\n",
      "\n",
      "C) 'Transform input data into a more useful representation'\n",
      "   âŒ Wrong: This describes feature transformation/embedding\n",
      "   âŒ Attention weights existing features, doesn't transform them\n",
      "\n",
      "D) 'Randomly select parts of input data for predictions'\n",
      "   âŒ Wrong: Attention is learned, not random\n",
      "   âŒ Selection is based on importance, not randomness\n",
      "\n",
      "============================================================\n",
      "ATTENTION MECHANISM MATHEMATICS\n",
      "============================================================\n",
      "Attention-weighted prediction:\n",
      "prediction = Î£(attention_weight_i Ã— feature_i)\n",
      "\n",
      "Where:\n",
      "â€¢ attention_weight_i: learned importance of feature i\n",
      "â€¢ feature_i: value of feature i\n",
      "â€¢ Î£: sum over all features\n",
      "\n",
      "Example calculation:\n",
      "Customer risk score: 9.67\n",
      "\n",
      "============================================================\n",
      "BUSINESS VALUE OF ATTENTION IN CHURN PREDICTION\n",
      "============================================================\n",
      "1. INTERPRETABILITY:\n",
      "   â€¢ Shows which factors most influence churn risk\n",
      "   â€¢ Helps business understand customer behavior\n",
      "   â€¢ Enables targeted retention strategies\n",
      "\n",
      "2. IMPROVED ACCURACY:\n",
      "   â€¢ Focuses on most predictive features\n",
      "   â€¢ Reduces noise from irrelevant data\n",
      "   â€¢ Better identification of at-risk customers\n",
      "\n",
      "3. ACTIONABLE INSIGHTS:\n",
      "   â€¢ High attention on 'customer service calls' â†’ improve support\n",
      "   â€¢ High attention on 'payment delays' â†’ offer payment plans\n",
      "   â€¢ Low attention on 'international calls' â†’ not a churn indicator\n",
      "\n",
      "ðŸŽ¯ KEY TAKEAWAY:\n",
      "Attention mechanism assigns importance weights to input features,\n",
      "ensuring the most predictive parts of customer data have the\n",
      "greatest influence on churn predictions.\n",
      "Example: Customer call data features and their attention weights\n",
      "\n",
      "Customer Call Features:\n",
      "Feature                   | Attention Weight | Importance\n",
      "-----------------------------------------------------------------\n",
      "Call duration average     | 0.05            | LOW\n",
      "Call frequency per week   | 0.15            | MEDIUM\n",
      "Late night calls count    | 0.08            | LOW\n",
      "Customer service calls    | 0.25            | HIGH\n",
      "Missed call percentage    | 0.20            | HIGH\n",
      "International calls count | 0.03            | LOW\n",
      "Weekend call ratio        | 0.04            | LOW\n",
      "Bill payment delays       | 0.20            | HIGH\n",
      "\n",
      "Total attention weights: 1.00 (should sum to 1.0)\n",
      "\n",
      "============================================================\n",
      "HOW ATTENTION IMPROVES CHURN PREDICTION\n",
      "============================================================\n",
      "Without Attention:\n",
      "â€¢ All features treated equally\n",
      "â€¢ Model might focus on irrelevant patterns\n",
      "â€¢ Less accurate predictions\n",
      "\n",
      "With Attention:\n",
      "â€¢ Model learns that 'Customer service calls' (0.25) is very important\n",
      "â€¢ 'Bill payment delays' (0.20) also highly weighted\n",
      "â€¢ 'International calls' (0.03) given low importance\n",
      "â€¢ More accurate and interpretable predictions\n",
      "\n",
      "============================================================\n",
      "WHY OTHER OPTIONS ARE WRONG\n",
      "============================================================\n",
      "B) 'Measure similarity between different parts of input data'\n",
      "   âŒ Wrong: Attention weights importance, not similarity\n",
      "   âŒ This describes similarity metrics, not attention mechanism\n",
      "\n",
      "C) 'Transform input data into a more useful representation'\n",
      "   âŒ Wrong: This describes feature transformation/embedding\n",
      "   âŒ Attention weights existing features, doesn't transform them\n",
      "\n",
      "D) 'Randomly select parts of input data for predictions'\n",
      "   âŒ Wrong: Attention is learned, not random\n",
      "   âŒ Selection is based on importance, not randomness\n",
      "\n",
      "============================================================\n",
      "ATTENTION MECHANISM MATHEMATICS\n",
      "============================================================\n",
      "Attention-weighted prediction:\n",
      "prediction = Î£(attention_weight_i Ã— feature_i)\n",
      "\n",
      "Where:\n",
      "â€¢ attention_weight_i: learned importance of feature i\n",
      "â€¢ feature_i: value of feature i\n",
      "â€¢ Î£: sum over all features\n",
      "\n",
      "Example calculation:\n",
      "Customer risk score: 9.67\n",
      "\n",
      "============================================================\n",
      "BUSINESS VALUE OF ATTENTION IN CHURN PREDICTION\n",
      "============================================================\n",
      "1. INTERPRETABILITY:\n",
      "   â€¢ Shows which factors most influence churn risk\n",
      "   â€¢ Helps business understand customer behavior\n",
      "   â€¢ Enables targeted retention strategies\n",
      "\n",
      "2. IMPROVED ACCURACY:\n",
      "   â€¢ Focuses on most predictive features\n",
      "   â€¢ Reduces noise from irrelevant data\n",
      "   â€¢ Better identification of at-risk customers\n",
      "\n",
      "3. ACTIONABLE INSIGHTS:\n",
      "   â€¢ High attention on 'customer service calls' â†’ improve support\n",
      "   â€¢ High attention on 'payment delays' â†’ offer payment plans\n",
      "   â€¢ Low attention on 'international calls' â†’ not a churn indicator\n",
      "\n",
      "ðŸŽ¯ KEY TAKEAWAY:\n",
      "Attention mechanism assigns importance weights to input features,\n",
      "ensuring the most predictive parts of customer data have the\n",
      "greatest influence on churn predictions.\n"
     ]
    }
   ],
   "source": [
    "# Attention Mechanism in Customer Churn Prediction - Quiz Analysis\n",
    "print(\"ATTENTION MECHANISM IN CUSTOMER CHURN PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Question: You are building a model to predict customer churn for a telecom company.\")\n",
    "print(\"The model will take as input a customer's recent call data and predict whether\")\n",
    "print(\"the customer is likely to churn or not. You decide to use an attention-based model\")\n",
    "print(\"to help the model focus on the most important parts of the customer's call data.\")\n",
    "print(\"\\nWhich of the following statements best describes the role of the attention mechanism in this model?\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANSWER ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"CORRECT ANSWER: A) The attention mechanism is used to weight different parts\")\n",
    "print(\"of the input data, so that the most important parts are given more influence\")\n",
    "print(\"in the prediction\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHY OPTION A IS CORRECT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"1. CORE FUNCTION OF ATTENTION:\")\n",
    "print(\"   â€¢ Attention mechanisms assign different weights to different input features\")\n",
    "print(\"   â€¢ Higher weights = more importance in final prediction\")\n",
    "print(\"   â€¢ Lower weights = less influence on prediction\")\n",
    "\n",
    "print(\"\\n2. CUSTOMER CHURN CONTEXT:\")\n",
    "print(\"   â€¢ Call data contains many features (duration, frequency, time, etc.)\")\n",
    "print(\"   â€¢ Some features are more predictive of churn than others\")\n",
    "print(\"   â€¢ Attention helps identify which features matter most\")\n",
    "\n",
    "print(\"\\n3. WEIGHTING MECHANISM:\")\n",
    "print(\"   â€¢ Each input feature gets an attention score (0 to 1)\")\n",
    "print(\"   â€¢ Scores determine how much each feature contributes\")\n",
    "print(\"   â€¢ Model learns which patterns indicate churn risk\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PRACTICAL EXAMPLE: CUSTOMER CALL DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate customer call data and attention weights\n",
    "import numpy as np\n",
    "\n",
    "print(\"Example: Customer call data features and their attention weights\")\n",
    "print(\"\\nCustomer Call Features:\")\n",
    "features = [\n",
    "    \"Call duration average\",\n",
    "    \"Call frequency per week\", \n",
    "    \"Late night calls count\",\n",
    "    \"Customer service calls\",\n",
    "    \"Missed call percentage\",\n",
    "    \"International calls count\",\n",
    "    \"Weekend call ratio\",\n",
    "    \"Bill payment delays\"\n",
    "]\n",
    "\n",
    "# Simulated attention weights (higher = more important for churn prediction)\n",
    "attention_weights = [0.05, 0.15, 0.08, 0.25, 0.20, 0.03, 0.04, 0.20]\n",
    "\n",
    "print(f\"{'Feature':<25} | {'Attention Weight':<15} | {'Importance'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for feature, weight in zip(features, attention_weights):\n",
    "    importance = \"HIGH\" if weight > 0.15 else \"MEDIUM\" if weight > 0.08 else \"LOW\"\n",
    "    print(f\"{feature:<25} | {weight:<15.2f} | {importance}\")\n",
    "\n",
    "print(f\"\\nTotal attention weights: {sum(attention_weights):.2f} (should sum to 1.0)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HOW ATTENTION IMPROVES CHURN PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Without Attention:\")\n",
    "print(\"â€¢ All features treated equally\")\n",
    "print(\"â€¢ Model might focus on irrelevant patterns\")\n",
    "print(\"â€¢ Less accurate predictions\")\n",
    "\n",
    "print(\"\\nWith Attention:\")\n",
    "print(\"â€¢ Model learns that 'Customer service calls' (0.25) is very important\")\n",
    "print(\"â€¢ 'Bill payment delays' (0.20) also highly weighted\")\n",
    "print(\"â€¢ 'International calls' (0.03) given low importance\")\n",
    "print(\"â€¢ More accurate and interpretable predictions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHY OTHER OPTIONS ARE WRONG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"B) 'Measure similarity between different parts of input data'\")\n",
    "print(\"   âŒ Wrong: Attention weights importance, not similarity\")\n",
    "print(\"   âŒ This describes similarity metrics, not attention mechanism\")\n",
    "\n",
    "print(\"\\nC) 'Transform input data into a more useful representation'\")\n",
    "print(\"   âŒ Wrong: This describes feature transformation/embedding\")\n",
    "print(\"   âŒ Attention weights existing features, doesn't transform them\")\n",
    "\n",
    "print(\"\\nD) 'Randomly select parts of input data for predictions'\")\n",
    "print(\"   âŒ Wrong: Attention is learned, not random\")\n",
    "print(\"   âŒ Selection is based on importance, not randomness\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ATTENTION MECHANISM MATHEMATICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Attention-weighted prediction:\")\n",
    "print(\"prediction = Î£(attention_weight_i Ã— feature_i)\")\n",
    "print(\"\\nWhere:\")\n",
    "print(\"â€¢ attention_weight_i: learned importance of feature i\")\n",
    "print(\"â€¢ feature_i: value of feature i\")\n",
    "print(\"â€¢ Î£: sum over all features\")\n",
    "\n",
    "print(\"\\nExample calculation:\")\n",
    "feature_values = [45.2, 12, 3, 8, 15.5, 2, 0.3, 1]  # Example customer data\n",
    "\n",
    "weighted_sum = sum(w * v for w, v in zip(attention_weights, feature_values))\n",
    "print(f\"Customer risk score: {weighted_sum:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BUSINESS VALUE OF ATTENTION IN CHURN PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"1. INTERPRETABILITY:\")\n",
    "print(\"   â€¢ Shows which factors most influence churn risk\")\n",
    "print(\"   â€¢ Helps business understand customer behavior\")\n",
    "print(\"   â€¢ Enables targeted retention strategies\")\n",
    "\n",
    "print(\"\\n2. IMPROVED ACCURACY:\")\n",
    "print(\"   â€¢ Focuses on most predictive features\")\n",
    "print(\"   â€¢ Reduces noise from irrelevant data\")\n",
    "print(\"   â€¢ Better identification of at-risk customers\")\n",
    "\n",
    "print(\"\\n3. ACTIONABLE INSIGHTS:\")\n",
    "print(\"   â€¢ High attention on 'customer service calls' â†’ improve support\")\n",
    "print(\"   â€¢ High attention on 'payment delays' â†’ offer payment plans\")\n",
    "print(\"   â€¢ Low attention on 'international calls' â†’ not a churn indicator\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ KEY TAKEAWAY:\")\n",
    "print(\"Attention mechanism assigns importance weights to input features,\")\n",
    "print(\"ensuring the most predictive parts of customer data have the\")\n",
    "print(\"greatest influence on churn predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48479fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Word2Vec Average Vectorizer Function Explanation\n",
    "print(\"WORD2VEC AVERAGE VECTORIZER FUNCTION BREAKDOWN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"Function Definition:\")\n",
    "print(\"def average_vectorizer_Word2Vec(doc):\")\n",
    "print(\"    feature_vector = np.zeros((vec_size,), dtype='float64')\")\n",
    "print(\"    words_in_vocab = [word for word in doc.split() if word in words]\")\n",
    "print(\"    for word in words_in_vocab:\")\n",
    "print(\"        feature_vector += np.array(word_vector_dict[word])\")\n",
    "print(\"    if len(words_in_vocab) != 0:\")\n",
    "print(\"        feature_vector /= len(words_in_vocab)\")\n",
    "print(\"    return feature_vector\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PURPOSE AND CONTEXT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"WHAT IT DOES:\")\n",
    "print(\"â€¢ Converts a text document into a single numerical vector\")\n",
    "print(\"â€¢ Uses pre-trained Word2Vec embeddings\")\n",
    "print(\"â€¢ Creates document-level representation from word-level embeddings\")\n",
    "print(\"â€¢ Enables mathematical operations on documents\")\n",
    "\n",
    "print(\"\\nWHY IT'S NEEDED:\")\n",
    "print(\"â€¢ Word2Vec gives vectors for individual words\")\n",
    "print(\"â€¢ Documents contain multiple words\")\n",
    "print(\"â€¢ Need single vector to represent entire document\")\n",
    "print(\"â€¢ Averaging preserves semantic meaning\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LINE-BY-LINE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"1. FUNCTION SIGNATURE:\")\n",
    "print(\"   def average_vectorizer_Word2Vec(doc):\")\n",
    "print(\"   â”œâ”€ Input: 'doc' (string) - text document to vectorize\")\n",
    "print(\"   â””â”€ Output: numpy array representing document vector\")\n",
    "\n",
    "print(\"\\n2. INITIALIZE ZERO VECTOR:\")\n",
    "print(\"   feature_vector = np.zeros((vec_size,), dtype='float64')\")\n",
    "print(\"   â”œâ”€ Creates empty vector of size 'vec_size' (e.g., 300 dimensions)\")\n",
    "print(\"   â”œâ”€ All values start at 0.0\")\n",
    "print(\"   â”œâ”€ dtype='float64' ensures high precision\")\n",
    "print(\"   â””â”€ Will accumulate word vectors here\")\n",
    "\n",
    "print(\"\\n3. FILTER VOCABULARY WORDS:\")\n",
    "print(\"   words_in_vocab = [word for word in doc.split() if word in words]\")\n",
    "print(\"   â”œâ”€ doc.split() breaks document into individual words\")\n",
    "print(\"   â”œâ”€ 'if word in words' keeps only words in vocabulary\")\n",
    "print(\"   â”œâ”€ Filters out unknown/out-of-vocabulary words\")\n",
    "print(\"   â””â”€ Result: list of words that have embeddings\")\n",
    "\n",
    "print(\"\\n4. ACCUMULATE WORD VECTORS:\")\n",
    "print(\"   for word in words_in_vocab:\")\n",
    "print(\"       feature_vector += np.array(word_vector_dict[word])\")\n",
    "print(\"   â”œâ”€ Loops through each vocabulary word\")\n",
    "print(\"   â”œâ”€ word_vector_dict[word] gets embedding for that word\")\n",
    "print(\"   â”œâ”€ np.array() ensures proper numpy format\")\n",
    "print(\"   â””â”€ += adds each word vector to running sum\")\n",
    "\n",
    "print(\"\\n5. AVERAGE THE VECTORS:\")\n",
    "print(\"   if len(words_in_vocab) != 0:\")\n",
    "print(\"       feature_vector /= len(words_in_vocab)\")\n",
    "print(\"   â”œâ”€ Safety check: only divide if we found words\")\n",
    "print(\"   â”œâ”€ /= divides sum by number of words (averaging)\")\n",
    "print(\"   â””â”€ Prevents division by zero error\")\n",
    "\n",
    "print(\"\\n6. RETURN RESULT:\")\n",
    "print(\"   return feature_vector\")\n",
    "print(\"   â””â”€ Returns averaged document vector\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP-BY-STEP EXAMPLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simulate the function with example data\n",
    "import numpy as np\n",
    "\n",
    "# Mock global variables that would exist in real implementation\n",
    "vec_size = 4  # Simplified to 4 dimensions for example\n",
    "words = {'i', 'love', 'machine', 'learning', 'python', 'programming'}  # Vocabulary\n",
    "word_vector_dict = {\n",
    "    'i': [0.1, 0.2, 0.3, 0.1],\n",
    "    'love': [0.5, 0.6, 0.2, 0.3],\n",
    "    'machine': [0.8, 0.1, 0.7, 0.2],\n",
    "    'learning': [0.7, 0.3, 0.8, 0.4],\n",
    "    'python': [0.3, 0.8, 0.1, 0.6],\n",
    "    'programming': [0.4, 0.7, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "def average_vectorizer_Word2Vec_demo(doc):\n",
    "    print(f\"\\nProcessing document: '{doc}'\")\n",
    "    \n",
    "    # Step 1: Initialize zero vector\n",
    "    feature_vector = np.zeros((vec_size,), dtype=\"float64\")\n",
    "    print(f\"1. Initialize: feature_vector = {feature_vector}\")\n",
    "    \n",
    "    # Step 2: Filter vocabulary words\n",
    "    words_in_vocab = [word for word in doc.split() if word in words]\n",
    "    print(f\"2. Words in vocab: {words_in_vocab}\")\n",
    "    print(f\"   Original words: {doc.split()}\")\n",
    "    print(f\"   Filtered out: {[w for w in doc.split() if w not in words]}\")\n",
    "    \n",
    "    # Step 3: Accumulate word vectors\n",
    "    print(\"3. Adding word vectors:\")\n",
    "    for word in words_in_vocab:\n",
    "        word_vec = np.array(word_vector_dict[word])\n",
    "        print(f\"   '{word}': {word_vec}\")\n",
    "        feature_vector += word_vec\n",
    "        print(f\"   Running sum: {feature_vector}\")\n",
    "    \n",
    "    # Step 4: Average the vectors\n",
    "    print(\"4. Averaging:\")\n",
    "    print(f\"   Before averaging: {feature_vector}\")\n",
    "    print(f\"   Number of words: {len(words_in_vocab)}\")\n",
    "    \n",
    "    if len(words_in_vocab) != 0:\n",
    "        feature_vector /= len(words_in_vocab)\n",
    "        print(f\"   After averaging: {feature_vector}\")\n",
    "    else:\n",
    "        print(\"   No averaging needed (no words found)\")\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "# Example 1: Normal case\n",
    "result1 = average_vectorizer_Word2Vec_demo(\"I love machine learning\")\n",
    "\n",
    "# Example 2: With unknown words\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "result2 = average_vectorizer_Word2Vec_demo(\"I love advanced quantum computing\")\n",
    "\n",
    "# Example 3: All unknown words\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "result3 = average_vectorizer_Word2Vec_demo(\"xyz abc def\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY BENEFITS OF THIS APPROACH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"1. DOCUMENT REPRESENTATION:\")\n",
    "print(\"   â€¢ Converts variable-length documents to fixed-size vectors\")\n",
    "print(\"   â€¢ Enables machine learning algorithms to process text\")\n",
    "print(\"   â€¢ Preserves semantic meaning through averaging\")\n",
    "\n",
    "print(\"\\n2. SEMANTIC PRESERVATION:\")\n",
    "print(\"   â€¢ Documents about similar topics have similar vectors\")\n",
    "print(\"   â€¢ Mathematical relationships between documents become possible\")\n",
    "print(\"   â€¢ Enables document similarity calculations\")\n",
    "\n",
    "print(\"\\n3. ROBUSTNESS:\")\n",
    "print(\"   â€¢ Handles documents of any length\")\n",
    "print(\"   â€¢ Gracefully handles unknown words (filters them out)\")\n",
    "print(\"   â€¢ Prevents errors with empty documents\")\n",
    "\n",
    "print(\"\\n4. COMPUTATIONAL EFFICIENCY:\")\n",
    "print(\"   â€¢ Simple averaging operation\")\n",
    "print(\"   â€¢ Linear time complexity O(n) where n = number of words\")\n",
    "print(\"   â€¢ Memory efficient\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"REAL-WORLD APPLICATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"Document Classification:\")\n",
    "print(\"â€¢ Convert documents to vectors\")\n",
    "print(\"â€¢ Train classifier on these vectors\")\n",
    "print(\"â€¢ Classify new documents\")\n",
    "\n",
    "print(\"\\nDocument Similarity:\")\n",
    "print(\"â€¢ Vectorize two documents\")\n",
    "print(\"â€¢ Calculate cosine similarity\")\n",
    "print(\"â€¢ Measure how similar they are\")\n",
    "\n",
    "print(\"\\nClustering:\")\n",
    "print(\"â€¢ Vectorize collection of documents\")\n",
    "print(\"â€¢ Apply clustering algorithms\")\n",
    "print(\"â€¢ Group similar documents together\")\n",
    "\n",
    "print(\"\\nRecommendation Systems:\")\n",
    "print(\"â€¢ Vectorize user preferences/documents\")\n",
    "print(\"â€¢ Find similar users or content\")\n",
    "print(\"â€¢ Recommend based on similarity\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ASSUMPTIONS AND REQUIREMENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"Global Variables Required:\")\n",
    "print(\"â€¢ vec_size: Dimension of Word2Vec embeddings (e.g., 300)\")\n",
    "print(\"â€¢ words: Set/list of vocabulary words\")\n",
    "print(\"â€¢ word_vector_dict: Dictionary mapping words to vectors\")\n",
    "\n",
    "print(\"\\nAssumptions:\")\n",
    "print(\"â€¢ Word2Vec model has been pre-trained\")\n",
    "print(\"â€¢ word_vector_dict contains embeddings for vocabulary words\")\n",
    "print(\"â€¢ Input documents are already preprocessed (lowercase, tokenized)\")\n",
    "\n",
    "print(\"\\nLimitations:\")\n",
    "print(\"â€¢ Loses word order information\")\n",
    "print(\"â€¢ Equal weight to all words (no TF-IDF weighting)\")\n",
    "print(\"â€¢ Out-of-vocabulary words are ignored\")\n",
    "print(\"â€¢ Assumes pre-computed embeddings\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ SUMMARY:\")\n",
    "print(\"This function is a bridge between word-level and document-level\")\n",
    "print(\"representations, enabling machine learning on text data by\")\n",
    "print(\"converting documents into numerical vectors while preserving\")\n",
    "print(\"semantic meaning through careful averaging of word embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fixed_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
