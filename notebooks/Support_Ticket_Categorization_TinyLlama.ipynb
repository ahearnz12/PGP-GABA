{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmKLmjqmHMQM"
   },
   "source": [
    "<center><p float=\"center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
    "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
    "</p></center>\n",
    "\n",
    "<center><font size=10>Generative AI for Business Applications</center></font>\n",
    "<center><font size=6>Large Language Models & Prompt Engineering - Week 2</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPYAUYpJ5MpY"
   },
   "source": [
    "<center><p float=\"center\">\n",
    "  <img src=\"https://cdn.pixabay.com/photo/2017/07/24/04/23/technical-support-2533526_1280.png\" width=\"480\"/>\n",
    "</p></center>\n",
    "\n",
    "<center><font size=5>Support Ticket Categorization</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Bxnkoxf270y"
   },
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PIXpOjx3DGc"
   },
   "source": [
    "## Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x62rcEKC_B3l"
   },
   "source": [
    "In today‚Äôs dynamic business landscape, organizations recognize the critical role of customer feedback in shaping products and services. Effectively leveraging this feedback enhances customer experiences, drives growth, and fosters long-term relationships. For Product Managers and Analysts, staying aligned with the voice of the customer is a strategic imperative.\n",
    "\n",
    "While organizations receive vast amounts of customer feedback and support tickets, the challenge lies in managing and utilizing this data effectively. A structured approach is essential, one that identifies key issues, prioritizes efficiently, and allocates resources wisely. Implementing a Support Ticket Categorization system is a powerful strategy to meet these needs; without it, teams may struggle to respond promptly to critical issues, leading to decreased customer satisfaction and engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZyyo5p53BTG"
   },
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDktgZyA_LJG"
   },
   "source": [
    "The primary goals of the proposed support ticket categorization system are accurate classification, enabling a tagging mechanism, prioritization based on customer sentiment, and automated first response generation.\n",
    "\n",
    "The implementation of such a support ticket categorization system will empower the organization to respond proactively to customer feedback, ultimately leading to improved customer experiences and stronger, more enduring relationships with their client base, optimize resource allocation to address high-impact issues, and drive both growth and customer loyalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAaZfw3ae5hc"
   },
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqXhI8UAe-zj"
   },
   "source": [
    "The dataset contains the following two columns:\n",
    "\n",
    "* **support\\_tick\\_id**: A unique identifier assigned to each support ticket.\n",
    "* **support\\_ticket\\_text**: The text content describing the issue reported in the support ticket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bonUQGu23RK"
   },
   "source": [
    "# Installing and Importing Necessary Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "awsKQRs-OYo8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers==4.45.0 accelerate==0.26.0 openai==1.40.0 bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t1/m6dx844s77gcn0m2bfbk8xmm0000gn/T/ipykernel_12459/3838541777.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "‚úÖ bitsandbytes successfully imported\n",
      "üì¶ Version: 0.42.0\n",
      "‚ö†Ô∏è  CUDA not available - bitsandbytes will use CPU fallback\n",
      "ü§ó Transformers: 4.45.0\n",
      "üöÄ Accelerate: 0.26.0\n",
      "\n",
      "‚úÖ Environment setup appears to be working correctly!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# Verify bitsandbytes installation and compatibility\n",
    "import sys\n",
    "import pkg_resources\n",
    "\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(f\"‚úÖ bitsandbytes successfully imported\")\n",
    "    print(f\"üì¶ Version: {bnb.__version__}\")\n",
    "    \n",
    "    # Check if CUDA is available for bitsandbytes\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üöÄ CUDA available: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"üíæ CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        \n",
    "        # Test basic bitsandbytes functionality\n",
    "        try:\n",
    "            # This is a simple test to see if bitsandbytes can create quantized weights\n",
    "            test_linear = bnb.nn.Linear8bitLt(10, 10)\n",
    "            print(\"‚úÖ bitsandbytes 8-bit linear layer creation successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  8-bit quantization test failed: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  CUDA not available - bitsandbytes will use CPU fallback\")\n",
    "        \n",
    "    # Check other important packages\n",
    "    transformers_version = pkg_resources.get_distribution(\"transformers\").version\n",
    "    accelerate_version = pkg_resources.get_distribution(\"accelerate\").version\n",
    "    \n",
    "    print(f\"ü§ó Transformers: {transformers_version}\")\n",
    "    print(f\"üöÄ Accelerate: {accelerate_version}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Environment setup appears to be working correctly!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import bitsandbytes: {e}\")\n",
    "    print(\"Please run the installation cell above to install required packages.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")\n",
    "    print(\"There might be a compatibility issue with your system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix bitsandbytes compatibility issue\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üîß Fixing bitsandbytes compatibility issue...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "try:\n",
    "    # Uninstall the problematic version\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"bitsandbytes\", \"-y\"], \n",
    "                          capture_output=True, text=True)\n",
    "    print(\"‚úÖ Uninstalled old bitsandbytes\")\n",
    "    \n",
    "    # Reinstall with proper flags\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"bitsandbytes\", \n",
    "                           \"--no-cache-dir\", \"--force-reinstall\"], \n",
    "                          capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Successfully reinstalled bitsandbytes\")\n",
    "        print(\"üîÑ Please restart the kernel and run the verification cell again\")\n",
    "    else:\n",
    "        print(f\"‚ùå Installation failed: {result.stderr}\")\n",
    "        print(\"üí° Trying alternative installation method...\")\n",
    "        \n",
    "        # Try without binary if the above fails\n",
    "        result2 = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                                \"bitsandbytes\", \"--no-binary\", \"bitsandbytes\"], \n",
    "                               capture_output=True, text=True)\n",
    "        if result2.returncode == 0:\n",
    "            print(\"‚úÖ Successfully installed bitsandbytes from source\")\n",
    "        else:\n",
    "            print(f\"‚ùå Source installation also failed: {result2.stderr}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during installation: {e}\")\n",
    "    \n",
    "print(\"\\nüìù Next steps:\")\n",
    "print(\"1. Restart the notebook kernel (Kernel ‚Üí Restart)\")\n",
    "print(\"2. Run the verification cell below to test the installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EEnxPxFSCw-"
   },
   "source": [
    "Note:\n",
    "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
    "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM5RW5-0TopC"
   },
   "source": [
    "**Prompt:**\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b>I want to analyze the provided CSV data and work with AI models to understand the support tickets. Help me import the necessary Python libraries to:\n",
    "\n",
    "1. Read and manipulate the data</ul>\n",
    "2. Work with JSON data\n",
    "3. Working with system enviroment\n",
    "4. Connect to OpenAI models\n",
    "5. Use models from Hugging Face with AutoTokenizer and AutoModelForCausalLM\n",
    "\n",
    "</font>\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b>\n",
    "These libraries will help us load the data, connect with AI models, and prepare for further steps in the project.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wdeAT_1EgI9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.96.1\n",
      "  Using cached openai-1.96.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai==1.96.1) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai==1.96.1) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai==1.96.1) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai==1.96.1) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai==1.96.1) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai==1.96.1) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai==1.96.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from openai==1.96.1) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai==1.96.1) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai==1.96.1) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai==1.96.1) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.96.1) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai==1.96.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai==1.96.1) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai==1.96.1) (0.4.1)\n",
      "Using cached openai-1.96.1-py3-none-any.whl (757 kB)\n",
      "Using cached openai-1.96.1-py3-none-any.whl (757 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.40.0\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.40.0\n",
      "    Uninstalling openai-1.40.0:\n",
      "      Successfully uninstalled openai-1.40.0\n",
      "    Uninstalling openai-1.40.0:\n",
      "      Successfully uninstalled openai-1.40.0\n",
      "Successfully installed openai-1.96.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully installed openai-1.96.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0906 15:10:44.434000 12459 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai==1.96.1\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTpWESc53dL9"
   },
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "er-hahR_HS7L"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Mount the Google Drive\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvmBPiheIm4k"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JhIr_HKrCN5"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Load the CSV file named \"support_ticket_data\" and store it in the variable data.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ksv9hSCR4BM_"
   },
   "outputs": [],
   "source": [
    "# Load your CSV file\n",
    "data = pd.read_csv(\"../data/support_ticket_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8qUEOcQ3j5q"
   },
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23rEgLArjbuo"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Display the number of rows and columns in the `data`.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4fLXRyDA4m3S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imUEo-l6na6D"
   },
   "source": [
    "* There are 21 rows and 2 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J9f9vmyHir0"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Display the first 5 rows of the `data`.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bNOTBqaNVG4j"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support_tick_id</th>\n",
       "      <th>support_ticket_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ST2023-006</td>\n",
       "      <td>My internet connection has significantly slowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ST2023-007</td>\n",
       "      <td>Urgent help required! My laptop refuses to sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ST2023-008</td>\n",
       "      <td>I've accidentally deleted essential work docum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ST2023-009</td>\n",
       "      <td>Despite being in close proximity to my Wi-Fi r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ST2023-010</td>\n",
       "      <td>My smartphone battery is draining rapidly, eve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  support_tick_id                                support_ticket_text\n",
       "0      ST2023-006  My internet connection has significantly slowe...\n",
       "1      ST2023-007  Urgent help required! My laptop refuses to sta...\n",
       "2      ST2023-008  I've accidentally deleted essential work docum...\n",
       "3      ST2023-009  Despite being in close proximity to my Wi-Fi r...\n",
       "4      ST2023-010  My smartphone battery is draining rapidly, eve..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8G_fHQto9Y_0"
   },
   "source": [
    "We create a copy of the original DataFrame to ensure that we always have the original support ticket data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yue57kq49lOg"
   },
   "outputs": [],
   "source": [
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qP5KTLo3OOC"
   },
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwG-gzl0-zd1"
   },
   "source": [
    "We'll be using two language models:\n",
    "\n",
    "1. `TinyLlama/TinyLlama-1.1B-Chat-v1.0` - an open-source model from Hugging Face.\n",
    "2. OpenAI's GPT model - accessed using an API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jbb_RLK-3oM"
   },
   "source": [
    "Before we start using these models, we need to set up and securely load our API tokens into the environment.  \n",
    "This ensures authenticated access to both Hugging Face and OpenAI services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7Kxp8iTj5LF"
   },
   "source": [
    "## Setting Up Hugging Face Token and OpenAI API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xy8jodqHkuaw"
   },
   "source": [
    "Set Your OpenAI and Hugging face keys in the `config.json` file provided.\n",
    "\n",
    "1. Replace `\"your-hugging-face-token\"` with your actual Hugging Face key.\n",
    "2. Replace `\"your_api_key_here\"` with your actual OpenAI API key.\n",
    "3. Replace `\"your_base_url_here\"` with your OpenAI base URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c5Fv9Jeyggwx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading config from: Config.json\n",
      "‚úÖ Config file loaded successfully!\n",
      "\n",
      "üîç Configuration Status:\n",
      "========================================\n",
      "‚úÖ HF_TOKEN: Configured (37 characters)\n",
      "   Token preview: hf_IXMsTHd...\n",
      "‚úÖ OPENAI_API_KEY: Configured (67 characters)\n",
      "   Key preview: gl-U2FsdGV...\n",
      "‚úÖ OPENAI_API_BASE: https://aibe.mygreatlearning.com/openai\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load from your config.json (update the path if needed)\n",
    "config_file = \"Config.json\"\n",
    "\n",
    "# Check if config file exists, if not try alternative paths\n",
    "if not os.path.exists(config_file):\n",
    "    alternative_paths = [\"../data/Config.json\", \"data/Config.json\"]\n",
    "    for path in alternative_paths:\n",
    "        if os.path.exists(path):\n",
    "            config_file = path\n",
    "            break\n",
    "\n",
    "print(f\"üìÅ Loading config from: {config_file}\")\n",
    "\n",
    "try:\n",
    "    with open(config_file, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(\"‚úÖ Config file loaded successfully!\")\n",
    "    \n",
    "    # Extract the tokens\n",
    "    HF_TOKEN = config.get('HF_TOKEN')\n",
    "    OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
    "    OPENAI_API_BASE = config.get(\"OPENAI_API_BASE\")\n",
    "    \n",
    "    # Display configuration status\n",
    "    print(\"\\nüîç Configuration Status:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check HF_TOKEN\n",
    "    if HF_TOKEN:\n",
    "        if HF_TOKEN == \"your-hugging-face-token\":\n",
    "            print(\"‚ùå HF_TOKEN: Still using placeholder value\")\n",
    "            print(\"   Please update with your actual Hugging Face token\")\n",
    "        else:\n",
    "            print(f\"‚úÖ HF_TOKEN: Configured ({len(HF_TOKEN)} characters)\")\n",
    "            print(f\"   Token preview: {HF_TOKEN[:10]}...\")\n",
    "    else:\n",
    "        print(\"‚ùå HF_TOKEN: Missing from config\")\n",
    "    \n",
    "    # Check OPENAI_API_KEY\n",
    "    if OPENAI_API_KEY:\n",
    "        if OPENAI_API_KEY == \"your_api_key_here\":\n",
    "            print(\"‚ùå OPENAI_API_KEY: Still using placeholder value\")\n",
    "            print(\"   Please update with your actual OpenAI API key\")\n",
    "        else:\n",
    "            print(f\"‚úÖ OPENAI_API_KEY: Configured ({len(OPENAI_API_KEY)} characters)\")\n",
    "            print(f\"   Key preview: {OPENAI_API_KEY[:10]}...\")\n",
    "    else:\n",
    "        print(\"‚ùå OPENAI_API_KEY: Missing from config\")\n",
    "    \n",
    "    # Check OPENAI_API_BASE\n",
    "    if OPENAI_API_BASE:\n",
    "        if OPENAI_API_BASE == \"your_base_url_here\":\n",
    "            print(\"‚ùå OPENAI_API_BASE: Still using placeholder value\")\n",
    "            print(\"   Please update with your actual OpenAI base URL\")\n",
    "        else:\n",
    "            print(f\"‚úÖ OPENAI_API_BASE: {OPENAI_API_BASE}\")\n",
    "    else:\n",
    "        print(\"‚ùå OPENAI_API_BASE: Missing from config\")\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Show next steps if any placeholders detected\n",
    "    if (HF_TOKEN == \"your-hugging-face-token\" or \n",
    "        OPENAI_API_KEY == \"your_api_key_here\" or \n",
    "        OPENAI_API_BASE == \"your_base_url_here\"):\n",
    "        print(\"\\nüí° Next Steps:\")\n",
    "        print(\"1. Update Config.json with your actual API keys\")\n",
    "        print(\"2. Get HF token from: https://huggingface.co/settings/tokens\")\n",
    "        print(\"3. Accept TinyLlama model license: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "        print(\"4. Re-run this cell after updating\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Config file not found: {config_file}\")\n",
    "    print(\"Please ensure Config.json exists in the current directory or data folder\")\n",
    "    HF_TOKEN = None\n",
    "    OPENAI_API_KEY = None\n",
    "    OPENAI_API_BASE = None\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"‚ùå Error parsing Config.json: {e}\")\n",
    "    print(\"Please check that your Config.json file has valid JSON format\")\n",
    "    HF_TOKEN = None\n",
    "    OPENAI_API_KEY = None\n",
    "    OPENAI_API_BASE = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error loading config: {e}\")\n",
    "    HF_TOKEN = None\n",
    "    OPENAI_API_KEY = None\n",
    "    OPENAI_API_BASE = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIfkyqocLm-I"
   },
   "source": [
    "## Loading TinyLlama Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDjUfRIOzuWn"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Load the `TinyLlama/TinyLlama-1.1B-Chat-v1.0` from hugging face using 8-bit quantization.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzRIuDrAS3lU"
   },
   "source": [
    "**NOTE**\n",
    "\n",
    "1. We‚Äôre loading the entire model onto the local machine, which might take some time to initialize. To optimize this, we use 8-bit loading to reduce memory usage and speed up inference without significantly impacting performance.\n",
    "\n",
    "2. Before loading the model, you must first agree to its terms and conditions on Hugging Face. To do this, search for the model on the Hugging Face website, review its license or usage restrictions, and click ‚ÄúAgree and Access‚Äù to enable programmatic access via code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xwBfPKrgn4VP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged into Hugging Face\n",
      "Loading tokenizer...\n",
      "Loading model with 8-bit quantization...\n",
      "Loading model with 8-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\n",
      "Please check your internet connection and try again.\n",
      "You may also need to accept the model's license on Hugging Face website first.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    # Login to Hugging Face using your token\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Successfully logged into Hugging Face\")\n",
    "    \n",
    "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    \n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "    \n",
    "    print(\"Loading model with 8-bit quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        load_in_8bit=True,                 # Load the model with 8-bit quantization\n",
    "        torch_dtype=torch.float16,         # Use 16-bit floats on GPU\n",
    "        device_map=\"auto\",                 # Automatically assign GPU or CPU\n",
    "        token=HF_TOKEN                     # Hugging Face token for access\n",
    "    )\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Please check your internet connection and try again.\")\n",
    "    print(\"You may also need to accept the model's license on Hugging Face website first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ü§ñ TINYLLAMA MODEL LOADING WITH COMPREHENSIVE MONITORING\n",
      "======================================================================\n",
      "\n",
      "‚ÑπÔ∏è System Information:\n",
      "   üñ•Ô∏è  CPU: 10 cores\n",
      "   üíæ RAM: 16.0 GB\n",
      "   üöÄ GPU: Not available (CPU-only mode)\n",
      "   üêç Python: 3.13.7\n",
      "   üî• PyTorch: 2.8.0\n",
      "\n",
      "üîÑ Authenticating with Hugging Face...\n",
      "üíª CPU: 12.3% | RAM: 13.0/16.0GB (95.5%)\n",
      "‚úÖ Successfully authenticated with Hugging Face\n",
      "\n",
      "‚ÑπÔ∏è Target Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "\n",
      "                                                                                                    \n",
      "\n",
      "üîÑ Loading tokenizer...\n",
      "\n",
      "‚úÖ Successfully authenticated with Hugging Face\n",
      "\n",
      "‚ÑπÔ∏è Target Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "\n",
      "                                                                                                    \n",
      "\n",
      "üîÑ Loading tokenizer...\n",
      "\n",
      "‚úÖ Tokenizer loaded successfully\n",
      "\n",
      "                                                                                                    \n",
      "\n",
      "üîÑ Strategy 1/4: 8-bit quantization\n",
      "   üìù Most memory efficient, requires bitsandbytes\n",
      "\n",
      "‚úÖ Tokenizer loaded successfully\n",
      "\n",
      "                                                                                                    \n",
      "\n",
      "üîÑ Strategy 1/4: 8-bit quantization\n",
      "   üìù Most memory efficient, requires bitsandbytes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b14dbf8216047fa8d8ab19375fd8038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading 8-bit quantization: 0step [00:00, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå 8-bit quantization failed after 0.5s\n",
      "   üîç Error: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U...\n",
      "\n",
      "                                                                                                    \n",
      "\n",
      "üîÑ Strategy 2/4: 4-bit quantization\n",
      "   üìù Ultra memory efficient, experimental\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cafea5574044f4bf5c25e178708402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading 4-bit quantization: 0step [00:00, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùå 4-bit quantization failed after 0.3s\n",
      "   üîç Error: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U...\n",
      "\n",
      "                                                                                                    \n",
      "\n",
      "üîÑ Strategy 3/4: Standard GPU loading\n",
      "   üìù Full precision on GPU, higher memory usage\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed05559b53147eb97a2c4ac865c5ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Standard GPU loading: 0step [00:00, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª CPU: 15.8% | RAM: 13.8/16.0GB (97.0%)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model loaded with Standard GPU loading in 7.4s\n",
      "   üìç Device: mps:0\n",
      "   üî¢ Data type: torch.float16\n",
      "   üìä Parameters: 1,100,048,384 (1.1B)\n",
      "\n",
      "                                                                                                    \n",
      "üíª CPU: 37.6% | RAM: 13.0/16.0GB (96.0%)\n",
      "======================================================================\n",
      "\n",
      "‚úÖ MODEL LOADING COMPLETED SUCCESSFULLY in 9.5s\n",
      "\n",
      "üìä FINAL SYSTEM STATE:\n",
      "   üíæ System RAM: 12.9/16.0 GB (96.0%)\n",
      "\n",
      "‚úÖ Model is ready for inference!\n",
      "\n",
      "======================================================================\n",
      "\n",
      "‚úÖ MODEL LOADING COMPLETED SUCCESSFULLY in 9.5s\n",
      "\n",
      "üìä FINAL SYSTEM STATE:\n",
      "   üíæ System RAM: 12.9/16.0 GB (96.0%)\n",
      "\n",
      "‚úÖ Model is ready for inference!\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Enhanced model loading with comprehensive monitoring\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "import threading\n",
    "import sys\n",
    "\n",
    "class ModelLoadingMonitor:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.monitoring = False\n",
    "        self.monitor_thread = None\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start resource monitoring in a separate thread\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.monitoring = True\n",
    "        self.monitor_thread = threading.Thread(target=self._monitor_resources, daemon=True)\n",
    "        self.monitor_thread.start()\n",
    "        \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop resource monitoring\"\"\"\n",
    "        self.monitoring = False\n",
    "        if self.monitor_thread:\n",
    "            self.monitor_thread.join(timeout=1)\n",
    "            \n",
    "    def _monitor_resources(self):\n",
    "        \"\"\"Monitor system resources during loading\"\"\"\n",
    "        while self.monitoring:\n",
    "            try:\n",
    "                # CPU usage\n",
    "                cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "                \n",
    "                # Memory usage\n",
    "                memory = psutil.virtual_memory()\n",
    "                memory_used_gb = memory.used / (1024**3)\n",
    "                memory_total_gb = memory.total / (1024**3)\n",
    "                memory_percent = memory.percent\n",
    "                \n",
    "                # GPU memory if available\n",
    "                gpu_info = \"\"\n",
    "                if torch.cuda.is_available():\n",
    "                    gpu_memory_used = torch.cuda.memory_allocated() / (1024**3)\n",
    "                    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "                    gpu_percent = (gpu_memory_used / gpu_memory_total) * 100\n",
    "                    gpu_info = f\" | GPU: {gpu_memory_used:.1f}/{gpu_memory_total:.1f}GB ({gpu_percent:.1f}%)\"\n",
    "                \n",
    "                # Clear previous line and print status\n",
    "                print(f\"\\rüíª CPU: {cpu_percent:.1f}% | RAM: {memory_used_gb:.1f}/{memory_total_gb:.1f}GB ({memory_percent:.1f}%){gpu_info}\", end=\"\", flush=True)\n",
    "                \n",
    "                time.sleep(2)  # Update every 2 seconds\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "    def get_elapsed_time(self):\n",
    "        \"\"\"Get elapsed time since monitoring started\"\"\"\n",
    "        if self.start_time:\n",
    "            return time.time() - self.start_time\n",
    "        return 0\n",
    "\n",
    "def print_status(message, status_type=\"info\"):\n",
    "    \"\"\"Print formatted status messages\"\"\"\n",
    "    icons = {\"info\": \"‚ÑπÔ∏è\", \"success\": \"‚úÖ\", \"error\": \"‚ùå\", \"warning\": \"‚ö†Ô∏è\", \"loading\": \"üîÑ\"}\n",
    "    icon = icons.get(status_type, \"‚ÑπÔ∏è\")\n",
    "    print(f\"\\n{icon} {message}\")\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Get detailed system information\"\"\"\n",
    "    print_status(\"System Information:\", \"info\")\n",
    "    print(f\"   üñ•Ô∏è  CPU: {psutil.cpu_count()} cores\")\n",
    "    print(f\"   üíæ RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name()\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        print(f\"   üöÄ GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "        print(f\"   üîß CUDA Version: {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(f\"   üöÄ GPU: Not available (CPU-only mode)\")\n",
    "    \n",
    "    print(f\"   üêç Python: {sys.version.split()[0]}\")\n",
    "    print(f\"   üî• PyTorch: {torch.__version__}\")\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = ModelLoadingMonitor()\n",
    "\n",
    "try:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ü§ñ TINYLLAMA MODEL LOADING WITH COMPREHENSIVE MONITORING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    get_system_info()\n",
    "    \n",
    "    # Start resource monitoring\n",
    "    monitor.start_monitoring()\n",
    "    \n",
    "    print_status(\"Authenticating with Hugging Face...\", \"loading\")\n",
    "    login(token=HF_TOKEN)\n",
    "    print_status(\"Successfully authenticated with Hugging Face\", \"success\")\n",
    "    \n",
    "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    print_status(f\"Target Model: {model_id}\", \"info\")\n",
    "    \n",
    "    # Clear monitoring line and load tokenizer\n",
    "    print(\"\\n\" + \" \"*100)  # Clear monitoring line\n",
    "    print_status(\"Loading tokenizer...\", \"loading\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "    print_status(\"Tokenizer loaded successfully\", \"success\")\n",
    "    \n",
    "    # Model loading strategies with detailed monitoring\n",
    "    model = None\n",
    "    strategies = [\n",
    "        {\n",
    "            \"name\": \"8-bit quantization\",\n",
    "            \"params\": {\"load_in_8bit\": True, \"torch_dtype\": torch.float16, \"device_map\": \"auto\"},\n",
    "            \"description\": \"Most memory efficient, requires bitsandbytes\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"4-bit quantization\", \n",
    "            \"params\": {\"load_in_4bit\": True, \"torch_dtype\": torch.float16, \"device_map\": \"auto\"},\n",
    "            \"description\": \"Ultra memory efficient, experimental\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Standard GPU loading\",\n",
    "            \"params\": {\"torch_dtype\": torch.float16, \"device_map\": \"auto\"},\n",
    "            \"description\": \"Full precision on GPU, higher memory usage\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CPU loading\",\n",
    "            \"params\": {\"torch_dtype\": torch.float32, \"device_map\": \"cpu\"},\n",
    "            \"description\": \"Fallback option, slower inference\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, strategy in enumerate(strategies, 1):\n",
    "        print(\"\\n\" + \" \"*100)  # Clear monitoring line\n",
    "        print_status(f\"Strategy {i}/4: {strategy['name']}\", \"loading\")\n",
    "        print(f\"   üìù {strategy['description']}\")\n",
    "        \n",
    "        strategy_start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Force garbage collection before loading\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            with tqdm(desc=f\"Loading {strategy['name']}\", unit=\"step\") as pbar:\n",
    "                pbar.set_description(f\"Initializing {strategy['name']}\")\n",
    "                pbar.update(1)\n",
    "                \n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_id,\n",
    "                    token=HF_TOKEN,\n",
    "                    **strategy[\"params\"]\n",
    "                )\n",
    "                \n",
    "                pbar.set_description(\"Finalizing model setup\")\n",
    "                pbar.update(1)\n",
    "                pbar.close()\n",
    "            \n",
    "            strategy_time = time.time() - strategy_start_time\n",
    "            print_status(f\"Model loaded with {strategy['name']} in {strategy_time:.1f}s\", \"success\")\n",
    "            \n",
    "            # Print model details\n",
    "            if hasattr(model, 'device'):\n",
    "                print(f\"   üìç Device: {model.device}\")\n",
    "            if hasattr(model, 'dtype'):\n",
    "                print(f\"   üî¢ Data type: {model.dtype}\")\n",
    "            \n",
    "            # Model size estimation\n",
    "            param_count = sum(p.numel() for p in model.parameters())\n",
    "            print(f\"   üìä Parameters: {param_count:,} ({param_count/1e9:.1f}B)\")\n",
    "            \n",
    "            # Memory usage after loading\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_memory = torch.cuda.memory_allocated() / (1024**3)\n",
    "                print(f\"   üíæ GPU Memory Used: {gpu_memory:.2f} GB\")\n",
    "            \n",
    "            break  # Success, exit the loop\n",
    "            \n",
    "        except Exception as e:\n",
    "            strategy_time = time.time() - strategy_start_time\n",
    "            print_status(f\"{strategy['name']} failed after {strategy_time:.1f}s\", \"error\")\n",
    "            print(f\"   üîç Error: {str(e)[:100]}...\")\n",
    "            \n",
    "            if i == len(strategies):\n",
    "                print_status(\"All loading strategies failed\", \"error\")\n",
    "                model = None\n",
    "    \n",
    "    # Stop monitoring\n",
    "    print(\"\\n\" + \" \"*100)  # Clear monitoring line\n",
    "    monitor.stop_monitoring()\n",
    "    \n",
    "    # Final status report\n",
    "    total_time = monitor.get_elapsed_time()\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    if model is not None:\n",
    "        print_status(f\"MODEL LOADING COMPLETED SUCCESSFULLY in {total_time:.1f}s\", \"success\")\n",
    "        \n",
    "        # Final system state\n",
    "        print(\"\\nüìä FINAL SYSTEM STATE:\")\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"   üíæ System RAM: {memory.used/(1024**3):.1f}/{memory.total/(1024**3):.1f} GB ({memory.percent:.1f}%)\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory_used = torch.cuda.memory_allocated() / (1024**3)\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            print(f\"   üöÄ GPU Memory: {gpu_memory_used:.1f}/{gpu_memory_total:.1f} GB\")\n",
    "            \n",
    "        print_status(\"Model is ready for inference!\", \"success\")\n",
    "        \n",
    "    else:\n",
    "        print_status(f\"MODEL LOADING FAILED after {total_time:.1f}s\", \"error\")\n",
    "        print(\"\\nüîß TROUBLESHOOTING SUGGESTIONS:\")\n",
    "        print(\"   1. Check internet connection\")\n",
    "        print(\"   2. Verify HF_TOKEN is valid\")\n",
    "        print(\"   3. Accept model license on Hugging Face\")\n",
    "        print(\"   4. Try restarting the kernel\")\n",
    "        print(\"   5. Consider using a smaller model\")\n",
    "        \n",
    "except Exception as e:\n",
    "    monitor.stop_monitoring()\n",
    "    print(\"\\n\" + \" \"*100)  # Clear monitoring line\n",
    "    print_status(f\"Critical error during setup: {e}\", \"error\")\n",
    "    \n",
    "finally:\n",
    "    # Ensure monitoring is stopped\n",
    "    monitor.stop_monitoring()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model monitoring utilities for ongoing use\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelMonitor:\n",
    "    \"\"\"Utility class for monitoring model performance and resource usage\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.inference_times = []\n",
    "        self.memory_usage = []\n",
    "        self.timestamps = []\n",
    "        \n",
    "    def check_model_status(self, model=None, tokenizer=None):\n",
    "        \"\"\"Quick health check of the loaded model\"\"\"\n",
    "        print(\"üîç MODEL STATUS CHECK\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if model exists and is loaded\n",
    "        if model is not None:\n",
    "            print(\"‚úÖ Model: Loaded and ready\")\n",
    "            \n",
    "            # Check model device\n",
    "            try:\n",
    "                device = next(model.parameters()).device\n",
    "                print(f\"üìç Device: {device}\")\n",
    "            except:\n",
    "                print(\"‚ö†Ô∏è  Device: Unable to determine\")\n",
    "                \n",
    "            # Check model memory usage\n",
    "            if torch.cuda.is_available() and str(device).startswith('cuda'):\n",
    "                gpu_memory = torch.cuda.memory_allocated() / (1024**3)\n",
    "                gpu_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "                print(f\"üíæ GPU Memory: {gpu_memory:.2f}/{gpu_total:.1f} GB\")\n",
    "                \n",
    "            # Parameter count\n",
    "            try:\n",
    "                param_count = sum(p.numel() for p in model.parameters())\n",
    "                print(f\"üìä Parameters: {param_count:,} ({param_count/1e9:.1f}B)\")\n",
    "            except:\n",
    "                print(\"üìä Parameters: Unable to count\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Model: Not loaded\")\n",
    "            \n",
    "        # Check tokenizer\n",
    "        if tokenizer is not None:\n",
    "            print(\"‚úÖ Tokenizer: Loaded and ready\")\n",
    "            try:\n",
    "                vocab_size = len(tokenizer.get_vocab())\n",
    "                print(f\"üìù Vocabulary size: {vocab_size:,}\")\n",
    "            except:\n",
    "                print(\"üìù Vocabulary: Unable to determine size\")\n",
    "        else:\n",
    "            print(\"‚ùå Tokenizer: Not loaded\")\n",
    "            \n",
    "        # System resources\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"üíª System RAM: {memory.percent:.1f}% used\")\n",
    "        print(f\"üñ•Ô∏è  CPU Usage: {psutil.cpu_percent():.1f}%\")\n",
    "        \n",
    "    def monitor_inference(self, input_text, model, tokenizer, max_length=100):\n",
    "        \"\"\"Monitor a single inference with detailed metrics\"\"\"\n",
    "        print(f\"\\nüöÄ MONITORING INFERENCE: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Pre-inference state\n",
    "        start_time = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            gpu_mem_before = torch.cuda.memory_allocated() / (1024**3)\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input\n",
    "            print(\"üîÑ Tokenizing input...\")\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
    "            input_length = inputs['input_ids'].shape[1]\n",
    "            print(f\"üìù Input tokens: {input_length}\")\n",
    "            \n",
    "            # Move to device\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate\n",
    "            print(\"ü§ñ Generating response...\")\n",
    "            generation_start = time.time()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            generation_time = time.time() - generation_start\n",
    "            \n",
    "            # Decode output\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            output_length = len(outputs[0])\n",
    "            \n",
    "            # Post-inference metrics\n",
    "            total_time = time.time() - start_time\n",
    "            tokens_per_second = (output_length - input_length) / generation_time if generation_time > 0 else 0\n",
    "            \n",
    "            # Memory usage\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                gpu_mem_after = torch.cuda.memory_allocated() / (1024**3)\n",
    "                memory_delta = gpu_mem_after - gpu_mem_before\n",
    "            else:\n",
    "                memory_delta = 0\n",
    "            \n",
    "            # Results\n",
    "            print(\"\\nüìä INFERENCE METRICS:\")\n",
    "            print(f\"   ‚è±Ô∏è  Total time: {total_time:.3f}s\")\n",
    "            print(f\"   üöÄ Generation time: {generation_time:.3f}s\")\n",
    "            print(f\"   üìà Tokens/second: {tokens_per_second:.1f}\")\n",
    "            print(f\"   üìù Output tokens: {output_length - input_length}\")\n",
    "            if memory_delta != 0:\n",
    "                print(f\"   üíæ Memory delta: {memory_delta:+.3f} GB\")\n",
    "            \n",
    "            # Store metrics for analysis\n",
    "            self.inference_times.append(total_time)\n",
    "            self.memory_usage.append(memory_delta)\n",
    "            self.timestamps.append(datetime.now())\n",
    "            \n",
    "            print(f\"\\nüìÑ RESPONSE PREVIEW:\")\n",
    "            preview = response[:200] + \"...\" if len(response) > 200 else response\n",
    "            print(f\"   {preview}\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Inference failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def plot_performance_history(self):\n",
    "        \"\"\"Plot performance metrics over time\"\"\"\n",
    "        if not self.inference_times:\n",
    "            print(\"‚ö†Ô∏è  No inference data to plot\")\n",
    "            return\n",
    "            \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Inference times\n",
    "        ax1.plot(range(len(self.inference_times)), self.inference_times, 'b-o', markersize=4)\n",
    "        ax1.set_title('Inference Time History')\n",
    "        ax1.set_ylabel('Time (seconds)')\n",
    "        ax1.set_xlabel('Inference #')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Memory usage\n",
    "        if any(mem != 0 for mem in self.memory_usage):\n",
    "            ax2.plot(range(len(self.memory_usage)), self.memory_usage, 'r-o', markersize=4)\n",
    "            ax2.set_title('Memory Usage Delta')\n",
    "            ax2.set_ylabel('Memory (GB)')\n",
    "            ax2.set_xlabel('Inference #')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'No GPU memory data available', \n",
    "                    transform=ax2.transAxes, ha='center', va='center')\n",
    "            ax2.set_title('Memory Usage Delta (No Data)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "        print(f\"   üî¢ Total inferences: {len(self.inference_times)}\")\n",
    "        print(f\"   ‚ö° Average time: {np.mean(self.inference_times):.3f}s\")\n",
    "        print(f\"   üìä Min/Max time: {min(self.inference_times):.3f}s / {max(self.inference_times):.3f}s\")\n",
    "        if any(mem != 0 for mem in self.memory_usage):\n",
    "            print(f\"   üíæ Avg memory delta: {np.mean(self.memory_usage):.3f} GB\")\n",
    "\n",
    "# Create global monitor instance\n",
    "model_monitor = ModelMonitor()\n",
    "\n",
    "print(\"‚úÖ Model monitoring utilities loaded!\")\n",
    "print(\"üìö Available functions:\")\n",
    "print(\"   ‚Ä¢ model_monitor.check_model_status(model, tokenizer) - Quick health check\")\n",
    "print(\"   ‚Ä¢ model_monitor.monitor_inference(text, model, tokenizer) - Monitor single inference\")\n",
    "print(\"   ‚Ä¢ model_monitor.plot_performance_history() - Plot performance trends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative model loading with fallback options\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    # Login to Hugging Face using your token\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Successfully logged into Hugging Face\")\n",
    "    \n",
    "    # Try TinyLlama first, fallback to more accessible models if needed\n",
    "    model_options = [\n",
    "        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # Original choice\n",
    "        \"microsoft/DialoGPT-medium\",           # Fallback 1: Doesn't require license\n",
    "        \"google/flan-t5-base\"                  # Fallback 2: Open and accessible\n",
    "    ]\n",
    "    \n",
    "    model_loaded = False\n",
    "    for model_id in model_options:\n",
    "        try:\n",
    "            print(f\"Attempting to load: {model_id}\")\n",
    "            print(\"Loading tokenizer...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "            \n",
    "            print(\"Loading model with optimizations...\")\n",
    "            \n",
    "            # Use different loading strategies based on model\n",
    "            if \"tinyllama\" in model_id.lower():\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_id,\n",
    "                    load_in_8bit=True,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    token=HF_TOKEN\n",
    "                )\n",
    "            else:\n",
    "                # Simpler loading for fallback models\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_id,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    token=HF_TOKEN\n",
    "                )\n",
    "            \n",
    "            print(f\"‚úÖ Model loaded successfully: {model_id}\")\n",
    "            model_loaded = True\n",
    "            break\n",
    "            \n",
    "        except Exception as model_error:\n",
    "            print(f\"‚ùå Failed to load {model_id}: {model_error}\")\n",
    "            continue\n",
    "    \n",
    "    if not model_loaded:\n",
    "        print(\"‚ùå Failed to load any model. Please check:\")\n",
    "        print(\"1. Internet connection\")\n",
    "        print(\"2. Hugging Face token permissions\")\n",
    "        print(\"3. Model license acceptance\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå General error: {e}\")\n",
    "    print(\"Please check your internet connection and Hugging Face token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S5Ithq_EzCQ"
   },
   "source": [
    "* `load_in_8bit=True`: Loads the model using 8-bit quantization to save memory.\n",
    "* `torch_dtype=torch.float16`: Uses half-precision (16-bit) floats for faster computation on GPU.\n",
    "* `device_map=\"auto\"`: Automatically places model layers across available devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sR-dGKF_UBW"
   },
   "source": [
    "The Hugging Face model is now ready. Let‚Äôs test it on an example input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAsdu3G5nIOC"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Ask the TinyLlama model: What is the capital of France?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edSsEJ5UsErS"
   },
   "outputs": [],
   "source": [
    "# Check if the model and tokenizer are loaded\n",
    "try:\n",
    "\t# Test if tokenizer and model are available\n",
    "\tif 'tokenizer' not in globals() or 'model' not in globals():\n",
    "\t\tprint(\"Model and tokenizer are not loaded. Please run cell 36 first to load the TinyLlama model.\")\n",
    "\telse:\n",
    "\t\t# Define the prompt and query\n",
    "\t\tsystem_prompt = \"You are a helpful assistant that answers questions clearly and concisely.\"\n",
    "\t\tquery = \"What is the capital of France?\"\n",
    "\n",
    "\t\t# Use the query_tinyllama function that was defined earlier\n",
    "\t\tresponse = query_tinyllama(system_prompt, query)\n",
    "\t\tprint(f\"Question: {query}\")\n",
    "\t\tprint(f\"Answer: {response}\")\n",
    "except NameError as e:\n",
    "\tprint(f\"Error: {e}\")\n",
    "\tprint(\"Please ensure that cell 36 (model loading) and cell 44 (query_tinyllama function) have been executed successfully.\")\n",
    "except Exception as e:\n",
    "\tprint(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YG4KRgMA_kYC"
   },
   "source": [
    "As we can see, the model is returning results successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hefY6U1-_nH2"
   },
   "source": [
    "Let‚Äôs define a function that takes a `prompt` and a `query` as inputs and returns the model‚Äôs output.  \n",
    "\n",
    "- This will make it easier to reuse the model across different inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDCQOP1NnwaN"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Create a function that accepts a prompt and query, and returns the response generated by the TinyLlama model.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UminawUZtUL3"
   },
   "outputs": [],
   "source": [
    "def query_tinyllama(prompt, query):\n",
    "    \"\"\"\n",
    "    Queries the TinyLlama model with a given prompt and query.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt for the model.\n",
    "        query (str): The query to be answered by the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "\n",
    "    attention_mask = (inputs != pad_token_id).long()\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=100,  # Adjust as needed\n",
    "        do_sample=True,\n",
    "        temperature=0.7,     # Adjust as needed\n",
    "        top_p=0.9,           # Adjust as needed\n",
    "        pad_token_id=pad_token_id  # Prevents warning\n",
    "    )\n",
    "\n",
    "    # Decode and print the output, skipping the input tokens\n",
    "    response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18d1tmR1mVqw"
   },
   "source": [
    "In the code snippet defined above, the following components are used:\n",
    "\n",
    "1. `tokenizer.apply_chat_template()`: This method converts the `messages` list into a single formatted string (e.g., adding special tokens or chat-style formatting), and tokenizes it into a tensor using PyTorch (`return_tensors=\"pt\"`). The `.to(model.device)` part ensures the tokenized input is moved to the same device as the model (like a GPU or CPU).\n",
    "\n",
    "2. `pad_token_id`: This variable is assigned the padding token ID used by the tokenizer. If the tokenizer does not explicitly define a `pad_token_id`, it falls back to the `eos_token_id` (end-of-sequence token). This is needed to handle padding properly during attention and generation.\n",
    "\n",
    "3. `attention_mask:` This creates a mask that tells the model which tokens should be attended to (represented by 1) and which should be ignored (usually padding tokens, represented by 0). It ensures the model focuses only on valid input tokens during processing.\n",
    "\n",
    "In the `generate()` function defined above, the following arguments are used:\n",
    "\n",
    "1. `max_new_tokens`: This parameter determines the maximum length of the generated sequence. In the provided code, max_new_tokens is set to 100, which means the generated sequence should not exceed 100 tokens.\n",
    "\n",
    "2. `temperature`: The temperature parameter controls the level of randomness in the generation process. A higher temperature (e.g., closer to 1) makes the output more diverse and creative but potentially less focused, while a lower temperature (e.g., close to 0) produces more deterministic and focused but potentially repetitive outputs. In the code, temperature is set to 0.7, indicating a very low temperature and, consequently, a more deterministic sampling.\n",
    "\n",
    "3. `do_sample`: This is a boolean parameter that determines whether to use sampling during generation (do_sample=True) or use greedy decoding (do_sample=False). When set to True, as in the provided code, the model samples from the distribution of predicted tokens at each step, introducing randomness in the generation process.\n",
    "\n",
    "4. `top_p`: Controls how many top probable tokens to consider during generation. If set to 0.9, it samples from the smallest set of tokens whose combined probability is at least 90%, balancing creativity and coherence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWvf3R3An5K4"
   },
   "source": [
    "## Loading OpenAI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbsa0OGPoFLJ"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Create an OpenAI client\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSsdWE74oLR1"
   },
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wfff9Bpw_yHR"
   },
   "source": [
    "OpenAI model is now ready. Let‚Äôs test it on an example input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZAPl1pgoreW"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Ask the OpenAI model: What is the capital of France?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyNtDmZtoTRC"
   },
   "outputs": [],
   "source": [
    "# prompt: Ask the OpenAI model: What is the capital of France?\n",
    "\n",
    "query = \"What is the capital of France?\"\n",
    "\n",
    "# Try different model names that might be available on your endpoint\n",
    "model_options = [\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-4-turbo\", \"text-davinci-003\"]\n",
    "\n",
    "response = None\n",
    "for model_name in model_options:\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            max_tokens=100\n",
    "        )\n",
    "        print(f\"‚úÖ Successfully used model: {model_name}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model {model_name} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "if response:\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Answer: {response.choices[0].message.content}\")\n",
    "else:\n",
    "    print(\"‚ùå All models failed. Please check your OpenAI configuration.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoF1jl23_t7V"
   },
   "source": [
    "The model is returning results successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBtMvtIk_t7V"
   },
   "source": [
    "So, let‚Äôs define a function that takes a `prompt` and a `query` as inputs and returns the model‚Äôs output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYUZqUIgotcY"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Create a function that accepts a prompt and query, and returns the response generated by the OpenAI model.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6Mo3U1EokSS"
   },
   "outputs": [],
   "source": [
    "# prompt: Create a function that accepts a prompt and query, and returns the response generated by the OpenAI model.\n",
    "\n",
    "def query_openai(prompt, query):\n",
    "    \"\"\"\n",
    "    Queries the OpenAI model with a given prompt and query.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt for the model.\n",
    "        query (str): The query to be answered by the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Or another suitable OpenAI model\n",
    "        messages=messages,\n",
    "        max_tokens=500  # Adjust max_tokens as needed\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage:\n",
    "# prompt_text = \"You are a helpful assistant.\"\n",
    "# query_text = \"Explain the process of ticket categorization.\"\n",
    "# openai_response = query_openai(prompt_text, query_text)\n",
    "# print(openai_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lc49MXeEo_pY"
   },
   "source": [
    "# Ticket Categorization and Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wprva8z-AEem"
   },
   "source": [
    "Let‚Äôs take a sample from the support tickets to observe how the model performs on each task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YumXLnrlAW2_"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b>Take the first \"support_ticket_text\" from the data and store it in the variable sample_ticket.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agbWvyhYpGgn"
   },
   "outputs": [],
   "source": [
    "sample_ticket=data['support_ticket_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le80Djip27mc"
   },
   "source": [
    "## Task 1: Ticket Categorization using Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaI6ed21A-xF"
   },
   "source": [
    "Our first task is to classify the support ticket into a predefined category. We will write a prompt that instructs the model to return the category in **JSON format**, so that we receive the output as structured data that‚Äôs easy to parse and store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPIFcGW3x-_L"
   },
   "outputs": [],
   "source": [
    "classification_prompt = \"\"\"\n",
    " You are a technical assistant. Classify the support ticket based on the Support Ticket Text presented in the input into the following categories and not any other.\n",
    "    - Technical issues\n",
    "    - Hardware issues\n",
    "    - Data recovery\n",
    "Return only a structured JSON output in the following format:\n",
    "{\"Category\": \"category_prediction\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfqBaEwD3FNh"
   },
   "source": [
    "### Using TinyLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSSp4w-O3FNh"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Define a function named classify_ticket_tinyllama that takes the classification prompt and the support ticket text as input, gets the result from query_tinyllama function, and returns the result in a JSON format.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iv3KecC3FNi"
   },
   "outputs": [],
   "source": [
    "def classify_ticket_tinyllama(prompt, query):\n",
    "    \"\"\"\n",
    "    Classifies a support ticket using the TinyLlama model and returns the result in JSON format.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The classification prompt for the model.\n",
    "        query (str): The support ticket text to be classified.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the classification result, or None if classification fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response_text = query_tinyllama(prompt, query)\n",
    "        # Attempt to parse the response text as JSON\n",
    "        classification_result = json.loads(response_text)\n",
    "        return classification_result\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from TinyLlama response: {e}\")\n",
    "        print(f\"Raw TinyLlama response: {response_text}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SIgRYtY3FNi"
   },
   "outputs": [],
   "source": [
    "classify=classify_ticket_tinyllama(classification_prompt,sample_ticket)\n",
    "print(sample_ticket)\n",
    "print(classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy6UkNOr3FNj"
   },
   "source": [
    "> Ticket text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
    "\n",
    "> Category : Technical Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBrB_F85223i"
   },
   "source": [
    "### Using OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lcVobKup7SJ"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Define a function named classify_ticket_openai that takes the classification prompt and the support ticket text as input, gets the result from query_openai function, and returns the result in a JSON format.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUhcgUvSpNAS"
   },
   "outputs": [],
   "source": [
    "def classify_ticket_openai(prompt, query):\n",
    "    \"\"\"\n",
    "    Classifies a support ticket using the OpenAI model and returns the result in JSON format.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The classification prompt for the model.\n",
    "        query (str): The support ticket text to be classified.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the classification result, or None if classification fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response_text = query_openai(prompt, query)\n",
    "        # Attempt to parse the response text as JSON\n",
    "        classification_result = json.loads(response_text)\n",
    "        return classification_result\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
    "        print(f\"Raw OpenAI response: {response_text}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOHKXNImyaDd"
   },
   "outputs": [],
   "source": [
    "classify=classify_ticket_openai(classification_prompt,sample_ticket)\n",
    "print(sample_ticket)\n",
    "print(classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxdQtSqJRORK"
   },
   "source": [
    "> Ticket Text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
    "\n",
    "> Category : Technical Issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGm3fOxoBixV"
   },
   "source": [
    "Both models generate results in the desired format. However, we'll proceed with OpenAI, as its API-based setup provides slightly faster responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlpGNtUyrAhN"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b>Generate the category for each support_ticket_text in the DataFrame using the classify_ticket_openai function, and store the result in a new column.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0mIwZjgq24j"
   },
   "outputs": [],
   "source": [
    "# Apply the classification function to each row in the DataFrame\n",
    "df['Category'] = df['support_ticket_text'].apply(lambda x: classify_ticket_openai(classification_prompt, x)['Category'] if classify_ticket_openai(classification_prompt, x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNThjS9My8Ax"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z795llV0elBQ"
   },
   "source": [
    "## Task 2: Creating Tags using Few Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_9oOTqGodMp"
   },
   "source": [
    "For this task, we will use few-shot prompting instead of zero-shot prompting because it provides the model with concrete examples, helping it understand how to extract and structure the information accurately.\n",
    "- Zero-shot prompting may result in inconsistent formats, incorrect category\n",
    "mapping, or misinterpretation of impact levels due to a lack of context and guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-8JVxT7ryQv"
   },
   "outputs": [],
   "source": [
    "metadata_prompt = \"\"\"\n",
    "You are an intelligent assistant that extracts structured metadata from technical support queries.\n",
    "Analyze the query and extract the following information:\n",
    "\n",
    "* Device (e.g., Laptop, Phone, Router, etc.)\n",
    "* Problem Type (e.g., Not Turning On, Lost Internet, Deleted Files)\n",
    "* User Impact - Estimate based on how severely the issue affects the user's ability to continue working or using the device:\n",
    "\n",
    "    - * Major: The user cannot proceed with work at all.\n",
    "    - * Moderate: The user is impacted but may have a workaround.\n",
    "    - * Minor: The issue is present but does not significantly hinder usage.\n",
    "\n",
    "Use the following examples as guidance.\n",
    "\n",
    "Query Text: My phone battery is draining rapidly even on battery saver mode. I barely use it and it drops 50% in a few hours.\n",
    "Output: {\"Device\": \"Phone\", \"Problem Type\": \"Battery Draining\", \"User Impact\": \"Minor\"}\n",
    "\n",
    "Query Text: I accidentally deleted a folder containing all project files. Please help me recover it.\n",
    "Output: {\"Device\": \"Laptop\", \"Problem Type\": \"Deleted Files\", \"User Impact\": \"Major\"}\n",
    "\n",
    "Query Text: My router is not working.\n",
    "Output: {\"Device\": \"Router\", \"Problem Type\": \"Lost Internet\", \"User Impact\": \"Moderate\"}\n",
    "\n",
    "Return the final output only in a valid JSON format without any additional explanation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO7H34JN34vK"
   },
   "source": [
    "### Using TinyLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzjzYDIv34vK"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Define a function named extract_metadata_tinyllama that takes the metadata prompt and query as input get the result from query_tinyllama function return the result in JSON format.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcUXwDG134vK"
   },
   "outputs": [],
   "source": [
    "def extract_metadata_tinyllama(prompt, query):\n",
    "    \"\"\"\n",
    "    Extracts metadata from a support ticket using the TinyLlama model and returns the result in JSON format.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The metadata extraction prompt for the model.\n",
    "        query (str): The support ticket text to extract metadata from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted metadata, or None if extraction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response_text = query_tinyllama(prompt, query)\n",
    "        # Attempt to parse the response text as JSON\n",
    "        metadata_result = json.loads(response_text)\n",
    "        return metadata_result\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from TinyLlama response: {e}\")\n",
    "        print(f\"Raw TinyLlama response: {response_text}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0-9a5qz34vL"
   },
   "outputs": [],
   "source": [
    "metadata=extract_metadata_tinyllama(metadata_prompt,sample_ticket)\n",
    "print(sample_ticket)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3GL2I7H34vL"
   },
   "source": [
    "> Ticket text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
    "\n",
    "> Category : Technical Issue\n",
    "\n",
    "> Device : Laptop\n",
    "\n",
    "> Problem Type : Lost Internet\n",
    "\n",
    "> User Impace : Major"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKmYACL64p9g"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "* The issue described is \"Slowed Internet\" due to frequent disconnections, not a complete loss; the problem likely lies with the router, not the laptop.\n",
    "* The model incorrectly tagged the **Device** as \"Laptop\" and **Problem Type** as \"Lost Internet.\"\n",
    "\n",
    "Let's try to generate metadata using OpenAI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gVTYch-30qW"
   },
   "source": [
    "### Using Openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2kQLdXPrPL-"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Define a function named extract_metadata_openai that takes the metadata prompt and query as input get the result from query_openai function return the result in JSON format.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P66XewvrXE5"
   },
   "outputs": [],
   "source": [
    "def extract_metadata_openai(prompt, query):\n",
    "    \"\"\"\n",
    "    Extracts metadata from a support ticket using the OpenAI model and returns the result in JSON format.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The metadata extraction prompt for the model.\n",
    "        query (str): The support ticket text to extract metadata from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted metadata, or None if extraction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response_text = query_openai(prompt, query)\n",
    "        # Attempt to parse the response text as JSON\n",
    "        metadata_result = json.loads(response_text)\n",
    "        return metadata_result\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
    "        print(f\"Raw OpenAI response: {response_text}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAN_KmJazOEv"
   },
   "outputs": [],
   "source": [
    "metadata=extract_metadata_openai(metadata_prompt,sample_ticket)\n",
    "print(sample_ticket)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zETDRec3RbyD"
   },
   "source": [
    "> Ticket text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
    "\n",
    "> Category : Technical Issue\n",
    "\n",
    "> Device : Router\n",
    "\n",
    "> Problem Type : Slow Internet and Frequent Disconnections\n",
    "\n",
    "> User Impace : Major"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTSVHgbQ5Bdm"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "* OpenAI is generating the correct metadata based on the ticket description.\n",
    "* Therefore, we will proceed with OpenAI for further ticket categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXM1xt1Tr-fH"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Generate the metadata for each support_ticket_text in the DataFrame using extract_metadata_openai, store it in a new column, and extract individual fields into separate columns.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOgH1IjXrrU5"
   },
   "outputs": [],
   "source": [
    "# Generate metadata for each support_ticket_text and store it in a new column\n",
    "df['Metadata'] = df['support_ticket_text'].apply(lambda x: extract_metadata_openai(metadata_prompt, x))\n",
    "\n",
    "# Extract individual metadata fields into separate columns\n",
    "df['Device'] = df['Metadata'].apply(lambda x: x.get('Device') if x else None)\n",
    "df['Problem Type'] = df['Metadata'].apply(lambda x: x.get('Problem Type') if x else None)\n",
    "df['User Impact'] = df['Metadata'].apply(lambda x: x.get('User Impact') if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zshqw5GjPi8a"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdwE7rguh8sD"
   },
   "source": [
    "## Task 3: Predicting Priority using Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKshedOPpLDK"
   },
   "source": [
    "For this task, Chain-of-Thought (CoT) prompting is crucial because assigning support ticket priority involves understanding several subtle cues like urgency, usability, and user sentiment.\n",
    "\n",
    "- CoT allows the model to reason step-by-step through these factors, leading to\n",
    "more accurate and context-aware decisions. Without this, zero-shot prompts may miss key details and result in incorrect or inconsistent priority levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGXJ_lGpz0H-"
   },
   "outputs": [],
   "source": [
    "priority_prompt =\"\"\"\n",
    "You are an intelligent assistant that determines the priority level of a support ticket.\n",
    "\n",
    "For any given ticket, follow this step-by-step reasoning process to assign the correct priority level: Low, Medium, High.\n",
    "\n",
    "Step-by-step Evaluation:\n",
    "\n",
    "Is the device or service completely unusable?\n",
    "\n",
    "Is the issue blocking critical or time-sensitive work?\n",
    "\n",
    "Is there a specific deadline or urgency mentioned by the user?\n",
    "\n",
    "Does the user mention partial functionality or ongoing work?\n",
    "\n",
    "Is the tone or language expressing frustration or emergency?\n",
    "\n",
    "After evaluating all the above steps, decide the most appropriate priority level based on the impact and urgency.\n",
    "\n",
    "Return the final output only in a valid JSON format as mentioned below without any additional explanation:\n",
    "{\"priority\": \"High\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLMl9uH2tGUm"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Define a function that takes the priority prompt, problem type, user Impact, and query as input get the result from query_openai function return the result in JSON format.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z62s6umCtJd8"
   },
   "outputs": [],
   "source": [
    "def predict_priority(prompt, query, problem_type, user_impact):\n",
    "    \"\"\"\n",
    "    Predicts the priority of a support ticket using the OpenAI model and returns the result in JSON format.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The priority prediction prompt for the model.\n",
    "        query (str): The support ticket text to predict the priority for.\n",
    "        problem_type (str): The extracted problem type.\n",
    "        user_impact (str): The extracted user impact.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the predicted priority, or None if prediction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Include problem_type and user_impact in the query sent to the model\n",
    "        full_query = f\"\"\"\n",
    "        Support Ticket: {query}\n",
    "        Problem Type: {problem_type}\n",
    "        User Impact: {user_impact}\n",
    "\n",
    "        Based on the support ticket, problem type, and user impact, predict the priority: Low, Medium, High.\n",
    "        Return only a structured JSON output in the following format:\n",
    "        {{\"priority\": \"priority_prediction\"}}\n",
    "        \"\"\"\n",
    "        response_text = query_openai(prompt, full_query)\n",
    "        priority_result = json.loads(response_text)\n",
    "        return priority_result\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
    "        print(f\"Raw OpenAI response: {response_text}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JyzeYNP0EV2"
   },
   "outputs": [],
   "source": [
    "priority = predict_priority(priority_prompt, sample_ticket,metadata['Problem Type'],metadata['User Impact'])\n",
    "print(sample_ticket)\n",
    "priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEstjB8iT9tt"
   },
   "source": [
    "> Ticket text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
    "\n",
    "> Category : Technical Issue\n",
    "\n",
    "> Device : Router\n",
    "\n",
    "> Problem Type : Slow Internet Connection\n",
    "\n",
    "> User Impact : Major\n",
    "\n",
    "> Priority: High"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZZmaaf9ugxV"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Generate the priotity for each support_ticket_text in the df and store it in a new column.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLpVJiwnul8Q"
   },
   "outputs": [],
   "source": [
    "# prompt: Generate the priotity for each support_ticket_text in the df and store it in a new column\n",
    "# Generate the priority for each support_ticket_text and store it in a new column\n",
    "df['Priority'] = df.apply(lambda row: predict_priority(priority_prompt, row['support_ticket_text'], row['Problem Type'], row['User Impact'])['priority'] if predict_priority(priority_prompt, row['support_ticket_text'], row['Problem Type'], row['User Impact']) else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMNqIujH2seq"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7m73qRcne4g"
   },
   "source": [
    "## Task 4 - Creating a Draft Response using Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAroVSTOpZEe"
   },
   "source": [
    "We will continue using Chain-of-Thought (CoT) prompting for this task because generating a thoughtful, empathetic response requires the model to reason through multiple elements, such as the user's concern, the issue type, priority-based urgency, and appropriate tone.\n",
    "- CoT helps the model integrate all these cues step-by-step to produce a relevant and human-like reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CYnMUZUnqkf"
   },
   "outputs": [],
   "source": [
    "response_prompt = \"\"\"\n",
    "You are provided with a support ticket's text along with its Category, Tags, and assigned Priority level.\n",
    "\n",
    "Follow these steps before generating your final response:\n",
    "\n",
    "1. Analyze the ticket text to understand the customer's sentiment and main concern.\n",
    "2. Identify the issue type using the provided Category and Tags.\n",
    "3. Determine the appropriate ETA based on the Priority level.\n",
    "4. Compose a short, empathetic response that reassures the customer, acknowledges their concern, and includes the ETA.\n",
    "\n",
    "Ensure the final response:\n",
    "\n",
    "1. Is under 50 words\n",
    "2. Has a polite and empathetic tone\n",
    "3. Addresses the issue clearly\n",
    "\n",
    "Return only the final response to the customer. Do not include your reasoning steps in the output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3SWGDv2vLAc"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Define a function that takes the `response_prompt`, `query`, `category`, `metadata_tags`, and `priority` as inputs. Pass `response_prompt` as the system prompt, and combine the remaining inputs (`query`, `category`, `metadata_tags`, and `priority`) into a single message. Pass this message to the `query_openai` function and return the result.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmIo_dmivifl"
   },
   "outputs": [],
   "source": [
    "# prompt: Define a function that takes response_prompt, query, category, metadata_tags, and priority as inputs. Pass response_prompt as the system prompt, and combine the remaining inputs (query, category, metadata_tags, and priority) into a single message. Pass this message to the query_openai function and return the result.\n",
    "def generate_response(response_prompt, query, category, metadata_tags, priority):\n",
    "    \"\"\"\n",
    "    Generates a draft response for a support ticket using the OpenAI model.\n",
    "\n",
    "    Args:\n",
    "        response_prompt (str): The prompt for generating the response.\n",
    "        query (str): The original support ticket text.\n",
    "        category (str): The predicted category of the ticket.\n",
    "        metadata_tags (dict): The extracted metadata tags (Device, Problem Type, User Impact).\n",
    "        priority (str): The predicted priority of the ticket.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response text, or None if response generation fails.\n",
    "    \"\"\"\n",
    "    # Combine the inputs into a single message for the model\n",
    "    user_message = f\"\"\"\n",
    "    Support Ticket: {query}\n",
    "    Category: {category}\n",
    "    Metadata Tags: {metadata_tags}\n",
    "    Priority: {priority}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Pass the combined message to the query_openai function\n",
    "        response_text = query_openai(response_prompt, user_message)\n",
    "        return response_text\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during response generation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage (assuming you have values for query, category, metadata_tags, priority):\n",
    "# sample_query = df['support_ticket_text'][0]\n",
    "# sample_category = df['Category'][0]\n",
    "# sample_metadata_tags = df['Metadata'][0]\n",
    "# sample_priority = df['Priority'][0]\n",
    "\n",
    "# draft_response = generate_response(response_prompt, sample_query, sample_category, sample_metadata_tags, sample_priority)\n",
    "# print(draft_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LDDAsfH0f8-"
   },
   "outputs": [],
   "source": [
    "sample_query = df['support_ticket_text'][0]\n",
    "sample_category = df['Category'][0]\n",
    "sample_metadata_tags = df['Metadata'][0]\n",
    "sample_priority = df['Priority'][0]\n",
    "\n",
    "draft_response = generate_response(response_prompt, sample_query, sample_category, sample_metadata_tags, sample_priority)\n",
    "print(draft_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnRfOCRKWs_V"
   },
   "source": [
    "> Ticket text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
    "\n",
    "> Category : Technical Issue\n",
    "\n",
    "> Device : Router\n",
    "\n",
    "> Problem Type : Slow Internet Connection\n",
    "\n",
    "> User Impact : Major\n",
    "\n",
    "> Priority : High\n",
    "\n",
    "> Response : I understand the urgency of your situation. Our team will prioritize resolving your slow internet and disconnection issues. We aim to have this addressed within the next 24 hours to improve your work efficiency. Thank you for your patience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miyKuq5WwtUv"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Generate the draft_response for each support_ticket_text in the df and store it in a new column.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkAhxKF7xBcV"
   },
   "outputs": [],
   "source": [
    "df['draft_response'] = df.apply(lambda row: generate_response(response_prompt, row['support_ticket_text'], row['Category'], row['Metadata'], row['Priority']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyVE0w05-gfW"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07Fny78vk063"
   },
   "source": [
    "## Reviewing Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Snk7Nu1jDvd"
   },
   "source": [
    "Now that we have converted all the support tickets into structured data, let's review a few tickets along with their classifications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4VJLk9MtQiX"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Create a function that takes a support ticket ID as input and displays the details extracted from the model on a separate line.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgkjRP3PjEzZ"
   },
   "outputs": [],
   "source": [
    "def display_ticket_details(ticket_id, dataframe):\n",
    "  \"\"\"\n",
    "  Displays all details of a support ticket from a DataFrame, with each detail on a new line.\n",
    "\n",
    "  Args:\n",
    "    ticket_id (str): The ID of the support ticket to display.\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the support ticket data.\n",
    "  \"\"\"\n",
    "  ticket_details = dataframe[dataframe['support_tick_id'] == ticket_id]\n",
    "\n",
    "  if not ticket_details.empty:\n",
    "    for column in ticket_details.columns:\n",
    "      print(f\"{column}: {ticket_details.iloc[0][column]}\")\n",
    "  else:\n",
    "    print(f\"Ticket with ID {ticket_id} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsJd2se4kUDK"
   },
   "source": [
    "### Ticket ID: ST2023-009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKUmv15KkPmT"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "display_ticket_details('ST2023-009', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdvAQcq159mh"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "- The system correctly identifies the device as *Router* and the problem type as *Weak Wi-Fi Signal*, matching the ticket description.\n",
    "- The impact is rightly marked as *Moderate*, as the issue is persistent and affects usability.\n",
    "- The draft response is polite, empathetic, and includes a clear resolution timeline, which helps manage user expectations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGv_lU9mkntt"
   },
   "source": [
    "### Ticket ID: ST2023-018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsO1z03Hkntu"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "display_ticket_details('ST2023-018', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bP96ZOd36Hf_"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "- The system accurately detects the *Device* as **Laptop** and the *Problem Type* as **Liquid Damage**, aligning with the ticket text.\n",
    "- The *User Impact* is rightly set as **Major**, given the device won‚Äôt turn on, and *Priority* is appropriately marked as **High**.\n",
    "- The draft response is empathetic and assures immediate action with a ETA 2-3 days, which builds user trust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07i8IrXkkWMb"
   },
   "source": [
    "### Ticket ID: ST2023-023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geb4mxwhkbPv"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "display_ticket_details('ST2023-023', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51unCVOq6Rcp"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "- The *Device* is correctly identified as **USB Drive**, and the *Problem Type* as **Formatted Drive**, matching the user‚Äôs concern.\n",
    "- The *User Impact* is rightly marked **Major** due to potential data loss, with *Priority* set to **High**.\n",
    "- The draft reply is empathetic and ensures quick action with a clear resolution timeline of **24 hours**, reassuring the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zmr3wnbT_jAX"
   },
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXKQbrnoMmxX"
   },
   "source": [
    "Now, we deploy the system using Hugging Face Spaces with Streamlit.\n",
    "\n",
    "> **Note:** This is purely for demonstration purposes, so we won't dive deep into the code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHKUaar2KtDi"
   },
   "source": [
    "## Streamlit on Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l54U7FCxDnDl"
   },
   "source": [
    "### app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKWjfsLj8mbR"
   },
   "source": [
    "**NOTE**: You can use this prompt to generate the code for buidling the Streamlit app. However, a few minor adjustments might be needed afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3sEOz9O8X2q"
   },
   "source": [
    "***Prompt***:\n",
    "\n",
    "<font size=3 color=\"#4682B4\"><b> Create a complete Streamlit app that includes all required functions and prompts, uses the OpenAI API to fetch results, and displays all the final outputs at the end.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_FsPjxIIZKa"
   },
   "outputs": [],
   "source": [
    "# %%writefile app.py\n",
    "\n",
    "# import streamlit as st\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# import os\n",
    "# from openai import OpenAI\n",
    "\n",
    "# # Load OpenAI API key and base URL from Colab secrets\n",
    "# try:\n",
    "#     OPENAI_API_KEY = os.environ.get(\"API_KEY\")\n",
    "#     OPENAI_API_BASE = os.environ.get(\"API_BASE\")\n",
    "#     openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)\n",
    "# except Exception as e:\n",
    "#     st.error(f\"Error loading OpenAI credentials: {e}\")\n",
    "#     st.stop()\n",
    "\n",
    "\n",
    "# # Define the functions for categorization, metadata extraction, priority prediction, and response generation\n",
    "# def query_openai(prompt, query):\n",
    "#     \"\"\"\n",
    "#     Queries the OpenAI model with a given prompt and query.\n",
    "#     Args:\n",
    "#         prompt (str): The prompt for the model.\n",
    "#         query (str): The query to be answered by the model.\n",
    "#     Returns:\n",
    "#         str: The model's response.\n",
    "#     \"\"\"\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": prompt},\n",
    "#         {\"role\": \"user\", \"content\": query}\n",
    "#     ]\n",
    "#     response = openai_client.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo\",  # Or another suitable OpenAI model\n",
    "#         messages=messages,\n",
    "#         max_tokens=100  # Adjust max_tokens as needed\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "# def classify_ticket(prompt, query):\n",
    "#     \"\"\"\n",
    "#     Classifies a support ticket using the OpenAI model and returns the result in JSON format.\n",
    "#     Args:\n",
    "#         prompt (str): The classification prompt for the model.\n",
    "#         query (str): The support ticket text to be classified.\n",
    "#     Returns:\n",
    "#         dict: A dictionary containing the classification result, or None if classification fails.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         response_text = query_openai(prompt, query)\n",
    "#         # Attempt to parse the response text as JSON\n",
    "#         classification_result = json.loads(response_text)\n",
    "#         return classification_result\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         st.error(f\"Error decoding JSON from OpenAI response: {e}\")\n",
    "#         st.text(f\"Raw OpenAI response: {response_text}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         st.error(f\"An unexpected error occurred during classification: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def extract_metadata(prompt, query):\n",
    "#     \"\"\"\n",
    "#     Extracts metadata from a support ticket using the OpenAI model and returns the result in JSON format.\n",
    "#     Args:\n",
    "#         prompt (str): The metadata extraction prompt for the model.\n",
    "#         query (str): The support ticket text to extract metadata from.\n",
    "#     Returns:\n",
    "#         dict: A dictionary containing the extracted metadata, or None if extraction fails.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         response_text = query_openai(prompt, query)\n",
    "#         # Attempt to parse the response text as JSON\n",
    "#         metadata_result = json.loads(response_text)\n",
    "#         return metadata_result\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         st.error(f\"Error decoding JSON from OpenAI response: {e}\")\n",
    "#         st.text(f\"Raw OpenAI response: {response_text}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         st.error(f\"An unexpected error occurred during metadata extraction: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def predict_priority(prompt, query, problem_type, user_impact):\n",
    "#     \"\"\"\n",
    "#     Predicts the priority of a support ticket using the OpenAI model and returns the result in JSON format.\n",
    "#     Args:\n",
    "#         prompt (str): The priority prediction prompt for the model.\n",
    "#         query (str): The support ticket text to predict the priority for.\n",
    "#         problem_type (str): The extracted problem type.\n",
    "#         user_impact (str): The extracted user impact.\n",
    "#     Returns:\n",
    "#         dict: A dictionary containing the predicted priority, or None if prediction fails.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Include problem_type and user_impact in the query sent to the model\n",
    "#         full_query = f\"\"\"\n",
    "#         Support Ticket: {query}\n",
    "#         Problem Type: {problem_type}\n",
    "#         User Impact: {user_impact}\n",
    "#         Based on the support ticket, problem type, and user impact, predict the priority: Low, Medium, High, or Urgent.\n",
    "#         Return only a structured JSON output in the following format:\n",
    "#         {{\"priority\": \"priority_prediction\"}}\n",
    "#         \"\"\"\n",
    "#         response_text = query_openai(prompt, full_query)\n",
    "#         priority_result = json.loads(response_text)\n",
    "#         return priority_result\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         st.error(f\"Error decoding JSON from OpenAI response: {e}\")\n",
    "#         st.text(f\"Raw OpenAI response: {response_text}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         st.error(f\"An unexpected error occurred during priority prediction: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def generate_response(response_prompt, query, category, metadata_tags, priority):\n",
    "#     \"\"\"\n",
    "#     Generates a draft response for a support ticket using the OpenAI model.\n",
    "#     Args:\n",
    "#         response_prompt (str): The prompt for generating the response.\n",
    "#         query (str): The original support ticket text.\n",
    "#         category (str): The predicted category of the ticket.\n",
    "#         metadata_tags (dict): The extracted metadata tags (Device, Problem Type, User Impact).\n",
    "#         priority (str): The predicted priority of the ticket.\n",
    "#     Returns:\n",
    "#         str: The generated response text, or None if response generation fails.\n",
    "#     \"\"\"\n",
    "#     # Combine the inputs into a single message for the model\n",
    "#     user_message = f\"\"\"\n",
    "#     Support Ticket: {query}\n",
    "#     Category: {category}\n",
    "#     Metadata Tags: {metadata_tags}\n",
    "#     Priority: {priority}\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         # Pass the combined message to the query_openai function\n",
    "#         response_text = query_openai(response_prompt, user_message)\n",
    "#         return response_text\n",
    "#     except Exception as e:\n",
    "#         st.error(f\"An unexpected error occurred during response generation: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Define the prompts\n",
    "# classification_prompt = \"\"\"\n",
    "#  You are a technical assistant. Classify the support ticket based on the Support Ticket Text presented in the input into the following categories and not any other.\n",
    "#       - Technical issues\n",
    "#       - Hardware issues\n",
    "#       - Data recovery\n",
    "#   Return only a structured JSON output in the following format:\n",
    "#   {\"Category\": \"category_prediction\"}\n",
    "# \"\"\"\n",
    "\n",
    "# metadata_prompt = f\"\"\"\n",
    "# You are an intelligent assistant that extracts structured metadata from technical support queries.\n",
    "# Analyze the query and extract the following information:\n",
    "\n",
    "# * Device (e.g., Laptop, Phone, Router, etc.)\n",
    "# * Problem Type (e.g., Not Turning On, Lost Internet, Deleted Files)\n",
    "# * User Impact - Estimate based on how severely the issue affects the user's ability to continue working or using the device:\n",
    "\n",
    "#     - * Major: The user cannot proceed with work at all.\n",
    "#     - * Moderate: The user is impacted but may have a workaround.\n",
    "#     - * Minor: The issue is present but does not significantly hinder usage.\n",
    "\n",
    "# Use the following examples as guidance.\n",
    "\n",
    "# Query Text: My phone battery is draining rapidly even on battery saver mode. I barely use it and it drops 50% in a few hours.\n",
    "# Output: {\"Device\": \"Phone\", \"Problem Type\": \"Battery Draining\", \"User Impact\": \"Minor\"}\n",
    "\n",
    "# Query Text: I accidentally deleted a folder containing all project files. Please help me recover it.\n",
    "# Output: {\"Device\": \"Laptop\", \"Problem Type\": \"Deleted Files\", \"User Impact\": \"Major\"}\n",
    "\n",
    "# Query Text: My router is not working.\n",
    "# Output: {\"Device\": \"Router\", \"Problem Type\": \"Lost Internet\", \"User Impact\": \"Moderate\"}\n",
    "\n",
    "# Return the final output only in a valid JSON format without any additional explanation.\n",
    "# \"\"\"\n",
    "\n",
    "# priority_prompt =\"\"\"\n",
    "# You are an intelligent assistant that determines the priority level of a support ticket.\n",
    "# For any given ticket, follow this step-by-step reasoning process to assign the correct priority level: Low, Medium, High.\n",
    "# Step-by-step Evaluation:\n",
    "# Is the device or service completely unusable?\n",
    "# Is the issue blocking critical or time-sensitive work?\n",
    "# Is there a specific deadline or urgency mentioned by the user?\n",
    "# Does the user mention partial functionality or ongoing work?\n",
    "# Is the tone or language expressing frustration or emergency?\n",
    "# After evaluating each step, decide the most appropriate priority level based on the impact and urgency.\n",
    "# Finally, return only the structured output in valid JSON format, like this:\n",
    "# {\"priority\": \"High\"}\n",
    "# Do not include your reasoning in the output ‚Äî just the JSON.\n",
    "# \"\"\"\n",
    "\n",
    "# response_prompt = \"\"\"\n",
    "# You are provided with a support ticket's text along with its Category, Tags, and assigned Priority level.\n",
    "\n",
    "# Follow these steps before generating your final response:\n",
    "\n",
    "# 1. Analyze the ticket text to understand the customer's sentiment and main concern.\n",
    "# 2. Identify the issue type using the provided Category and Tags.\n",
    "# 3. Determine the appropriate ETA based on the Priority level.\n",
    "# 4. Compose a short, empathetic response that reassures the customer, acknowledges their concern, and includes the ETA.\n",
    "\n",
    "# Ensure the final response:\n",
    "\n",
    "# 1. Is under 50 words\n",
    "# 2. Has a polite and empathetic tone\n",
    "# 3. Addresses the issue clearly\n",
    "\n",
    "# Return only the final response to the customer. Do not include your reasoning steps in the output.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# # Streamlit App\n",
    "# st.title(\"Support Ticket Categorization System\")\n",
    "\n",
    "# st.write(\"Enter the support ticket text below:\")\n",
    "\n",
    "# support_ticket_input = st.text_area(\"Support Ticket Text\", height=200)\n",
    "\n",
    "# if st.button(\"Process Ticket\"):\n",
    "#     if support_ticket_input:\n",
    "#         st.write(\"Processing...\")\n",
    "\n",
    "#         # Categorization\n",
    "#         category_result = classify_ticket(classification_prompt, support_ticket_input)\n",
    "#         category = category_result.get('Category') if category_result else \"N/A\"\n",
    "#         st.subheader(\"Category:\")\n",
    "#         st.write(category)\n",
    "\n",
    "#         # Metadata Extraction\n",
    "#         metadata_result = extract_metadata(metadata_prompt, support_ticket_input)\n",
    "#         device = metadata_result.get('Device') if metadata_result else \"N/A\"\n",
    "#         problem_type = metadata_result.get('Problem Type') if metadata_result else \"N/A\"\n",
    "#         user_impact = metadata_result.get('User Impact') if metadata_result else \"N/A\"\n",
    "\n",
    "#         st.subheader(\"Metadata:\")\n",
    "#         st.write(f\"Device: {device}\")\n",
    "#         st.write(f\"Problem Type: {problem_type}\")\n",
    "#         st.write(f\"User Impact: {user_impact}\")\n",
    "\n",
    "#         # Priority Prediction\n",
    "#         priority_result = predict_priority(priority_prompt, support_ticket_input, problem_type, user_impact)\n",
    "#         priority = priority_result.get('priority') if priority_result else \"N/A\"\n",
    "#         st.subheader(\"Priority:\")\n",
    "#         st.write(priority)\n",
    "\n",
    "#         # Draft Response Generation\n",
    "#         draft_response = generate_response(response_prompt, support_ticket_input, category, metadata_result, priority)\n",
    "#         st.subheader(\"Draft Response:\")\n",
    "#         st.write(draft_response)\n",
    "\n",
    "\n",
    "#     else:\n",
    "#         st.warning(\"Please enter support ticket text to process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17VcIdIcDadL"
   },
   "source": [
    "### Docker File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASfiJU8kDXsP"
   },
   "outputs": [],
   "source": [
    "# %%writefile Dockerfile\n",
    "\n",
    "# FROM python:3.9-slim\n",
    "\n",
    "# WORKDIR /app\n",
    "\n",
    "# RUN apt-get update && apt-get install -y \\\n",
    "#     build-essential \\\n",
    "#     curl \\\n",
    "#     software-properties-common \\\n",
    "#     git \\\n",
    "#     && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# COPY requirements.txt ./\n",
    "# COPY app.py ./\n",
    "# COPY src/ ./src/\n",
    "\n",
    "# RUN pip3 install -r requirements.txt\n",
    "\n",
    "# EXPOSE 8501\n",
    "\n",
    "# HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n",
    "\n",
    "# ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DSuuldPKKId"
   },
   "source": [
    "### requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4lT85myKON8"
   },
   "outputs": [],
   "source": [
    "# %%writefile requirements.txt\n",
    "# altair==5.5.0\n",
    "# pandas==2.2.2\n",
    "# streamlit==1.47.1\n",
    "# openai==1.97.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgOKkAclIi46"
   },
   "source": [
    "## Hugging Face Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nTsv5ZqoH-z"
   },
   "source": [
    "#### 1. Login to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um5a84WIoP2j"
   },
   "source": [
    "Go to [Hugging face](https://huggingface.co) and sign up or log in to your account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCzdrLYgoYsG"
   },
   "source": [
    "#### 2. Create a New Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iReg7welocEh"
   },
   "source": [
    "   * Navigate to [Hugging face Spaces ](https://huggingface.co/spaces).\n",
    "   * Click **Create New Space**.\n",
    "   * Fill in:\n",
    "\n",
    "     * Name for your Space.\n",
    "     * Space SDK: Select **Docker**.\n",
    "     * Choose a Docker template: Select **Streamlit**\n",
    "     * Visibility: Choose *Public* or *Private*.\n",
    "   * Click **Create Space**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_GU8gcMJ5e1"
   },
   "source": [
    "#### 3. Setup OpenAI tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3p8iaRcqJwPk"
   },
   "source": [
    "- Open your Hugging Face Space\n",
    "\n",
    "- Click on the **‚ÄúSettings‚Äù** tab at the top of the Space.\n",
    "\n",
    "- Scroll down to the **‚ÄúRepository secrets‚Äù** section.\n",
    "\n",
    "- Click **‚ÄúAdd a new secret‚Äù**.\n",
    "\n",
    "- In the **Name** field, type token name\n",
    "\n",
    "- In the **Secret** field, paste your secret key\n",
    "\n",
    "- Click **‚ÄúAdd secret‚Äù** to save it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrEhsirkos1w"
   },
   "source": [
    "#### 4. Upload Your Files\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f2e8YFbo2Jg"
   },
   "source": [
    "   * In the new Space, click the **Files** tab.\n",
    "   * Delete existing requirements.txt , DockerFile\n",
    "   * Click Contribute then Upload files and add:\n",
    "\n",
    "     * `app.py`\n",
    "     * `requirements.txt`\n",
    "     * `DockerFile`\n",
    "   * Commit the upload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMJf9s2WpMoi"
   },
   "source": [
    "#### 5. Build and Launch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7MBsOPppdvk"
   },
   "source": [
    "   * Hugging Face will automatically detect the `Dockerfile` and start building the container.\n",
    "   * Wait a few minutes for the build to complete and the app to go live."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIHFm30QpluH"
   },
   "source": [
    "#### 6. Access and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_hHgiFlpoN0"
   },
   "source": [
    "* Go to the App tab or the Space URL to view and test your running Streamlit app.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_OxiKnrjQ1M"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcP6LhUoCd0Z"
   },
   "source": [
    "The primary objective here was to demonstrate how we can convert unstructured support ticket text into structured information using large language models and prompt engineering.  \n",
    "\n",
    "By classifying tickets into categories, assigning priorities, and generating structured outputs in JSON format, we‚Äôve created a scalable and reusable system that can significantly streamline customer support operations.\n",
    "\n",
    "\n",
    "**Business Impact**\n",
    "\n",
    "- **Improved Efficiency:** Automating ticket classification and prioritization helps support teams triage issues faster, reducing resolution time.\n",
    "- **Better Resource Allocation:** Tickets can be routed based on urgency and category, ensuring critical issues are addressed first.\n",
    "- **Data-Driven Insights:** Structured data enables better tracking, reporting, and trend analysis.\n",
    "\n",
    "This approach serves as a strong foundation for building intelligent customer support systems that are adaptable, efficient, and business-aligned.\n",
    "\n",
    "**Improvement Areas**\n",
    "\n",
    "- The current system lacks access to the latest company policies and procedural updates, making responses generic or outdated. RAG addresses this by retrieving the most relevant, up-to-date internal documents to generate accurate and context-aware replies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybRlzaIhWaM9"
   },
   "source": [
    "<font size=6 color='#4682B4'>Power Ahead</font>\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5Bxnkoxf270y",
    "AAaZfw3ae5hc",
    "UfqBaEwD3FNh",
    "RO7H34JN34vK",
    "17VcIdIcDadL",
    "5DSuuldPKKId",
    "LgOKkAclIi46",
    "7nTsv5ZqoH-z",
    "MCzdrLYgoYsG",
    "N_GU8gcMJ5e1",
    "mrEhsirkos1w",
    "mMJf9s2WpMoi",
    "nIHFm30QpluH"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
