{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "lQMN7B5FKR2S",
      "metadata": {
        "id": "lQMN7B5FKR2S"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=10>Generative AI for Business Applications</center></font>\n",
        "<center><font size=6>Large Language Models & Prompt Engineering - Week 2</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HxgFkEuv1raU",
      "metadata": {
        "id": "HxgFkEuv1raU"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://images.pexels.com/photos/262918/pexels-photo-262918.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1\" width=720/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=6>Restaurant Review Analysis</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G-jLkpBYdP89",
      "metadata": {
        "id": "G-jLkpBYdP89"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xClk0SOxdU_s",
      "metadata": {
        "id": "xClk0SOxdU_s"
      },
      "source": [
        "### Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cPfUxkrbQpmc",
      "metadata": {
        "id": "cPfUxkrbQpmc"
      },
      "source": [
        "In the food industry, customer satisfaction plays a pivotal role in shaping the success of individual outlets and the overall brand. A leading global food aggregator is keen on understanding and improving customer experiences across the diverse range of restaurants it lists on its platform. The company recognizes the significance of customer reviews in gaining insights into service quality, food offerings, and overall satisfaction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mgGZFBqvdX3_",
      "metadata": {
        "id": "mgGZFBqvdX3_"
      },
      "source": [
        "### Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Q0UjpRLUteW",
      "metadata": {
        "id": "5Q0UjpRLUteW"
      },
      "source": [
        "The objective is to develop a **Large Language Model (LLM)-based sentiment analysis system** that can extract meaningful insights from restaurant reviews using only **prompt engineering** (without Retrieval-Augmented Generation). The system will:\n",
        "\n",
        "1. Identify the **overall sentiment** (positive, negative, neutral) for each review.\n",
        "2. Capture **aspect-level sentiments** for key experience categories such as food quality, service, and ambience.\n",
        "3. Extract **liked and disliked features** within each aspect to provide granular insights for each restaurant.\n",
        "\n",
        "This approach aims to enable **scalable, automated review analysis** that helps restaurants understand customer feedback in detail, improve service quality, and enhance customer satisfaction, all achieved through **carefully designed prompts**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Dictionary"
      ],
      "metadata": {
        "id": "G9vXfqH0ZUzv"
      },
      "id": "G9vXfqH0ZUzv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset comprises three columns:\n",
        "\n",
        "1. **restaurant\\_id** – Unique identifier for each restaurant.\n",
        "2. **rating\\_review** – Numerical or categorical rating provided by the customer.\n",
        "3. **review\\_full** – Full text of the customer’s review.\n"
      ],
      "metadata": {
        "id": "FKjxe7wPZYjr"
      },
      "id": "FKjxe7wPZYjr"
    },
    {
      "cell_type": "markdown",
      "id": "b8pDl8uVKR2W",
      "metadata": {
        "id": "b8pDl8uVKR2W"
      },
      "source": [
        "## Installing and Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.53.2 \\\n",
        "                  accelerate==1.8.1 \\\n",
        "                  bitsandbytes==0.46.1"
      ],
      "metadata": {
        "id": "isAaX2zs3yIr"
      },
      "id": "isAaX2zs3yIr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
      ],
      "metadata": {
        "id": "BwSXdAIWLL8Q"
      },
      "id": "BwSXdAIWLL8Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt:**\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>I want to analyze the provided CSV data and work with AI models to understand the restrauant reviews. Help me import the necessary Python libraries to:\n",
        "\n",
        "1. Read and manipulate the data</ul>\n",
        "2. Working with system enviroment\n",
        "3. Use models from Hugging Face with AutoTokenizer and AutoModelForCausalLM\n",
        "\n",
        "</font>\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>\n",
        "These libraries will help us load the data, connect with AI models, and prepare for further steps in the project.\n",
        "\n",
        "</font>"
      ],
      "metadata": {
        "id": "RM5RW5-0TopC"
      },
      "id": "RM5RW5-0TopC"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "wdeAT_1EgI9b"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wdeAT_1EgI9b"
    },
    {
      "cell_type": "markdown",
      "id": "mv_4wV7dTNqQ",
      "metadata": {
        "id": "mv_4wV7dTNqQ"
      },
      "source": [
        "## Import the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er-hahR_HS7L"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Mount the Google Drive\n",
        "</font>"
      ],
      "id": "er-hahR_HS7L"
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yvmBPiheIm4k"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yvmBPiheIm4k"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JhIr_HKrCN5"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Load the CSV file named \"restaurant_reviews.csv\" and store it in the variable data.\n",
        "</font>"
      ],
      "id": "6JhIr_HKrCN5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zb2dDd4ekE_V",
      "metadata": {
        "id": "zb2dDd4ekE_V"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"restaurant_reviews.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H-51wMmpGzcr",
      "metadata": {
        "id": "H-51wMmpGzcr"
      },
      "source": [
        "## Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J9f9vmyHir0"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Display the first 5 rows of the `data`.\n",
        "</font>"
      ],
      "id": "0J9f9vmyHir0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NA2wS2mVkK7X",
      "metadata": {
        "id": "NA2wS2mVkK7X"
      },
      "outputs": [],
      "source": [
        "# checking the first five rows of the data\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23rEgLArjbuo"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Display the number of rows and columns in the `data`.\n",
        "</font>"
      ],
      "id": "23rEgLArjbuo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fLXRyDA4m3S"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ],
      "id": "4fLXRyDA4m3S"
    },
    {
      "cell_type": "markdown",
      "id": "udY09oFqTm1B",
      "metadata": {
        "id": "udY09oFqTm1B"
      },
      "source": [
        "**Observations**\n",
        "\n",
        "- Data has 20 rows and 3 columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qMV8hC_2G9aW",
      "metadata": {
        "id": "qMV8hC_2G9aW"
      },
      "outputs": [],
      "source": [
        "# checking for missing values\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8zETWVmeHBIk",
      "metadata": {
        "id": "8zETWVmeHBIk"
      },
      "source": [
        "**Observations**\n",
        "\n",
        "- There are no missing values in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ScqJfjzyajgD",
      "metadata": {
        "id": "ScqJfjzyajgD"
      },
      "outputs": [],
      "source": [
        "# creating a copy of the data\n",
        "df = data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L5pIwimGK5_1",
      "metadata": {
        "id": "L5pIwimGK5_1"
      },
      "source": [
        "# Model Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**\n",
        "\n",
        "1. We're loading the entire model, which might take some time to initialize. To optimize this, we use 8-bit loading to reduce memory usage and speed up inference without significantly impacting performance.\n",
        "\n",
        "2. Before loading the model, you must first agree to its terms and conditions on Hugging Face. To do this, search for the model on the Hugging Face website, review its license or usage restrictions, and click “Agree and Access” to enable programmatic access via code.\n"
      ],
      "metadata": {
        "id": "EzRIuDrAS3lU"
      },
      "id": "EzRIuDrAS3lU"
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'config.json'                                                       # Name of the configuration file\n",
        "with open(file_name, 'r') as file:                                              # Open the config file in read mode\n",
        "    config = json.load(file)                                                    # Load the JSON content as a dictionary\n",
        "    HF_TOKEN = config.get(\"HF_TOKEN\")\n",
        "\n",
        "\n",
        "# Store API credentials in environment variables\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN\n"
      ],
      "metadata": {
        "id": "f7Katz_Yw4dE"
      },
      "id": "f7Katz_Yw4dE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDjUfRIOzuWn"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Load the `mistralai/Mistral-7B-Instruct-v0.1` from hugging face using 8-bit quantization.\n",
        "\n",
        "</font>"
      ],
      "id": "qDjUfRIOzuWn"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    load_in_8bit=True,  # Load the model with 8-bit quantization\n",
        "    torch_dtype=torch.float16,         # Use 16-bit floats on GPU\n",
        "    device_map=\"auto\",                 # Automatically assign GPU or CPU\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "xwBfPKrgn4VP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xwBfPKrgn4VP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `load_in_8bit=True`: Loads the model using 8-bit quantization to save memory.\n",
        "* `torch_dtype=torch.float16`: Uses half-precision (16-bit) floats for faster computation on GPU.\n",
        "* `device_map=\"auto\"`: Automatically places model layers across available devices.\n"
      ],
      "metadata": {
        "id": "8S5Ithq_EzCQ"
      },
      "id": "8S5Ithq_EzCQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face model is now ready. Let’s test it on an example input."
      ],
      "metadata": {
        "id": "_sR-dGKF_UBW"
      },
      "id": "_sR-dGKF_UBW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAsdu3G5nIOC"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Ask the Mistral model: What is the capital of France?\n",
        "</font>"
      ],
      "id": "VAsdu3G5nIOC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt (question)\n",
        "prompt = \"### Question: What is the capital of France?\\n### Answer:\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(**inputs)\n",
        "\n",
        "# Decode and print the output\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "edSsEJ5UsErS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "edSsEJ5UsErS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the model is returning results successfully."
      ],
      "metadata": {
        "id": "YG4KRgMA_kYC"
      },
      "id": "YG4KRgMA_kYC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s define a function that takes a `prompt` and a `query` as inputs and returns the model’s output.  \n",
        "\n",
        "This will make it easier to reuse the model across different inputs."
      ],
      "metadata": {
        "id": "hefY6U1-_nH2"
      },
      "id": "hefY6U1-_nH2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDCQOP1NnwaN"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Create a function that accepts a prompt and query, and returns the response generated by the Mistral model.\n",
        "</font>"
      ],
      "id": "NDCQOP1NnwaN"
    },
    {
      "cell_type": "code",
      "source": [
        "def query_mistral(prompt, query):\n",
        "    \"\"\"\n",
        "    Queries the Mistral model with a given prompt and query.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt for the model.\n",
        "        query (str): The query to be answered by the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "\n",
        "    attention_mask = (inputs != pad_token_id).long()\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=300,  # Adjust as needed\n",
        "        do_sample=True,\n",
        "        temperature=0.7,     # Adjust as needed\n",
        "        top_p=0.9,           # Adjust as needed\n",
        "        pad_token_id=pad_token_id  # Prevents warning\n",
        "    )\n",
        "\n",
        "    # Decode and print the output, skipping the input tokens\n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "UminawUZtUL3"
      },
      "execution_count": null,
      "outputs": [],
      "id": "UminawUZtUL3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code snippet defined above, the following components were used before generation:\n",
        "\n",
        "1. `tokenizer.apply_chat_template()`: This method converts the `messages` list into a single formatted string (e.g., adding special tokens or chat-style formatting), and tokenizes it into a tensor using PyTorch (`return_tensors=\"pt\"`). The `.to(model.device)` part ensures the tokenized input is moved to the same device as the model (like a GPU or CPU).\n",
        "\n",
        "2. `pad_token_id`: This variable is assigned the padding token ID used by the tokenizer. If the tokenizer does not explicitly define a `pad_token_id`, it falls back to the `eos_token_id` (end-of-sequence token). This is needed to handle padding properly during attention and generation.\n",
        "\n",
        "3. `attention_mask:` This creates a mask that tells the model which tokens should be attended to (represented by 1) and which should be ignored (usually padding tokens, represented by 0). It ensures the model focuses only on valid input tokens during processing.\n",
        "\n",
        "In the `generate()` function defined above, the following arguments were used:\n",
        "\n",
        "1. `max_new_tokens`: This parameter determines the maximum length of the generated sequence. In the provided code, max_new_tokens is set to 100, which means the generated sequence should not exceed 100 tokens.\n",
        "\n",
        "2. `temperature`: The temperature parameter controls the level of randomness in the generation process. A higher temperature (e.g., closer to 1) makes the output more diverse and creative but potentially less focused, while a lower temperature (e.g., close to 0) produces more deterministic and focused but potentially repetitive outputs. In the code, temperature is set to 0.7, indicating a very low temperature and, consequently, a more deterministic sampling.\n",
        "\n",
        "3. `do_sample`: This is a boolean parameter that determines whether to use sampling during generation (do_sample=True) or use greedy decoding (do_sample=False). When set to True, as in the provided code, the model samples from the distribution of predicted tokens at each step, introducing randomness in the generation process.\n",
        "\n",
        "4. `top_p`: Controls how many top probable tokens to consider during generation. If set to 0.9, it samples from the smallest set of tokens whose combined probability is at least 90%, balancing creativity and coherence.\n"
      ],
      "metadata": {
        "id": "18d1tmR1mVqw"
      },
      "id": "18d1tmR1mVqw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reviews Sentiment Analysis"
      ],
      "metadata": {
        "id": "sM3OI3bvyo6x"
      },
      "id": "sM3OI3bvyo6x"
    },
    {
      "cell_type": "markdown",
      "id": "sq1Aq8BXajfg",
      "metadata": {
        "id": "sq1Aq8BXajfg"
      },
      "source": [
        "## 1. Overall Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kOCUHdAiajgE",
      "metadata": {
        "id": "kOCUHdAiajgE"
      },
      "outputs": [],
      "source": [
        "# defining the instructions for the model\n",
        "instruction_1 = \"\"\"\n",
        "    You are an AI analyzing restaurant reviews. Classify the sentiment of the provided review into the following categories:\n",
        "    - Positive\n",
        "    - Negative\n",
        "    - Neutral\n",
        "\n",
        "    And return in only JSON format. No extra text and analysis\n",
        "    {\"Sentiment\":\"Positive\"}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSSp4w-O3FNh"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Define a function named classify_sentiment that takes the instructtion_1 and the review text as input, gets the result from query_mistral function, and returns the result in a JSON format.\n",
        "</font>"
      ],
      "id": "qSSp4w-O3FNh"
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentiment(prompt, query):\n",
        "    try:\n",
        "        response_text = query_mistral(prompt, query)\n",
        "        # Attempt to parse the response text as JSON\n",
        "        classification_result = json.loads(response_text)\n",
        "        return classification_result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
        "        print(f\"Raw OpenAI response: {response_text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "2iv3KecC3FNi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2iv3KecC3FNi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlpGNtUyrAhN"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>Generate the category for each support_ticket_text in the DataFrame using the classify_ticket_openai function, and store the result in a new column.\n",
        "\n",
        "</font>"
      ],
      "id": "WlpGNtUyrAhN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the classification function to each row in the DataFrame\n",
        "df['Sentiment'] = df['review_full'].apply(lambda x: classify_sentiment(instruction_1, x)['Sentiment'] if classify_sentiment(instruction_1, x) else None)"
      ],
      "metadata": {
        "id": "W0mIwZjgq24j"
      },
      "execution_count": null,
      "outputs": [],
      "id": "W0mIwZjgq24j"
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "LNThjS9My8Ax"
      },
      "execution_count": null,
      "outputs": [],
      "id": "LNThjS9My8Ax"
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "7pExYwJodmxq"
      },
      "id": "7pExYwJodmxq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across the different restaurants, negative sentiment slightly outweighs positive and neutral feedback, indicating more dissatisfaction overall.\n"
      ],
      "metadata": {
        "id": "kGFGjz-5uuni"
      },
      "id": "kGFGjz-5uuni"
    },
    {
      "cell_type": "markdown",
      "id": "HslGURoah3VI",
      "metadata": {
        "id": "HslGURoah3VI"
      },
      "source": [
        "## 2. Sentiment toward Different Aspects of the Experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YhdIDYq4lf3C",
      "metadata": {
        "id": "YhdIDYq4lf3C"
      },
      "outputs": [],
      "source": [
        "# defining the instructions for the model\n",
        "instruction_2 = \"\"\"\n",
        "    You are an AI analyzing restaurant reviews. Classify the following aspects in the review and classify the sentiment of each aspect as \"Positive\", \"Negative\", or \"Neutral\":\n",
        "    1. \"Food Quality\"\n",
        "    2. \"Service\"\n",
        "    3. \"Ambience\"\n",
        "\n",
        "    Output the overall sentiment and sentiment for each category in a JSON format with the following keys:\n",
        "    {\n",
        "        \"Food Quality\": \"your_sentiment_prediction\",\n",
        "        \"Service\": \"your_sentiment_prediction\",\n",
        "        \"Ambience\": \"your_sentiment_prediction\"\n",
        "    }\n",
        "\n",
        "    In case one of the three aspects is not mentioned in the review, set \"Not Applicable\" (including quotes) for the corresponding JSON key value.\n",
        "    Only return the JSON, do not return any other information.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0iXcpcRtgPs"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>Define a function that takes the instruction_2 prompt and query as input get the result from query_mistral function return the result in JSON format\n",
        "</font>"
      ],
      "id": "U0iXcpcRtgPs"
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_aspect_sentiment(prompt, query):\n",
        "    \"\"\"\n",
        "    Classifies the sentiment of aspects of the review using the Mistral model\n",
        "    and returns the result as a JSON object.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt for the model (instruction_2).\n",
        "        query (str): The review text.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the sentiment classification for each aspect,\n",
        "              or None if JSON decoding fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response_text = query_mistral(prompt, query)\n",
        "        # Attempt to parse the response text as JSON\n",
        "        classification_result = json.loads(response_text)\n",
        "        return classification_result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from model response: {e}\")\n",
        "        print(f\"Raw model response: {response_text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "RJNWlKHHtP1I"
      },
      "id": "RJNWlKHHtP1I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xza1kUVVuA4o"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>Generate the aspect_sentiment for each review in the DataFrame using classify_aspect_sentiment, store it in a new column, and extract individual fields into separate columns.\n",
        "</font>"
      ],
      "id": "xza1kUVVuA4o"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the classification function to each row in the DataFrame\n",
        "df['aspect_sentiment'] = df['review_full'].apply(lambda x: classify_aspect_sentiment(instruction_2, x))\n",
        "\n",
        "# Normalize the JSON results into separate columns\n",
        "aspect_sentiment_df = pd.json_normalize(df['aspect_sentiment'])\n",
        "\n",
        "# Concatenate the original DataFrame with the new columns\n",
        "df = pd.concat([df, aspect_sentiment_df], axis=1)"
      ],
      "metadata": {
        "id": "Kw0zm7qrtrvy"
      },
      "id": "Kw0zm7qrtrvy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "gs9SFzut_Y4A"
      },
      "id": "gs9SFzut_Y4A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Food Quality'].value_counts()"
      ],
      "metadata": {
        "id": "vuZ_ePLrdr1j"
      },
      "id": "vuZ_ePLrdr1j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, food quality feedback leans negative, with 8 unfavorable mentions compared to 6 positive, while a few reviews were neutral or not applicable.\n"
      ],
      "metadata": {
        "id": "bZXu8p-Hu0eS"
      },
      "id": "bZXu8p-Hu0eS"
    },
    {
      "cell_type": "code",
      "source": [
        "df['Service'].value_counts()"
      ],
      "metadata": {
        "id": "g_zDHZiud_YP"
      },
      "id": "g_zDHZiud_YP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Service-related feedback is predominantly negative, with 10 unfavorable mentions outweighing the 8 positive and 2 neutral reviews.\n"
      ],
      "metadata": {
        "id": "Kch1rXhju4XI"
      },
      "id": "Kch1rXhju4XI"
    },
    {
      "cell_type": "code",
      "source": [
        "df['Ambience'].value_counts()"
      ],
      "metadata": {
        "id": "KV8H2j0VeDiT"
      },
      "id": "KV8H2j0VeDiT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ambience is viewed largely positively, with 8 favorable mentions and minimal negative feedback.\n"
      ],
      "metadata": {
        "id": "FFxSEUuku9xl"
      },
      "id": "FFxSEUuku9xl"
    },
    {
      "cell_type": "markdown",
      "id": "RxpHCNQeh3VL",
      "metadata": {
        "id": "RxpHCNQeh3VL"
      },
      "source": [
        "## 3. Identifying Liked/Disliked Features of the Different Aspects of the Experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iJVySI-LcX3X",
      "metadata": {
        "id": "iJVySI-LcX3X"
      },
      "outputs": [],
      "source": [
        "# defining the instructions for the model\n",
        "instruction_3 = \"\"\"\n",
        "\n",
        "You are an AI model assigned to analyze restaurant reviews. Your task is to extract the **specific features** that the customer **liked or disliked**, categorized under the following aspects of the dining experience:\n",
        "\n",
        "* Food Quality\n",
        "* Service\n",
        "* Ambience\n",
        "\n",
        "Return the result in the following strict JSON format:\n",
        "\n",
        "{\n",
        "  \"Food Quality Features\": [\"specific liked/disliked features\"],\n",
        "  \"Service Features\": [\"specific liked/disliked features\"],\n",
        "  \"Ambience Features\": [\"specific liked/disliked features\"]\n",
        "}\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "* Only list **concrete features** (e.g., “taste”, “temperature”, “presentation”, “waiting time”, “staff behavior”, “lighting”, “music volume”) that are mentioned positively or negatively in the review.\n",
        "* Do **not** include generic phrases like “liked feature” or “disliked feature”.\n",
        "* If a particular aspect has no feature mentioned, return an empty list for that aspect.\n",
        "* Output **only the JSON**, with keys exactly as specified. Do not add any explanations or comments.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKZ6Hp9Ozf_U"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>Define a function that takes the instruction_3 prompt and query as input get the result from query_mistral function return the result in JSON format\n",
        "</font>"
      ],
      "id": "VKZ6Hp9Ozf_U"
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_features(prompt, query):\n",
        "    \"\"\"\n",
        "    Extracts liked/disliked features from the review using the Mistral model\n",
        "    and returns the result as a JSON object.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt for the model (instruction_3).\n",
        "        query (str): The review text.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted features for each aspect,\n",
        "              or None if JSON decoding fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response_text = query_mistral(prompt, query)\n",
        "        # Attempt to parse the response text as JSON\n",
        "        classification_result = json.loads(response_text)\n",
        "        return classification_result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from model response: {e}\")\n",
        "        print(f\"Raw model response: {response_text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "WubjFDEEy4WF"
      },
      "id": "WubjFDEEy4WF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKe-3DzQzgmO"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>Generate the Features for each review in the DataFrame using classify_features, store them in a new column, and extract individual fields into separate columns.\n",
        "</font>"
      ],
      "id": "UKe-3DzQzgmO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the classification function to each row in the DataFrame\n",
        "df['aspect_features'] = df['review_full'].apply(lambda x: classify_features(instruction_3, x))\n",
        "\n",
        "# Normalize the JSON results into separate columns\n",
        "aspect_features_df = pd.json_normalize(df['aspect_features'])\n",
        "\n",
        "# Concatenate the original DataFrame with the new columns\n",
        "df = pd.concat([df, aspect_features_df], axis=1)"
      ],
      "metadata": {
        "id": "iVvO4njmzTuT"
      },
      "id": "iVvO4njmzTuT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Go5vZ5Us-f-Q"
      },
      "id": "Go5vZ5Us-f-Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0k9WJ3bXxcdz",
      "metadata": {
        "id": "0k9WJ3bXxcdz"
      },
      "source": [
        "## 4. Sharing a Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AuaVtos0xcd0",
      "metadata": {
        "id": "AuaVtos0xcd0"
      },
      "outputs": [],
      "source": [
        "# defining the instructions for the model\n",
        "instruction_4 = \"\"\"\n",
        "You are an AI analyzing restaurant reviews. Your task is to generate a **polite and empathetic response** directly based on the sentiment of the review.\n",
        "\n",
        "Follow this structure:\n",
        "\n",
        "* Start with a thank you for their feedback.\n",
        "* Then:\n",
        "\n",
        "  1. If the review is positive, say you’re glad they enjoyed the experience and that it would be great to have them again.\n",
        "  2. If the review is neutral, thank them and ask what the restaurant could have done better.\n",
        "  3. If the review is negative, apologize for the inconvenience and mention that the team will look into the concerns raised.\n",
        "\n",
        "Constraints:\n",
        "\n",
        "* Do not start with “Dear Customer” or any greeting.\n",
        "* Only output the final response. No sentiment label, explanation, or extra text.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbOcIrg02xl0"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>Define a function that takes the instruction_4 prompt and query as input, gets the result from the query_mistral function, and returns the result\n",
        "</font>"
      ],
      "id": "EbOcIrg02xl0"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_customer_response(prompt, query):\n",
        "    \"\"\"\n",
        "    Generates a customer response based on the review using the Mistral model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt for the model (instruction_4).\n",
        "        query (str): The review text.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated customer response.\n",
        "    \"\"\"\n",
        "    response_text = query_mistral(prompt, query)\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "st9vAczB2agZ"
      },
      "id": "st9vAczB2agZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYPJysiD39ws"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>Generate the response for each review in the DataFrame using generate_customer_response, and store them in a new column.\n",
        "</font>"
      ],
      "id": "yYPJysiD39ws"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the classification function to each row in the DataFrame\n",
        "df['customer_response'] = df['review_full'].apply(lambda x: generate_customer_response(instruction_4, x))"
      ],
      "metadata": {
        "id": "S9XjtoYB307W"
      },
      "id": "S9XjtoYB307W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "BVAum9vN_SjW"
      },
      "id": "BVAum9vN_SjW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "H3A46S7Tx_U6",
      "metadata": {
        "id": "H3A46S7Tx_U6"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "muFFmuCnyA43",
      "metadata": {
        "id": "muFFmuCnyA43"
      },
      "source": [
        "We used a Large Language Model (LLM) in a **multi-stage process** to progressively extract richer insights from restaurant reviews:\n",
        "\n",
        "1. We began by identifying the **overall sentiment** of each review, which showed that across restaurants, negative sentiment (8 reviews) slightly outweighed positive (6) and neutral (6).\n",
        "2. We then extended the analysis to capture **sentiment for specific aspects** of the customer experience (food quality, service, ambience):\n",
        "\n",
        "   * **Food Quality** – 8 negative, 6 positive, 5 neutral, 1 not applicable\n",
        "   * **Service** – 10 negative, 8 positive, 2 neutral (most criticized aspect)\n",
        "   * **Ambience** – 8 positive, 6 neutral, 5 not applicable, 1 negative (strongest positive driver)\n",
        "3. Next, we extracted **metadata** for each review, food quality feature, service feature and ambience feature, enabling restaurant-specific insights.\n",
        "4. Finally, we generated a **personalized response** that could be shared with the customer based on their review content, overall sentiment, and aspect-level feedback.\n",
        "\n",
        "To evaluate the LLM's performance, we can **manually label** a subset of data (for overall and aspect-level sentiments) and **compare it with the model's output** to obtain a quantitative measure of accuracy and reliability.\n",
        "\n",
        "To further improve performance, we explored several tuning strategies, including:\n",
        "\n",
        "* **Refining the prompt** for clarity and specificity\n",
        "* **Adjusting model parameters** such as `temperature`, `top_p`, and others to control response diversity and confidence\n",
        "\n",
        "This step-by-step approach allows for scalable, automated review analysis while maintaining control over **insight quality, depth, and customer engagement tone**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybRlzaIhWaM9"
      },
      "source": [
        "<font size=6 color='#4682B4'>Power Ahead</font>\n",
        "___"
      ],
      "id": "ybRlzaIhWaM9"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "H-51wMmpGzcr"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}