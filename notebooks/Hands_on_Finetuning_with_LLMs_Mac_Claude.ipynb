{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><p float=\"center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
    "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
    "</p></center>\n",
    "\n",
    "<center><font size=10>Generative AI for Business Applications</center></font>\n",
    "<center><font size=6>Fine-Tuning LLMs - Week 1 (Mac-Compatible Version)</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><p float=\"center\">\n",
    "  <img src=\"\" width=720></a>\n",
    "<center><font size=6>Fine-Tuned AI for Summarizing Insurance Sales Conversations</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An enterprise sales representative at a global insurance provider is preparing for a crucial renewal meeting with one of the largest clients. Over the past year, numerous emails have been exchanged, several calls conducted, and in-person meetings held. However, this valuable context is fragmented across the inbox, CRM records, and call notes.\n",
    "\n",
    "With limited time and growing pressure to personalize service and identify cross-sell opportunities, it is difficult to recall key details, such as the products the client was interested in, concerns raised in the last quarter, and commitments made during previous meetings.\n",
    "\n",
    "This challenge reflects a broader industry problem where client interactions are rich but scattered. Sales teams often face:\n",
    "\n",
    "* **Overload of unstructured data** from emails, calls, and notes.\n",
    "* **Lack of standardized, accurate summaries** to capture client context.\n",
    "* **Manual, error-prone preparation** that consumes significant time.\n",
    "* **Missed upsell and personalization opportunities**, weakening client trust.\n",
    "\n",
    "As a result, client engagement is inconsistent, preparation is inefficient, and revenue opportunities are lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to introduce a **smart assistant** capable of synthesizing multi-modal client interactions and generating precise, context-aware summaries.\n",
    "\n",
    "Such a solution would:\n",
    "\n",
    "* Consolidate insights from emails, CRM logs, call transcripts, and meeting notes.\n",
    "* Deliver concise, tailored client briefs before every touchpoint.\n",
    "* Help sales teams maintain continuity, honor past commitments, and personalize conversations.\n",
    "* Unlock new revenue by surfacing upsell and cross-sell opportunities at the right moment.\n",
    "\n",
    "By reducing preparation time and improving personalization, this assistant can transform client engagement in the insurance sector, strengthen relationships, and drive sustainable growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of two primary columns:\n",
    "\n",
    "Conversation - Contains the raw transcripts of client-sales representative interactions, which are often lengthy, multi-turn, and unstructured.\n",
    "\n",
    "Summary - Provides the corresponding concise, structured summaries of key discussion points, client interests, concerns, and commitments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Solution Approach**\n",
    "Provide a Custom Fine-Tuned AI Model for Sales Interaction Summarization\n",
    "\n",
    "To address this challenge, we propose training a domain-specific fine-tuned language model tailored for enterprise insurance communication.\n",
    "The model will:\n",
    "\n",
    "1. Ingest few multi-modal inputs (emails, transcripts, notes).\n",
    "2. Identify intent, extract key discussion points, client interests, pain points, and commitments.\n",
    "3. Generate concise, actionable summaries under 200 words, customized for enterprise insurance workflows.\n",
    "4. Be fine-tuned on real-world communication data to learn domain-specific vocabulary and interaction patterns.\n",
    "\n",
    "This AI-powered tool will augment sales productivity, enhance client engagement, and ensure consistent follow-ups, turning scattered conversations into strategic intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Installing and Importing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac-compatible installation - removed unsloth and CUDA-specific packages\n",
    "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
    "!pip install transformers==4.51.3\n",
    "!pip install accelerate peft trl==0.15.2\n",
    "!pip install -q datasets evaluate bert-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
    "- This Mac-compatible version uses standard transformers without 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "# Function to check device compatibility for Mac\n",
    "def get_device():\n",
    "    \"\"\"Detect and return the best available device for this system\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"  # Apple Silicon GPU\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda\"  # NVIDIA GPU\n",
    "    else:\n",
    "        return \"cpu\"   # CPU fallback\n",
    "\n",
    "# Function to check if bfloat16 is supported on current hardware\n",
    "def is_bfloat16_supported():\n",
    "    \"\"\"Check if the current hardware supports bfloat16 precision\"\"\"\n",
    "    device = get_device()\n",
    "    if device == \"cuda\":\n",
    "        return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    elif device == \"mps\":\n",
    "        return True  # MPS supports bfloat16\n",
    "    else:\n",
    "        return False  # CPU doesn't support bfloat16\n",
    "\n",
    "# Get and display the device we'll be using\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"bfloat16 supported: {is_bfloat16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Evaluation of LLM before Fine-Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the testing CSV into a Pandas DataFrame\n",
    "testing_data = pd.read_csv(\"../data/finetuning_testing.csv\")\n",
    "\n",
    "# Extract all dialogues into a list for model input\n",
    "test_dialogues = [sample for sample in testing_data['Dialogues']]\n",
    "\n",
    "# Extract all human-written summaries into a list for evaluation\n",
    "test_summaries = [sample for sample in testing_data['Summary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Mistral Model (Mac-Compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                        # Check if model is already loaded\n",
    "if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "    print(\"Loading Mistral model for the first time...\")\n",
    "    \n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Mac-compatible model loading without quantization\n",
    "    print(\"Loading model (this may take a few minutes)...\")\n",
    "    \n",
    "    # Choose appropriate dtype based on device\n",
    "    if device == \"mps\":\n",
    "        dtype = torch.float16  # MPS supports float16\n",
    "    elif device == \"cpu\":\n",
    "        dtype = torch.float32  # CPU works best with float32\n",
    "    else:  # cuda\n",
    "        dtype = torch.float16\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=dtype,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\" if device == \"cuda\" else None\n",
    "    )\n",
    "    \n",
    "    # Move model to appropriate device if not using device_map\n",
    "    if device != \"cuda\":\n",
    "        model = model.to(device)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"✓ Model loaded successfully on {device}!\")\n",
    "else:\n",
    "    print(\"✓ Model already loaded, skipping...\")\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"Using existing model on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for inference (generating predictions)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Alpaca instruction prompt is a general purpose prompt template that can be adapted to any task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Write a concise summary of the following dialogue.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the summaries generated by the model\n",
    "predicted_summaries = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are generating summaries for each dialogue in our test set using the fine-tuned model.\n",
    "\n",
    "**Step-by-step Approach:**\n",
    "\n",
    "1. **Iterate through test dialogues** - `for dialogue in tqdm(test_dialogues):`\n",
    "\n",
    "   * Loops through each test dialogue while showing a progress bar (`tqdm`).\n",
    "\n",
    "2. **Format the prompt**\n",
    "\n",
    "   * Inserts the dialogue into the summarization template.\n",
    "\n",
    "3. **Tokenize input**\n",
    "\n",
    "   * Converts the text prompt into tokens (numbers) and moves them to the appropriate device (MPS/CUDA/CPU).\n",
    "\n",
    "4. **Generate output**\n",
    "\n",
    "   * The model predicts the summary using `.generate()`.\n",
    "   * `max_new_tokens=128`: limits summary length.\n",
    "   * `temperature=0`: makes output deterministic (no randomness).\n",
    "   * `pad_token_id`: ensures proper padding using EOS token.\n",
    "\n",
    "5. **Decode output**\n",
    "\n",
    "   * Converts model tokens back into human-readable text.\n",
    "   * Skips special tokens and cleans formatting.\n",
    "\n",
    "6. **Store prediction**\n",
    "\n",
    "   * Appends the generated summary to `predicted_summaries`.\n",
    "\n",
    "7. **Error handling**\n",
    "\n",
    "   * If an error occurs, it prints the error and continues with the next dialogue instead of stopping.\n",
    "\n",
    "This loop **takes each dialogue -> feeds it to the model -> generates a summary -> saves it for evaluation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED INFERENCE - Mac-friendly version\n",
    "import gc  # For garbage collection\n",
    "import psutil  # For memory monitoring\n",
    "\n",
    "# Get the device from the model and verify it's using GPU\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model device: {device}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Disable gradient computation globally for inference speedup\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# REDUCED Configuration for Mac compatibility\n",
    "BATCH_SIZE = 1  # Process one at a time - more stable on Mac\n",
    "MAX_NEW_TOKENS = 64  # Reduced from 128 for faster generation\n",
    "MAX_LENGTH = 1024  # Reduced from 2048 to save memory\n",
    "\n",
    "# Test with smaller subset first\n",
    "TEST_SUBSET_SIZE = 5  # Only process first 5 dialogues for testing\n",
    "test_subset = test_dialogues[:TEST_SUBSET_SIZE]\n",
    "\n",
    "print(f\"Processing {len(test_subset)} dialogues (subset for testing)\")\n",
    "print(f\"Total available memory: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "\n",
    "# Process dialogues one at a time\n",
    "for i in tqdm(range(0, len(test_subset), BATCH_SIZE), desc=\"Generating summaries\"):\n",
    "    try:\n",
    "        # Memory cleanup before each batch\n",
    "        if i > 0:  # Skip first iteration\n",
    "            gc.collect()\n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "        \n",
    "        # Get current dialogue (just one)\n",
    "        dialogue = test_subset[i]\n",
    "        \n",
    "        # Format prompt\n",
    "        prompt = alpaca_prompt_template.format(dialogue, '')\n",
    "        \n",
    "        # Tokenize with reduced length\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,        \n",
    "            max_length=MAX_LENGTH   # Reduced max length\n",
    "        ).to(device)\n",
    "        \n",
    "        print(f\"Batch {i+1}: Input tokens: {inputs.input_ids.shape[-1]}\")\n",
    "        \n",
    "        # Generate summary\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,  # Reduced tokens\n",
    "                use_cache=True,              \n",
    "                temperature=0.1,             # Small temperature instead of 0\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=True,              # Changed to True with small temperature\n",
    "                early_stopping=True          # Stop when EOS is generated\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        prompt_length = inputs.input_ids.shape[-1]\n",
    "        prediction = tokenizer.decode(\n",
    "            outputs[0][prompt_length:],\n",
    "            skip_special_tokens=True,\n",
    "            cleanup_tokenization_spaces=True\n",
    "        )\n",
    "        \n",
    "        predicted_summaries.append(prediction)\n",
    "        \n",
    "        # Show progress\n",
    "        print(f\"✓ Batch {i+1} completed. Summary preview: {prediction[:100]}...\")\n",
    "        print(f\"Memory usage: {psutil.virtual_memory().percent:.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing dialogue {i}: {e}\")\n",
    "        predicted_summaries.append(\"\")  # Empty summary on failure\n",
    "\n",
    "# Re-enable gradients after inference\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(predicted_summaries)} summaries\")\n",
    "print(\"If this works well, you can increase TEST_SUBSET_SIZE or remove the subset limitation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are evaluating our base model to check how well the generated summaries align with human-written summaries. For this, we are using BERTScore, which measures the semantic similarity between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERTScore** is a metric for evaluating text generation tasks, including summarization, translation, and captioning. Unlike traditional metrics like ROUGE or BLEU that rely on exact word overlaps, BERTScore uses embeddings from a pre-trained BERT model to measure **semantic similarity** between the generated text (predictions) and the human-written text (references). This makes it more robust in capturing meaning, even when different words are used.\n",
    "\n",
    "* **Precision** - Measures how much of the content in the generated text is actually relevant to the reference. High precision means the model is not adding irrelevant or \"extra\" information.\n",
    "\n",
    "* **Recall** - Measures how much of the important content from the reference is captured by the generated text. A high recall means the model covers most of the key points, even if it includes some extra details.\n",
    "\n",
    "* **F1 Score** - Combines both precision and recall into a balanced score. It demonstrates how well the generated text both covers the important content and remains relevant. This is usually reported as the main metric for BERTScore.\n",
    "\n",
    "In short, BERTScore helps evaluate not just word matching, but whether the **meaning** of the generated text aligns with the reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are proceeding with the F1-Score, as it provides a balanced measure of the overall semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERTScore evaluation metric from the Hugging Face 'evaluate' library\n",
    "bert_scorer = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters for `bert_scorer`\n",
    "\n",
    "* **`predictions`** - The summaries generated by our fine-tuned model.\n",
    "* **`references`** - The correct (gold-standard) summaries from the dataset.\n",
    "* **`lang`='en'** - Specifies the language as English.\n",
    "* **`rescale_with_baseline`=True** - Normalizes the scores so they are easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute BERTScore for the model's generated summaries\n",
    "score = bert_scorer.compute(\n",
    "    predictions=predicted_summaries,   # Summaries generated by the model\n",
    "    references=test_summaries,         # Human-written reference summaries\n",
    "    lang='en',                         # Language of the summaries\n",
    "    rescale_with_baseline=True         # Normalize scores for easier interpretation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the **average F1 score** across all evaluated summaries, giving an overall performance measure of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Since this is a generative model, the output may vary slightly each time. Additionally, because the evaluator is built on neural networks, its responses may also change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average F1 score across all generated summaries\n",
    "average_f1 = sum(score['f1']) / len(score['f1'])\n",
    "average_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The BERT Score of Mistral LLM is ~0.21**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Fine-Tuning LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read the CSV into a **Pandas DataFrame** because it is easy to inspect and manipulate tabular data. However, Hugging Face models and trainers do not work directly with DataFrames they expect data in the form of a **`Dataset` object** from the `datasets` library.\n",
    "\n",
    "That's why we convert the DataFrame into a **dictionary of lists**. The `Dataset.from_dict()` method then turns this dictionary into a Hugging Face `Dataset`, which is optimized for:\n",
    "\n",
    "* fast tokenization, shuffling, and batching,\n",
    "* direct compatibility with `Trainer` / `SFTTrainer`,\n",
    "* efficient storage and processing on large datasets.\n",
    "\n",
    "DataFrame stores data like a table (rows × columns), while a Dataset stores data as a dictionary of columns (each column is an array/list), making it better suited for ML pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the fine-tuning training CSV into a Pandas DataFrame\n",
    "training = pd.read_csv(\"../data/finetuning_training.csv\")\n",
    "\n",
    "# Convert the DataFrame into a dictionary of lists (required for Hugging Face Dataset)\n",
    "training_dict = training.to_dict(orient='list')\n",
    "\n",
    "# Create a Hugging Face Dataset from the dictionary\n",
    "training_dataset = Dataset.from_dict(training_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the end-of-sequence token (used to mark the end of each input/output text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the end-of-sequence (EOS) token from the tokenizer\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What `prompt_formatter` does?**\n",
    "\n",
    "* Takes a dataset row (Dialogues, Summary) and a prompt template (`prompt template`).\n",
    "* Adds an instruction: `\"Write a concise summary of the following dialogue.\"`\n",
    "* Fills the template with **instruction + dialogue + summary**.\n",
    "* Appends the **EOS token** to mark the end.\n",
    "* Returns the final prompt as `{'text': formatted_prompt}` for training.\n",
    "\n",
    "This ensures each example is structured like:\n",
    "**Instruction - Dialogue - Summary [EOS]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(example, prompt_template):\n",
    "    # Instruction for the model\n",
    "    instruction = 'Write a concise summary of the following dialogue.'\n",
    "\n",
    "    # Extract dialogue and reference summary from the dataset example\n",
    "    dialogue = example[\"Dialogues\"]\n",
    "    summary = example[\"Summary\"]\n",
    "\n",
    "    # Merge the instruction, dialogue, and summary into the prompt template\n",
    "    # Append EOS_TOKEN to mark the end of the sequence\n",
    "    formatted_prompt = prompt_template.format(instruction, dialogue, summary) + EOS_TOKEN\n",
    "\n",
    "    # Return as a dictionary in the format expected by the trainer\n",
    "    return {'text': formatted_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we are adding the end-of-sequence token to the prompt i.e. we're adding a special marker at the end of the prompt to show it's finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the prompt_formatter function to each example in the training dataset\n",
    "# This formats dialogues and summaries into prompts suitable for model training\n",
    "formatted_training_dataset = training_dataset.map(\n",
    "    prompt_formatter,\n",
    "    fn_kwargs={'prompt_template': alpaca_prompt}  # Pass the Alpaca-style prompt template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the fine-tuning validation CSV into a Pandas DataFrame\n",
    "validation = pd.read_csv(\"../data/finetuning_validation.csv\")\n",
    "\n",
    "# Convert the DataFrame into a dictionary of lists (required for Hugging Face Dataset)\n",
    "validation_dict = validation.to_dict(orient='list')\n",
    "\n",
    "# Create a Hugging Face Dataset from the dictionary\n",
    "validation_dataset = Dataset.from_dict(validation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the prompt_formatter function to each example in the validation dataset\n",
    "# This formats dialogues and summaries into prompts suitable for model evaluation\n",
    "formatted_validation_dataset = validation_dataset.map(\n",
    "    prompt_formatter,\n",
    "    fn_kwargs={'prompt_template': alpaca_prompt}  # Pass the Alpaca-style prompt template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now patch in the adapter modules to the base model using the `get_peft_model` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are adapting the large language model for our task using a technique called **LoRA (Low-Rank Adaptation)**. Instead of retraining the entire model (which would be very expensive), LoRA only updates a small number of parameters while keeping most of the model frozen.\n",
    "\n",
    "* **`r`** - Rank of low-rank matrices; higher = more adaptation, typical 4-64.\n",
    "* **`lora_alpha`** - Scaling factor for LoRA updates; higher = stronger effect, typical 8-32.\n",
    "* **`lora_dropout`** - Dropout on LoRA layers to prevent overfitting, 0-0.3.\n",
    "* **`target_modules`** - The specific parts of the model we allow to be updated.\n",
    "* **`task_type`** - The type of task (CAUSAL_LM for text generation).\n",
    "\n",
    "This step makes the model **lighter, faster, and cheaper to fine-tune**, while still learning how to summarize dialogues effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This is a LoRA model because we are only applying low-rank adapters on top of the frozen model weights. We're using standard PEFT (Parameter-Efficient Fine-Tuning) without quantization on Mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                         # Rank of the LoRA update matrices\n",
    "    lora_alpha=16,                # Scaling factor for LoRA updates\n",
    "    lora_dropout=0.05,            # Dropout rate for LoRA layers (prevents overfitting)\n",
    "    bias=\"none\",                  # How biases are handled (none = leave them unchanged)\n",
    "    target_modules=[              # Model layers where LoRA adapters will be applied\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\"         # Task type for causal language modeling\n",
    ")\n",
    "\n",
    "# Convert the base model into a LoRA fine-tunable model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **architecture** of the Mistral model, specifically the MistralForCausalLM, consists of several key components:\n",
    "\n",
    "1) Embedding Layer: The model starts with an embedding layer that converts input tokens into a dense representation with an output size of 4096, supporting a vocabulary of 32,000 tokens.\n",
    "\n",
    "2) Decoder Layers: The core of the model comprises 32 MistralDecoderLayer instances, each containing:\n",
    "- Self-Attention Mechanism: This includes multiple projection layers for queries, keys, values, and output. Rotary embeddings are also employed for position encoding.\n",
    "- Feedforward Network (MLP): The MLP features gates and projections to expand the dimensionality to 14,336 before reducing it back to 4096, using the SiLU activation function.\n",
    "- Layer Normalization: Each decoder layer includes input and post-attention normalization using MistralRMSNorm.\n",
    "\n",
    "3) Final Normalization: The entire model concludes with an additional normalization layer.\n",
    "\n",
    "4) Linear Output Head: The model includes a linear layer that maps the 4096-dimensional output back to the token vocabulary size (32,000), enabling the generation of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how LoRA adapters are attached to the layers specified during instantiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we use the following nuances borrowed from the broader deep learning discipline.\n",
    "\n",
    "- Low learning rates for smooth parameter updates\n",
    "- Early stopping to monitor for validation loss (negative log likelihood in this case)\n",
    "- Checkpointing to enable resumption of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating a **trainer** that will handle the fine-tuning of our model. The trainer takes care of feeding the data into the model, running the training loop, tracking progress, and saving results.\n",
    "\n",
    "Key points in this setup:\n",
    "\n",
    "* **Model & Tokenizer** - The language model and its tokenizer we are fine-tuning.\n",
    "* **Training & Validation Data** - Split datasets so the model can learn on one set and be tested on another.\n",
    "* **Max Sequence Length (2048)** - How much text the model can read at once.\n",
    "* **Data Collator** - Groups the data into batches in the right format.\n",
    "* **Batch Size & Gradient Accumulation** - Train on small pieces at a time (due to memory limits) and combine updates to act like a larger batch.\n",
    "* **Learning Rate & Optimizer** - Control how fast the model learns and how updates are applied.\n",
    "* **Epochs / Steps** - How long the model trains.\n",
    "* **FP16 / BF16** - Use lower precision for faster and more memory-efficient training.\n",
    "* **Output Directory** - Where trained model checkpoints and logs are saved.\n",
    "\n",
    "This trainer automates the whole training process from sending data into the model to adjusting weights, logging progress, and saving results, making fine-tuning efficient and manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,  # LoRA-adapted model to fine-tune\n",
    "    tokenizer = tokenizer,  # Tokenizer corresponding to the model\n",
    "    train_dataset = formatted_training_dataset,  # Training dataset in prompt-ready format\n",
    "    eval_dataset = formatted_validation_dataset,  # Validation dataset for evaluation\n",
    "    dataset_text_field = \"text\",  # Field in dataset containing the input text\n",
    "    max_seq_length = 2048,  # Maximum sequence length for training\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer),  # Handles batching\n",
    "    dataset_num_proc = 2,  # Number of processes for dataset preprocessing\n",
    "    packing = False,  # Packing short sequences can make training faster (disabled here)\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,  # Batch size per device\n",
    "        gradient_accumulation_steps = 4,  # Accumulate gradients over steps to simulate larger batch\n",
    "        warmup_steps = 5,  # Learning rate warmup steps\n",
    "        max_steps = 30,  # Total training steps (used here for quick demonstration)\n",
    "        learning_rate = 2e-4,  # Learning rate for optimizer\n",
    "        fp16 = not is_bfloat16_supported(),  # Use 16-bit float if bfloat16 not supported\n",
    "        bf16 = is_bfloat16_supported(),  # Use bfloat16 if supported\n",
    "        logging_steps = 1,  # Log metrics every step\n",
    "        optim = \"adamw_torch\",  # Standard AdamW optimizer (Mac-compatible)\n",
    "        weight_decay = 0.01,  # Regularization to prevent overfitting\n",
    "        lr_scheduler_type = \"linear\",  # Linear learning rate decay\n",
    "        seed = 3407,  # For reproducibility\n",
    "        output_dir = \"outputs\",  # Directory to save checkpoints and outputs\n",
    "        report_to = \"none\"  # No external logging (like WandB)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be saving the **LoRA Parameters** of our fine-tuned model so that we can test/evaluate the model later. Since fine-tuning is an expensive process, it's best to save these adapter files in case of crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model_name = \"finetuned_mistral_llm_mac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(lora_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ls -lh {folder}`\n",
    "\n",
    "* **ls** - Lists files and folders.\n",
    "* **-l** - Shows detailed information like permissions, owner, size, and modification date.\n",
    "* **-h** - Makes file sizes human-readable (KB, MB, GB instead of bytes).\n",
    "* `{folder}` - The folder whose contents you want to see.\n",
    "\n",
    "Shows the **contents and sizes** of a folder in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {lora_model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Evaluation of LLM after Fine-Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Fine-tuned Mistral LLM (Mac-Compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model first\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = get_device()\n",
    "\n",
    "# Choose appropriate dtype based on device\n",
    "if device == \"mps\":\n",
    "    dtype = torch.float16\n",
    "elif device == \"cpu\":\n",
    "    dtype = torch.float32\n",
    "else:  # cuda\n",
    "    dtype = torch.float16\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None\n",
    ")\n",
    "\n",
    "if device != \"cuda\":\n",
    "    base_model = base_model.to(device)\n",
    "\n",
    "# Load the fine-tuned LoRA adapters\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_name)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Fine-tuned model loaded successfully on {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Write a concise summary of the following dialogue.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED INFERENCE with batching and device verification\n",
    "# Get the device from the model and verify it's using GPU\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model device: {device}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Disable gradient computation globally for inference speedup\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 4  # Process 4 dialogues at once (adjust based on available memory)\n",
    "MAX_NEW_TOKENS = 128  # Keep at 128 or reduce to 64 if summaries are shorter\n",
    "\n",
    "# Process dialogues in batches\n",
    "for i in tqdm(range(0, len(test_dialogues), BATCH_SIZE), desc=\"Generating summaries\"):\n",
    "    try:\n",
    "        # Get current batch of dialogues\n",
    "        batch_dialogues = test_dialogues[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format all prompts in the batch\n",
    "        batch_prompts = [alpaca_prompt_template.format(dialogue, '') for dialogue in batch_dialogues]\n",
    "        \n",
    "        # Tokenize the entire batch with padding\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,           # Pad to same length\n",
    "            truncation=True,        # Truncate if too long\n",
    "            max_length=2048         # Match training max_seq_length\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate summaries for the batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                use_cache=True,              # Speed up generation\n",
    "                temperature=0,               # Deterministic output\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False              # Greedy decoding for speed\n",
    "            )\n",
    "        \n",
    "        # Decode each output in the batch\n",
    "        for j, output in enumerate(outputs):\n",
    "            # Calculate where the prompt ends for this specific sample\n",
    "            prompt_length = inputs.input_ids[j].shape[-1]\n",
    "            \n",
    "            # Decode only the generated tokens (skip the prompt)\n",
    "            prediction = tokenizer.decode(\n",
    "                output[prompt_length:],\n",
    "                skip_special_tokens=True,\n",
    "                cleanup_tokenization_spaces=True\n",
    "            )\n",
    "            \n",
    "            predicted_summaries.append(prediction)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch starting at index {i}: {e}\")\n",
    "        # Process failed batch one at a time as fallback\n",
    "        for dialogue in batch_dialogues:\n",
    "            try:\n",
    "                prompt = alpaca_prompt_template.format(dialogue, '')\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, temperature=0, pad_token_id=tokenizer.eos_token_id)\n",
    "                prediction = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "                predicted_summaries.append(prediction)\n",
    "            except:\n",
    "                predicted_summaries.append(\"\")  # Empty summary on failure\n",
    "        continue\n",
    "\n",
    "# Re-enable gradients after inference (good practice)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(predicted_summaries)} summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED BATCH INFERENCE - Use this cell instead of the one above\n",
    "# Get the device from the model and verify it's using GPU\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model device: {device}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "# Disable gradient computation globally for inference speedup\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 4  # Process 4 dialogues at once (adjust based on available memory)\n",
    "MAX_NEW_TOKENS = 128  # Keep at 128 or reduce to 64 if summaries are shorter\n",
    "\n",
    "# Process dialogues in batches\n",
    "for i in tqdm(range(0, len(test_dialogues), BATCH_SIZE), desc=\"Generating summaries\"):\n",
    "    try:\n",
    "        # Get current batch of dialogues\n",
    "        batch_dialogues = test_dialogues[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format all prompts in the batch\n",
    "        batch_prompts = [alpaca_prompt_template.format(dialogue, '') for dialogue in batch_dialogues]\n",
    "        \n",
    "        # Tokenize the entire batch with padding\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,           # Pad to same length\n",
    "            truncation=True,        # Truncate if too long\n",
    "            max_length=2048         # Match training max_seq_length\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate summaries for the batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                use_cache=True,              # Speed up generation\n",
    "                temperature=0,               # Deterministic output\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False              # Greedy decoding for speed\n",
    "            )\n",
    "        \n",
    "        # Decode each output in the batch\n",
    "        for j, output in enumerate(outputs):\n",
    "            # Calculate where the prompt ends for this specific sample\n",
    "            prompt_length = inputs.input_ids[j].shape[-1]\n",
    "            \n",
    "            # Decode only the generated tokens (skip the prompt)\n",
    "            prediction = tokenizer.decode(\n",
    "                output[prompt_length:],\n",
    "                skip_special_tokens=True,\n",
    "                cleanup_tokenization_spaces=True\n",
    "            )\n",
    "            \n",
    "            predicted_summaries.append(prediction)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch starting at index {i}: {e}\")\n",
    "        # Process failed batch one at a time as fallback\n",
    "        for dialogue in batch_dialogues:\n",
    "            try:\n",
    "                prompt = alpaca_prompt_template.format(dialogue, '')\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, temperature=0, pad_token_id=tokenizer.eos_token_id)\n",
    "                prediction = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "                predicted_summaries.append(prediction)\n",
    "            except:\n",
    "                predicted_summaries.append(\"\")  # Empty summary on failure\n",
    "        continue\n",
    "\n",
    "# Re-enable gradients after inference (good practice)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(predicted_summaries)} summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the quality of generated summaries using BERTScore\n",
    "score = bert_scorer.compute(\n",
    "    predictions=predicted_summaries,  # Summaries generated by the model\n",
    "    references=test_summaries,        # Ground-truth summaries from the dataset\n",
    "    lang='en',                        # Specify English language\n",
    "    rescale_with_baseline=True        # Normalize scores for easier interpretation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average F1 score across all test examples\n",
    "avg_f1 = sum(score['f1']) / len(score['f1'])\n",
    "avg_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The BERT Score of Fine-tuned Mistral LLM is expected to be ~0.53**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observed a significant improvement in the BERTScore after fine-tuning the Mistral model, also an observation can be made on the Predicted Summaries**\n",
    "\n",
    "- Previously, the generated summaries of client interactions were overly verbose and lacked alignment with user preferences and domain-specific needs.\n",
    "- By fine-tuning a language model on task-relevant and insurance-specific communication data, we significantly improved the model's ability to generate concise, actionable, and context-aware summaries.\n",
    "- The fine-tuned model now produces outputs that are not only more relevant and structured but also tailored to user expectations, enhancing sales productivity and ensuring better client engagement in the insurance domain.\n",
    "\n",
    "**Note:** This Mac-compatible version uses standard transformers and PEFT without quantization, making it suitable for Apple Silicon (M1/M2/M3) devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6 color=\"#4682B4\"><b> Power Ahead </font>\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
