{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmKLmjqmHMQM"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=10>Generative AI for Business Applications</center></font>\n",
        "<center><font size=6>Fine-Tunning LLMs - Week 1</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azxWym9-HMpz"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"\" width=720></a>\n",
        "<center><font size=6>Fine-Tuned AI for Summarizing Insurance Sales Conversations</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa1yXUesL7Zy"
      },
      "source": [
        "# Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFN8LrodMR7w"
      },
      "source": [
        "## Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTLRzlos0ney"
      },
      "source": [
        "An enterprise sales representative at a global insurance provider is preparing for a crucial renewal meeting with one of the largest clients. Over the past year, numerous emails have been exchanged, several calls conducted, and in-person meetings held. However, this valuable context is fragmented across the inbox, CRM records, and call notes.\n",
        "\n",
        "With limited time and growing pressure to personalize service and identify cross-sell opportunities, it is difficult to recall key details, such as the products the client was interested in, concerns raised in the last quarter, and commitments made during previous meetings.\n",
        "\n",
        "This challenge reflects a broader industry problem where client interactions are rich but scattered. Sales teams often face:\n",
        "\n",
        "* **Overload of unstructured data** from emails, calls, and notes.\n",
        "* **Lack of standardized, accurate summaries** to capture client context.\n",
        "* **Manual, error-prone preparation** that consumes significant time.\n",
        "* **Missed upsell and personalization opportunities**, weakening client trust.\n",
        "\n",
        "As a result, client engagement is inconsistent, preparation is inefficient, and revenue opportunities are lost.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDG_248npouB"
      },
      "source": [
        "##  Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gppGd2sN0s8A"
      },
      "source": [
        "The objective is to introduce a **smart assistant** capable of synthesizing multi-modal client interactions and generating precise, context-aware summaries.\n",
        "\n",
        "Such a solution would:\n",
        "\n",
        "* Consolidate insights from emails, CRM logs, call transcripts, and meeting notes.\n",
        "* Deliver concise, tailored client briefs before every touchpoint.\n",
        "* Help sales teams maintain continuity, honor past commitments, and personalize conversations.\n",
        "* Unlock new revenue by surfacing upsell and cross-sell opportunities at the right moment.\n",
        "\n",
        "By reducing preparation time and improving personalization, this assistant can transform client engagement in the insurance sector, strengthen relationships, and drive sustainable growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGxmR0XVwcAi"
      },
      "source": [
        "## Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpQ6dfJa1Iwg"
      },
      "source": [
        "The dataset consists of two primary columns:\n",
        "\n",
        "Conversation - Contains the raw transcripts of client-sales representative interactions, which are often lengthy, multi-turn, and unstructured.\n",
        "\n",
        "Summary - Provides the corresponding concise, structured summaries of key discussion points, client interests, concerns, and commitments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGUDO5qRb4oB"
      },
      "source": [
        "# **Solution Approach**\n",
        "Provide a Custom Fine-Tuned AI Model for Sales Interaction Summarization\n",
        "\n",
        "To address this challenge, we propose training a domain-specific fine-tuned language model tailored for enterprise insurance communication.\n",
        "The model will:\n",
        "\n",
        "1. Ingest few multi-modal inputs (emails, transcripts, notes).\n",
        "2. Identify intent, extract key discussion points, client interests, pain points, and commitments.\n",
        "3. Generate concise, actionable summaries under 200 words, customized for enterprise insurance workflows.\n",
        "4. Be fine-tuned on real-world communication data to learn domain-specific vocabulary and interaction patterns.\n",
        "\n",
        "This AI-powered tool will augment sales productivity, enhance client engagement, and ensure consistent follow-ups, turning scattered conversations into strategic intelligence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnwETBOE6Bz5"
      },
      "source": [
        "# **Installing and Importing Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfffBpmv6ciV",
        "outputId": "d904bb1a-f329-4f31-fe27-87bd3e796772"
      },
      "outputs": [],
      "source": [
        "# Install required packages for Mac compatibility\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers datasets evaluate peft accelerate\n",
        "!pip install sentencepiece protobuf huggingface_hub\n",
        "!pip install trl bitsandbytes\n",
        "!pip install bert-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDp-EYZH-69E"
      },
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- The above installation is optimized for Mac compatibility and removes Unsloth dependencies that cause hanging issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgtsNS-UrQg9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer  # Standard Hugging Face model and tokenizer loaders\n",
        "import torch                                   # Core deep learning library for tensor operations and model training.\n",
        "import evaluate                                # Hugging Face library for evaluating NLP models with standard metrics.\n",
        "from tqdm import tqdm                          # Progress bar utility for tracking loops and training progress.\n",
        "import pandas as pd                            # Data manipulation and analysis library (tabular data handling).\n",
        "from datasets import Dataset                   # Hugging Face library for creating and managing ML datasets.\n",
        "\n",
        "from trl import SFTTrainer                     # Trainer class for supervised fine-tuning of language models.\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback, DataCollatorForSeq2Seq\n",
        "# - TrainingArguments: configuration for training (batch size, learning rate, etc.)\n",
        "# - EarlyStoppingCallback: stops training when validation performance stops improving.\n",
        "# - DataCollatorForSeq2Seq: prepares batches of data for sequence-to-sequence models.\n",
        "\n",
        "from peft import LoraConfig, get_peft_model    # LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
        "\n",
        "# Function to check if bfloat16 is supported on current hardware\n",
        "def is_bfloat16_supported():\n",
        "\treturn torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhyMoqjoq48A"
      },
      "source": [
        "# **1. Evaluation of LLM before FineTuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp4LNGG30e8w"
      },
      "source": [
        "### Loading the Testing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dat_l5MlrLkX"
      },
      "outputs": [],
      "source": [
        "# Read the testing CSV into a Pandas DataFrame\n",
        "testing_data = pd.read_csv(\"../data/finetuning_testing.csv\")\n",
        "\n",
        "# Extract all dialogues into a list for model input\n",
        "test_dialogues = [sample for sample in testing_data['Dialogues']]\n",
        "\n",
        "# Extract all human-written summaries into a list for evaluation\n",
        "test_summaries = [sample for sample in testing_data['Summary']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHaWozqI0hmW"
      },
      "source": [
        "### Loading the Mistral Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZWyFbFXsn_D",
        "outputId": "aad2ff42-4bcc-4ad4-fd68-250a44388b8e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Configure 4-bit quantization for memory efficiency (only if CUDA is available)\n",
        "quantization_config = None\n",
        "if torch.cuda.is_available():\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "# Load the pre-trained Mistral model with Mac compatibility\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3VYp0p7C794",
        "outputId": "d0026067-05ac-48f5-e6db-002f298a1dc4"
      },
      "outputs": [],
      "source": [
        "# Prepare the model for inference (generating predictions) - Mac compatible\n",
        "model.eval()  # Set model to evaluation mode for inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9sW9YYntYcs"
      },
      "source": [
        "### Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U6zN9zd5YAs"
      },
      "source": [
        "The Alpaca instruction prompt is a general purpose prompt template that can be adapted to any task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHaAlmvZfTEp"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Write a concise summary of the following dialogue.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qp9Dtqvdtd6C"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store the summaries generated by the model\n",
        "predicted_summaries = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEUx94TogsMJ"
      },
      "source": [
        "We are generating summaries for each dialogue in our test set using the fine-tuned model.\n",
        "\n",
        "**Step-by-step Approach:**\n",
        "\n",
        "1. **Iterate through test dialogues** - `for dialogue in tqdm(test_dialogues):`\n",
        "\n",
        "   * Loops through each test dialogue while showing a progress bar (`tqdm`).\n",
        "\n",
        "2. **Format the prompt**\n",
        "\n",
        "   * Inserts the dialogue into the summarization template.\n",
        "\n",
        "3. **Tokenize input**\n",
        "\n",
        "   * Converts the text prompt into tokens (numbers) and moves them to the appropriate device.\n",
        "\n",
        "4. **Generate output**\n",
        "\n",
        "   * The model predicts the summary using `.generate()`.\n",
        "   * `max_new_tokens=128`: limits summary length.\n",
        "   * `temperature=0`: makes output deterministic (no randomness).\n",
        "   * `pad_token_id`: ensures proper padding using EOS token.\n",
        "\n",
        "5. **Decode output**\n",
        "\n",
        "   * Converts model tokens back into human-readable text.\n",
        "   * Skips special tokens and cleans formatting.\n",
        "\n",
        "6. **Store prediction**\n",
        "\n",
        "   * Appends the generated summary to `predicted_summaries`.\n",
        "\n",
        "7. **Error handling**\n",
        "\n",
        "   * If an error occurs, it prints the error and continues with the next dialogue instead of stopping.\n",
        "\n",
        "This loop **takes each dialogue -> feeds it to the model -> generates a summary -> saves it for evaluation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M96nqedgsn6Q",
        "outputId": "ad87c78c-8163-4588-d879-222a8e992a5b"
      },
      "outputs": [],
      "source": [
        "# Loop through each dialogue in the test set and generate summaries\n",
        "device = next(model.parameters()).device  # Get the device the model is on\n",
        "\n",
        "for dialogue in tqdm(test_dialogues):\n",
        "    try:\n",
        "        # Format the dialogue into the Alpaca-style prompt template\n",
        "        prompt = alpaca_prompt_template.format(dialogue, \" \")\n",
        "\n",
        "        # Tokenize the prompt and move it to the same device as the model\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate summary with the model\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,          # Limit summary length\n",
        "            use_cache=True,              # Speed up generation\n",
        "            temperature=0,               # Deterministic output\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode the generated tokens into human-readable text\n",
        "        prediction = tokenizer.decode(\n",
        "            outputs[0][inputs.input_ids.shape[-1]:],  # Skip prompt tokens\n",
        "            skip_special_tokens=True,\n",
        "            cleanup_tokenization_spaces=True\n",
        "        )\n",
        "\n",
        "        # Store the generated summary\n",
        "        predicted_summaries.append(prediction)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)  # Log any errors and continue with next dialogue\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC8DIfb80ULd"
      },
      "source": [
        "### Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEb0o6jPc1C5"
      },
      "source": [
        "Now we are evaluating our base model to check how well the generated summaries align with human-written summaries. For this, we are using BERTScore, which measures the semantic similarity between the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLHAna-S78Li"
      },
      "source": [
        "**BERTScore** is a metric for evaluating text generation tasks, including summarization, translation, and captioning. Unlike traditional metrics like ROUGE or BLEU that rely on exact word overlaps, BERTScore uses embeddings from a pre-trained BERT model to measure **semantic similarity** between the generated text (predictions) and the human-written text (references). This makes it more robust in capturing meaning, even when different words are used.\n",
        "\n",
        "* **Precision** - Measures how much of the content in the generated text is actually relevant to the reference. High precision means the model is not adding irrelevant or "extra" information.\n",
        "\n",
        "* **Recall** - Measures how much of the important content from the reference is captured by the generated text. A high recall means the model covers most of the key points, even if it includes some extra details.\n",
        "\n",
        "* **F1 Score** - Combines both precision and recall into a balanced score. It demonstrates how well the generated text both covers the important content and remains relevant. This is usually reported as the main metric for BERTScore.\n",
        "\n",
        "In short, BERTScore helps evaluate not just word matching, but whether the **meaning** of the generated text aligns with the reference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GabjYgxsdw-i"
      },
      "source": [
        "We are proceeding with the F1-Score, as it provides a balanced measure of the overall semantic similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_RZs84SuIZY"
      },
      "outputs": [],
      "source": [
        "# Load the BERTScore evaluation metric from the Hugging Face 'evaluate' library\n",
        "bert_scorer = evaluate.load(\"bertscore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDt6VUebIzH2"
      },
      "source": [
        "Hyperparameters for `bert_scorer`\n",
        "\n",
        "* **`predictions`** - The summaries generated by our fine-tuned model.\n",
        "* **`references`** - The correct (gold-standard) summaries from the dataset.\n",
        "* **`lang`='en'** - Specifies the language as English.\n",
        "* **`rescale_with_baseline`=True** - Normalizes the scores so they are easier to interpret.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "f483532192744e2c9a4eefb2567b02b6",
            "82ed1f8f01ba41d58e60732f502e2606",
            "703d0fe63c26440b9f251f760c9ef4ab",
            "f0af8c1d89704bd28d739a274869ad65",
            "03aedb3387ef4182a7eaab849fc21123",
            "ab6b2c906ff94da5a94201f409434574",
            "6424175902e045d1b30d3fa12ff73f32",
            "6fb638164b754e538e38492a9aa08204",
            "fc394f734d584b7d9315560b21640e5d",
            "5566162bc00d430b878336c23d24bdbf",
            "a547d8c63bb44f88a6522c7186b5e268",
            "553208bce9ea4ef69921389d09f807c5",
            "8afa630e5bc342e59807f0ea8c493f95",
            "80da8e51ccf64550ba5be2f58b28ac98",
            "c6a40368c70c427da29a8f56181ed5fa",
            "862b87382adf4c39b93e8f31e0ee6813",
            "a9925280086c420a89726da2b0a3b6d4",
            "e45a8ce6c93e4c069050076cfee78f09",
            "f329740b7b6441519c5dea729dd7d1ab",
            "22b6ba504e194e1183c963417f128677",
            "fe29dde9a1674024bd7a7fb46f1359f8",
            "fe5f10d9bd4d4db18c91be1612d7c4a8"
          ]
        },
        "id": "Ery2CVncuIW1",
        "outputId": "2bdb6709-f20f-4ff5-9bc5-8b7f7b570b9c"
      },
      "outputs": [],
      "source": [
        "# Compute BERTScore for the model's generated summaries\n",
        "score = bert_scorer.compute(\n",
        "    predictions=predicted_summaries,   # Summaries generated by the model\n",
        "    references=test_summaries,         # Human-written reference summaries\n",
        "    lang='en',                         # Language of the summaries\n",
        "    rescale_with_baseline=True         # Normalize scores for easier interpretation\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-uqPBrseLv6"
      },
      "source": [
        "Now we calculate the **average F1 score** across all evaluated summaries, giving an overall performance measure of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoY9XI8YY0pE"
      },
      "source": [
        "**Note:** Since this is a generative model, the output may vary slightly each time. Additionally, because the evaluator is built on neural networks, its responses may also change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DESywvMrug7X",
        "outputId": "c604e1c5-d706-44fd-d634-645a95a9c42c"
      },
      "outputs": [],
      "source": [
        "# Calculate the average F1 score across all generated summaries\n",
        "average_f1 = sum(score['f1']) / len(score['f1'])\n",
        "average_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvZ1hPLCWwYt"
      },
      "source": [
        "**The BERT Score of Mistral LLM is 0.21**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFBNy7vSq42O"
      },
      "source": [
        "# **2. Fine Tuning LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km_LXL5DaGBO"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18s8RG3i3ay5"
      },
      "source": [
        "We first read the CSV into a **Pandas DataFrame** because it is easy to inspect and manipulate tabular data. However, Hugging Face models and trainers do not work directly with DataFrames they expect data in the form of a **`Dataset` object** from the `datasets` library.\n",
        "\n",
        "That's why we convert the DataFrame into a **dictionary of lists**. The `Dataset.from_dict()` method then turns this dictionary into a Hugging Face `Dataset`, which is optimized for:\n",
        "\n",
        "* fast tokenization, shuffling, and batching,\n",
        "* direct compatibility with `Trainer` / `SFTTrainer`,\n",
        "* efficient storage and processing on large datasets.\n",
        "\n",
        "DataFrame stores data like a table (rows Ã— columns), while a Dataset stores data as a dictionary of columns (each column is an array/list), making it better suited for ML pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6GqVK_ShZ2m"
      },
      "source": [
        "#### Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjytqCN7rMBu"
      },
      "outputs": [],
      "source": [
        "# Read the fine-tuning training CSV into a Pandas DataFrame\n",
        "training = pd.read_csv(\"../data/finetuning_training.csv\")\n",
        "\n",
        "# Convert the DataFrame into a dictionary of lists (required for Hugging Face Dataset)\n",
        "training_dict = training.to_dict(orient='list')\n",
        "\n",
        "# Create a Hugging Face Dataset from the dictionary\n",
        "training_dataset = Dataset.from_dict(training_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbNMLPPg3rk8"
      },
      "source": [
        "Store the end-of-sequence token (used to mark the end of each input/output text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TURs0KUrL9v"
      },
      "outputs": [],
      "source": [
        "# Get the end-of-sequence (EOS) token from the tokenizer\n",
        "EOS_TOKEN = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6rSZF3Qhdb2"
      },
      "source": [
        "#### Create a prompt
