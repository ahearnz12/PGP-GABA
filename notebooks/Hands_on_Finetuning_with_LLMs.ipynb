{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmKLmjqmHMQM"
   },
   "source": [
    "<center><p float=\"center\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
    "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
    "</p></center>\n",
    "\n",
    "<center><font size=10>Generative AI for Business Applications</center></font>\n",
    "<center><font size=6>Fine-Tunning LLMs - Week 1</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azxWym9-HMpz"
   },
   "source": [
    "<center><p float=\"center\">\n",
    "  <img src=\"\" width=720></a>\n",
    "<center><font size=6>Fine-Tuned AI for Summarizing Insurance Sales Conversations</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aa1yXUesL7Zy"
   },
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFN8LrodMR7w"
   },
   "source": [
    "## Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTLRzlos0ney"
   },
   "source": [
    "An enterprise sales representative at a global insurance provider is preparing for a crucial renewal meeting with one of the largest clients. Over the past year, numerous emails have been exchanged, several calls conducted, and in-person meetings held. However, this valuable context is fragmented across the inbox, CRM records, and call notes.\n",
    "\n",
    "With limited time and growing pressure to personalize service and identify cross-sell opportunities, it is difficult to recall key details, such as the products the client was interested in, concerns raised in the last quarter, and commitments made during previous meetings.\n",
    "\n",
    "This challenge reflects a broader industry problem where client interactions are rich but scattered. Sales teams often face:\n",
    "\n",
    "* **Overload of unstructured data** from emails, calls, and notes.\n",
    "* **Lack of standardized, accurate summaries** to capture client context.\n",
    "* **Manual, error-prone preparation** that consumes significant time.\n",
    "* **Missed upsell and personalization opportunities**, weakening client trust.\n",
    "\n",
    "As a result, client engagement is inconsistent, preparation is inefficient, and revenue opportunities are lost.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDG_248npouB"
   },
   "source": [
    "##  Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gppGd2sN0s8A"
   },
   "source": [
    "The objective is to introduce a **smart assistant** capable of synthesizing multi-modal client interactions and generating precise, context-aware summaries.\n",
    "\n",
    "Such a solution would:\n",
    "\n",
    "* Consolidate insights from emails, CRM logs, call transcripts, and meeting notes.\n",
    "* Deliver concise, tailored client briefs before every touchpoint.\n",
    "* Help sales teams maintain continuity, honor past commitments, and personalize conversations.\n",
    "* Unlock new revenue by surfacing upsell and cross-sell opportunities at the right moment.\n",
    "\n",
    "By reducing preparation time and improving personalization, this assistant can transform client engagement in the insurance sector, strengthen relationships, and drive sustainable growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGxmR0XVwcAi"
   },
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpQ6dfJa1Iwg"
   },
   "source": [
    "The dataset consists of two primary columns:\n",
    "\n",
    "Conversation - Contains the raw transcripts of client-sales representative interactions, which are often lengthy, multi-turn, and unstructured.\n",
    "\n",
    "Summary - Provides the corresponding concise, structured summaries of key discussion points, client interests, concerns, and commitments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGUDO5qRb4oB"
   },
   "source": [
    "# **Solution Approach**\n",
    "Provide a Custom Fine-Tuned AI Model for Sales Interaction Summarization\n",
    "\n",
    "To address this challenge, we propose training a domain-specific fine-tuned language model tailored for enterprise insurance communication.\n",
    "The model will:\n",
    "\n",
    "1. Ingest few multi-modal inputs (emails, transcripts, notes).\n",
    "2. Identify intent, extract key discussion points, client interests, pain points, and commitments.\n",
    "3. Generate concise, actionable summaries under 200 words, customized for enterprise insurance workflows.\n",
    "4. Be fine-tuned on real-world communication data to learn domain-specific vocabulary and interaction patterns.\n",
    "\n",
    "This AI-powered tool will augment sales productivity, enhance client engagement, and ensure consistent follow-ups, turning scattered conversations into strategic intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnwETBOE6Bz5"
   },
   "source": [
    "# **Installing and Importing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfffBpmv6ciV",
    "outputId": "d904bb1a-f329-4f31-fe27-87bd3e796772"
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.32.post2 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf huggingface_hub hf_transfer\n",
    "!pip install transformers==4.51.3\n",
    "!pip install --no-deps unsloth\n",
    "\n",
    "!pip install -q datasets evaluate bert-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDp-EYZH-69E"
   },
   "source": [
    "**Note**:\n",
    "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
    "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgtsNS-UrQg9"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # Standard Hugging Face model and tokenizer loaders\n",
    "import torch                                   # Core deep learning library for tensor operations and model training.\n",
    "import evaluate                                # Hugging Face library for evaluating NLP models with standard metrics.\n",
    "from tqdm import tqdm                          # Progress bar utility for tracking loops and training progress.\n",
    "import pandas as pd                            # Data manipulation and analysis library (tabular data handling).\n",
    "from datasets import Dataset                   # Hugging Face library for creating and managing ML datasets.\n",
    "\n",
    "from trl import SFTTrainer                     # Trainer class for supervised fine-tuning of language models.\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback, DataCollatorForSeq2Seq\n",
    "# - TrainingArguments: configuration for training (batch size, learning rate, etc.)\n",
    "# - EarlyStoppingCallback: stops training when validation performance stops improving.\n",
    "# - DataCollatorForSeq2Seq: prepares batches of data for sequence-to-sequence models.\n",
    "\n",
    "from peft import LoraConfig, get_peft_model    # LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "\n",
    "# Import FastLanguageModel for Mac compatibility\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    # Check if we're on a supported platform\n",
    "    device = get_device()\n",
    "    if device == \"mps\":\n",
    "        print(\"Unsloth not supported on Apple Silicon, using standard transformers\")\n",
    "        FastLanguageModel = None\n",
    "except (ImportError, NotImplementedError):\n",
    "    print(\"Unsloth not available, using standard transformers\")\n",
    "    FastLanguageModel = None\n",
    "\n",
    "# Function to check device compatibility for Mac\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"  # Apple Silicon GPU\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda\"  # NVIDIA GPU\n",
    "    else:\n",
    "        return \"cpu\"   # CPU fallback\n",
    "\n",
    "# Function to check if bfloat16 is supported on current hardware\n",
    "def is_bfloat16_supported():\n",
    "\tdevice = get_device()\n",
    "\tif device == \"cuda\":\n",
    "\t\treturn torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "\telif device == \"mps\":\n",
    "\t\treturn True  # MPS supports bfloat16\n",
    "\telse:\n",
    "\t\treturn False  # CPU doesn't support bfloat16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhyMoqjoq48A"
   },
   "source": [
    "# **1. Evaluation of LLM before FineTuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp4LNGG30e8w"
   },
   "source": [
    "### Loading the Testing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dat_l5MlrLkX"
   },
   "outputs": [],
   "source": [
    "# Read the testing CSV into a Pandas DataFrame\n",
    "testing_data = pd.read_csv(\"../data/finetuning_testing.csv\")\n",
    "\n",
    "# Extract all dialogues into a list for model input\n",
    "test_dialogues = [sample for sample in testing_data['Dialogues']]\n",
    "\n",
    "# Extract all human-written summaries into a list for evaluation\n",
    "test_summaries = [sample for sample in testing_data['Summary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHaWozqI0hmW"
   },
   "source": [
    "### Loading the Mistral Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZWyFbFXsn_D",
    "outputId": "aad2ff42-4bcc-4ad4-fd68-250a44388b8e"
   },
   "outputs": [],
   "source": [
    "# Check if model is already loaded\n",
    "if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "    print(\"Loading Mistral model for the first time...\")\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "    # Get the appropriate device for this system\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Mac-compatible model loading\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        # CUDA-specific configuration with quantization\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    else:\n",
    "        # Mac/CPU-compatible configuration without quantization\n",
    "        print(\"Loading model (this may take a few minutes)...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if device == \"mps\" else torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        # Move model to appropriate device\n",
    "        model = model.to(device)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"✓ Model loaded successfully on {device}!\")\n",
    "else:\n",
    "    print(\"✓ Model already loaded, skipping...\")\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"Using existing model on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3VYp0p7C794",
    "outputId": "d0026067-05ac-48f5-e6db-002f298a1dc4"
   },
   "outputs": [],
   "source": [
    "# Prepare the model for inference (generating predictions)\n",
    "if FastLanguageModel is not None:\n",
    "    FastLanguageModel.for_inference(model)\n",
    "else:\n",
    "    # Fallback for Mac without unsloth\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9sW9YYntYcs"
   },
   "source": [
    "### Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U6zN9zd5YAs"
   },
   "source": [
    "The Alpaca instruction prompt is a general purpose prompt template that can be adapted to any task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHaAlmvZfTEp"
   },
   "outputs": [],
   "source": [
    "alpaca_prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Write a concise summary of the following dialogue.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp9Dtqvdtd6C"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the summaries generated by the model\n",
    "predicted_summaries = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEUx94TogsMJ"
   },
   "source": [
    "We are generating summaries for each dialogue in our test set using the fine-tuned model.\n",
    "\n",
    "**Step-by-step Approach:**\n",
    "\n",
    "1. **Iterate through test dialogues** - `for dialogue in tqdm(test_dialogues):`\n",
    "\n",
    "   * Loops through each test dialogue while showing a progress bar (`tqdm`).\n",
    "\n",
    "2. **Format the prompt**\n",
    "\n",
    "   * Inserts the dialogue into the summarization template.\n",
    "\n",
    "3. **Tokenize input**\n",
    "\n",
    "   * Converts the text prompt into tokens (numbers) and moves them to the GPU (`.to(\"cuda\")`).\n",
    "\n",
    "4. **Generate output**\n",
    "\n",
    "   * The model predicts the summary using `.generate()`.\n",
    "   * `max_new_tokens=128`: limits summary length.\n",
    "   * `temperature=0`: makes output deterministic (no randomness).\n",
    "   * `pad_token_id`: ensures proper padding using EOS token.\n",
    "\n",
    "5. **Decode output**\n",
    "\n",
    "   * Converts model tokens back into human-readable text.\n",
    "   * Skips special tokens and cleans formatting.\n",
    "\n",
    "6. **Store prediction**\n",
    "\n",
    "   * Appends the generated summary to `predicted_summaries`.\n",
    "\n",
    "7. **Error handling**\n",
    "\n",
    "   * If an error occurs, it prints the error and continues with the next dialogue instead of stopping.\n",
    "\n",
    "This loop **takes each dialogue -> feeds it to the model -> generates a summary -> saves it for evaluation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M96nqedgsn6Q",
    "outputId": "ad87c78c-8163-4588-d879-222a8e992a5b"
   },
   "outputs": [],
   "source": [
    "# Get the device from the model\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# Loop through each dialogue in the test set and generate summaries\n",
    "for dialogue in tqdm(test_dialogues):\n",
    "    try:\n",
    "        # Format the dialogue into the Alpaca-style prompt template\n",
    "        prompt = alpaca_prompt_template.format(dialogue, \" \")\n",
    "\n",
    "        # Tokenize the prompt and move to the correct device (MPS, CUDA, or CPU)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate summary with the model\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,          # Limit summary length\n",
    "                use_cache=True,              # Speed up generation\n",
    "                temperature=0,               # Deterministic output\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into human-readable text\n",
    "        prediction = tokenizer.decode(\n",
    "            outputs[0][inputs.input_ids.shape[-1]:],  # Skip prompt tokens\n",
    "            skip_special_tokens=True,\n",
    "            cleanup_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "        # Store the generated summary\n",
    "        predicted_summaries.append(prediction)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)  # Log any errors and continue with next dialogue\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC8DIfb80ULd"
   },
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEb0o6jPc1C5"
   },
   "source": [
    "Now we are evaluating our base model to check how well the generated summaries align with human-written summaries. For this, we are using BERTScore, which measures the semantic similarity between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLHAna-S78Li"
   },
   "source": [
    "**BERTScore** is a metric for evaluating text generation tasks, including summarization, translation, and captioning. Unlike traditional metrics like ROUGE or BLEU that rely on exact word overlaps, BERTScore uses embeddings from a pre-trained BERT model to measure **semantic similarity** between the generated text (predictions) and the human-written text (references). This makes it more robust in capturing meaning, even when different words are used.\n",
    "\n",
    "* **Precision** - Measures how much of the content in the generated text is actually relevant to the reference. High precision means the model is not adding irrelevant or “extra” information.\n",
    "\n",
    "* **Recall** - Measures how much of the important content from the reference is captured by the generated text. A high recall means the model covers most of the key points, even if it includes some extra details.\n",
    "\n",
    "* **F1 Score** - Combines both precision and recall into a balanced score. It demonstrates how well the generated text both covers the important content and remains relevant. This is usually reported as the main metric for BERTScore.\n",
    "\n",
    "In short, BERTScore helps evaluate not just word matching, but whether the **meaning** of the generated text aligns with the reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GabjYgxsdw-i"
   },
   "source": [
    "We are proceeding with the F1-Score, as it provides a balanced measure of the overall semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_RZs84SuIZY"
   },
   "outputs": [],
   "source": [
    "# Load the BERTScore evaluation metric from the Hugging Face 'evaluate' library\n",
    "bert_scorer = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDt6VUebIzH2"
   },
   "source": [
    "Hyperparameters for `bert_scorer`\n",
    "\n",
    "* **`predictions`** - The summaries generated by our fine-tuned model.\n",
    "* **`references`** - The correct (gold-standard) summaries from the dataset.\n",
    "* **`lang`='en'** - Specifies the language as English.\n",
    "* **`rescale_with_baseline`=True** - Normalizes the scores so they are easier to interpret.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "f483532192744e2c9a4eefb2567b02b6",
      "82ed1f8f01ba41d58e60732f502e2606",
      "703d0fe63c26440b9f251f760c9ef4ab",
      "f0af8c1d89704bd28d739a274869ad65",
      "03aedb3387ef4182a7eaab849fc21123",
      "ab6b2c906ff94da5a94201f409434574",
      "6424175902e045d1b30d3fa12ff73f32",
      "6fb638164b754e538e38492a9aa08204",
      "fc394f734d584b7d9315560b21640e5d",
      "5566162bc00d430b878336c23d24bdbf",
      "a547d8c63bb44f88a6522c7186b5e268",
      "553208bce9ea4ef69921389d09f807c5",
      "8afa630e5bc342e59807f0ea8c493f95",
      "80da8e51ccf64550ba5be2f58b28ac98",
      "c6a40368c70c427da29a8f56181ed5fa",
      "862b87382adf4c39b93e8f31e0ee6813",
      "a9925280086c420a89726da2b0a3b6d4",
      "e45a8ce6c93e4c069050076cfee78f09",
      "f329740b7b6441519c5dea729dd7d1ab",
      "22b6ba504e194e1183c963417f128677",
      "fe29dde9a1674024bd7a7fb46f1359f8",
      "fe5f10d9bd4d4db18c91be1612d7c4a8"
     ]
    },
    "id": "Ery2CVncuIW1",
    "outputId": "2bdb6709-f20f-4ff5-9bc5-8b7f7b570b9c"
   },
   "outputs": [],
   "source": [
    "# Compute BERTScore for the model's generated summaries\n",
    "score = bert_scorer.compute(\n",
    "    predictions=predicted_summaries,   # Summaries generated by the model\n",
    "    references=test_summaries,         # Human-written reference summaries\n",
    "    lang='en',                         # Language of the summaries\n",
    "    rescale_with_baseline=True         # Normalize scores for easier interpretation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-uqPBrseLv6"
   },
   "source": [
    "Now we calculate the **average F1 score** across all evaluated summaries, giving an overall performance measure of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoY9XI8YY0pE"
   },
   "source": [
    "**Note:** Since this is a generative model, the output may vary slightly each time. Additionally, because the evaluator is built on neural networks, its responses may also change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DESywvMrug7X",
    "outputId": "c604e1c5-d706-44fd-d634-645a95a9c42c"
   },
   "outputs": [],
   "source": [
    "# Calculate the average F1 score across all generated summaries\n",
    "average_f1 = sum(score['f1']) / len(score['f1'])\n",
    "average_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvZ1hPLCWwYt"
   },
   "source": [
    "**The BERT Score of Mistral LLM is 0.21**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFBNy7vSq42O"
   },
   "source": [
    "# **2. Fine Tuning LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Km_LXL5DaGBO"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18s8RG3i3ay5"
   },
   "source": [
    "We first read the CSV into a **Pandas DataFrame** because it is easy to inspect and manipulate tabular data. However, Hugging Face models and trainers do not work directly with DataFrames they expect data in the form of a **`Dataset` object** from the `datasets` library.\n",
    "\n",
    "That’s why we convert the DataFrame into a **dictionary of lists**. The `Dataset.from_dict()` method then turns this dictionary into a Hugging Face `Dataset`, which is optimized for:\n",
    "\n",
    "* fast tokenization, shuffling, and batching,\n",
    "* direct compatibility with `Trainer` / `SFTTrainer`,\n",
    "* efficient storage and processing on large datasets.\n",
    "\n",
    "DataFrame stores data like a table (rows × columns), while a Dataset stores data as a dictionary of columns (each column is an array/list), making it better suited for ML pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6GqVK_ShZ2m"
   },
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjytqCN7rMBu"
   },
   "outputs": [],
   "source": [
    "# Read the fine-tuning training CSV into a Pandas DataFrame\n",
    "training = pd.read_csv(\"/content/finetuning_training.csv\")\n",
    "\n",
    "# Convert the DataFrame into a dictionary of lists (required for Hugging Face Dataset)\n",
    "training_dict = training.to_dict(orient='list')\n",
    "\n",
    "# Create a Hugging Face Dataset from the dictionary\n",
    "training_dataset = Dataset.from_dict(training_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbNMLPPg3rk8"
   },
   "source": [
    "Store the end-of-sequence token (used to mark the end of each input/output text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TURs0KUrL9v"
   },
   "outputs": [],
   "source": [
    "# Get the end-of-sequence (EOS) token from the tokenizer\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6rSZF3Qhdb2"
   },
   "source": [
    "#### Create a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOAzduah_e_z"
   },
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIqdC93Rhi0g"
   },
   "source": [
    "### Prompt Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "513Rtfvx3-6d"
   },
   "source": [
    "**What `prompt_formatter` does?**\n",
    "\n",
    "* Takes a dataset row (Dialogues, Summary) and a prompt template (`prompt template`).\n",
    "* Adds an instruction: `\"Write a concise summary of the following dialogue.\"`\n",
    "* Fills the template with **instruction + dialogue + summary**.\n",
    "* Appends the **EOS token** to mark the end.\n",
    "* Returns the final prompt as `{'text': formatted_prompt}` for training.\n",
    "\n",
    "This ensures each example is structured like:\n",
    "**Instruction - Dialogue - Summary \\[EOS]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0sjC8_Y_e9j"
   },
   "outputs": [],
   "source": [
    "def prompt_formatter(example, prompt_template):\n",
    "    # Instruction for the model\n",
    "    instruction = 'Write a concise summary of the following dialogue.'\n",
    "\n",
    "    # Extract dialogue and reference summary from the dataset example\n",
    "    dialogue = example[\"Dialogues\"]\n",
    "    summary = example[\"Summary\"]\n",
    "\n",
    "    # Merge the instruction, dialogue, and summary into the prompt template\n",
    "    # Append EOS_TOKEN to mark the end of the sequence\n",
    "    formatted_prompt = prompt_template.format(instruction, dialogue, summary) + EOS_TOKEN\n",
    "\n",
    "    # Return as a dictionary in the format expected by the trainer\n",
    "    return {'text': formatted_prompt}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZFaxW79CctT"
   },
   "source": [
    "Notice how we are adding the end-of-sequence token to the prompt i.e. we're adding a special marker at the end of the prompt to show it's finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e2021eea82144517b2a89d67402f6296",
      "73726c48f256499daf6a7e5ff7c8d05d",
      "8595e39cc76842029c15847345922cdc",
      "70af7c7881c445d79d4ac28cf196c379",
      "bf846beed3634db78681d05271da7c01",
      "df5637d8d8e64c418e906500c4a925e6",
      "f2ead2bea789443692b70420e747fab4",
      "182044c6a2574c4cbf97810a098484c9",
      "e196b6f6012c4591bec9956bd1b5607b",
      "a733c57b43c04e749f6cb10ab3c5c647",
      "e452aa326751490485ba8437dadcc361"
     ]
    },
    "id": "V74wlPa7_e71",
    "outputId": "1b6c68bb-aac7-498d-c21d-876604368756"
   },
   "outputs": [],
   "source": [
    "# Apply the prompt_formatter function to each example in the training dataset\n",
    "# This formats dialogues and summaries into prompts suitable for model training\n",
    "formatted_training_dataset = training_dataset.map(\n",
    "    prompt_formatter,\n",
    "    fn_kwargs={'prompt_template': alpaca_prompt}  # Pass the Alpaca-style prompt template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvnCfQ9I_e5F"
   },
   "outputs": [],
   "source": [
    "# Read the fine-tuning validation CSV into a Pandas DataFrame\n",
    "validation = pd.read_csv(\"/content/finetuning_validation.csv\")\n",
    "\n",
    "# Convert the DataFrame into a dictionary of lists (required for Hugging Face Dataset)\n",
    "validation_dict = validation.to_dict(orient='list')\n",
    "\n",
    "# Create a Hugging Face Dataset from the dictionary\n",
    "validation_dataset = Dataset.from_dict(validation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d08975faef354735a691d3bc98894693",
      "9d50bfb3ceab4623875a8dc2045931f8",
      "b3b3980bff4a47ecb25f6da45fbbf6fc",
      "b00986e06e0e46e99419cc8db7de72e1",
      "f2daf4a9808d4746a8c9fb774cf3baea",
      "bc9aae332bad47faae3947721f8120d2",
      "cbf6ccaa7ed2469e92323f82ffed9af3",
      "60a9f2753a4c447fa1cedc9d43ff5b90",
      "c0337deda64f43eb9eef1c600a5e4270",
      "a28e4972fad143c09cc7618bc543f696",
      "c7746273b7624ecf8a8782eaf21582b8"
     ]
    },
    "id": "U2wTZleC_e2a",
    "outputId": "35e56e4f-5519-4951-ebe6-132e0dc97743"
   },
   "outputs": [],
   "source": [
    "# Apply the prompt_formatter function to each example in the validation dataset\n",
    "# This formats dialogues and summaries into prompts suitable for model evaluation\n",
    "formatted_validation_dataset = validation_dataset.map(\n",
    "    prompt_formatter,\n",
    "    fn_kwargs={'prompt_template': alpaca_prompt}  # Pass the Alpaca-style prompt template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCmKWuZkBzzR"
   },
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYRXt6WZBzlO"
   },
   "source": [
    "We now patch in the adapter modules to the base model using the `get_peft_model` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Goth8Q4O4dCJ"
   },
   "source": [
    "We are adapting the large language model for our task using a technique called **LoRA (Low-Rank Adaptation)**. Instead of retraining the entire model (which would be very expensive), LoRA only updates a small number of parameters while keeping most of the model frozen.\n",
    "\n",
    "\n",
    "* **`r`** - Rank of low-rank matrices; higher = more adaptation, typical 4-64.\n",
    "* **`lora_alpha`** - Scaling factor for LoRA updates; higher = stronger effect, typical 8-32.\n",
    "* **`lora_dropout`** - Dropout on LoRA layers to prevent overfitting, 0-0.3.\n",
    "* **`target_modules`** - The specific parts of the model we allow to be updated.\n",
    "* **`use_gradient_checkpointing`** - Save memory by recomputing activations, `True`/`False`.\n",
    "* **`random_state`** - Seed for reproducibility, any integer.\n",
    "\n",
    "This step makes the model **lighter, faster, and cheaper to fine-tune**, while still learning how to summarize dialogues effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ja43AfZ3I8Nb"
   },
   "source": [
    "For more information, please refer to the [Unsloth](https://github.com/unslothai/unsloth) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QAbRmouJgpo"
   },
   "source": [
    "**NOTE:** This is a LoRA model because we are only applying low-rank adapters on top of the frozen model weights. Although the base model is loaded in 4-bit precision, we are not using QLoRA’s specific quantization (NF4 + double quantization) or gradient handling required for QLoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnIsofeq_e0B",
    "outputId": "a5aefd10-bd5b-4327-de72-1b2b93f43dd1"
   },
   "outputs": [],
   "source": [
    "# Convert the base model into a LoRA (adapter) fine-tunable model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,                         # Rank of the LoRA update matrices\n",
    "    lora_alpha=16,                # Scaling factor for LoRA updates   #  `r=16` and `lora_alpha=16` provide a balanced trade-off, giving enough capacity\n",
    "                                                                      #    to learn task-specific patterns while keeping updates stable and efficient.\n",
    "    lora_dropout=0,               # Dropout rate for LoRA layers (set 0 for fast patching support)\n",
    "    bias=\"none\",                  # How biases are handled (none = leave them unchanged)\n",
    "    target_modules=[              # Model layers where LoRA adapters will be applied\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    use_gradient_checkpointing=True,  # Saves memory during training by recomputing activations\n",
    "    random_state=42                  # For reproducibility\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g05VpvboB9o7"
   },
   "source": [
    "The **architecture** of the Mistral model, specifically the MistralForCausalLM, consists of several key components:\n",
    "\n",
    "1) Embedding Layer: The model starts with an embedding layer that converts input tokens into a dense representation with an output size of 4096, supporting a vocabulary of 32,000 tokens.\n",
    "\n",
    "2) Decoder Layers: The core of the model comprises 32 MistralDecoderLayer instances, each containing:\n",
    "- Self-Attention Mechanism: This includes multiple projection layers for queries,\n",
    "keys, values, and output, all designed to handle 4-bit precision for efficient computation. Rotary embeddings are also employed for position encoding.\n",
    "- Feedforward Network (MLP): The MLP features gates and projections to expand the dimensionality to 14,336 before reducing it back to 4096, using the SiLU activation function.\n",
    "- Layer Normalization: Each decoder layer includes input and post-attention normalization using MistralRMSNorm.\n",
    "\n",
    "3) Final Normalization: The entire model concludes with an additional normalization layer.\n",
    "\n",
    "4) Linear Output Head: The model includes a linear layer that maps the 4096-dimensional output back to the token vocabulary size (32,000), enabling the generation of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCS6BYy-DkTw"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Da6_rFs77G78"
   },
   "source": [
    "Notice how LoRA adapters are attached to the layers specified during instantiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tB5eZxDp88t"
   },
   "source": [
    "```\n",
    "PeftModelForCausalLM(\n",
    "  (base_model): LoraModel(\n",
    "    (model): MistralForCausalLM(\n",
    "      (model): MistralModel(\n",
    "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
    "        (layers): ModuleList(\n",
    "          (0-31): 32 x MistralDecoderLayer(\n",
    "            (self_attn): MistralAttention(\n",
    "              (q_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
    "                zzz(lora_dropout): ModuleDict(\n",
    "                  (default): Identity()\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (k_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Identity()\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (v_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Identity()\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (o_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Identity()\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (rotary_emb): LlamaRotaryEmbedding()\n",
    "            )\n",
    "            (mlp): MistralMLP(\n",
    "              (gate_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Identity()\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (up_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Identity()\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (down_proj): lora.Linear4bit(\n",
    "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
    "                (lora_dropout): ModuleDict(\n",
    "                  (default): Identity()\n",
    "                )\n",
    "                (lora_A): ModuleDict(\n",
    "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
    "                )\n",
    "                (lora_B): ModuleDict(\n",
    "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
    "                )\n",
    "                (lora_embedding_A): ParameterDict()\n",
    "                (lora_embedding_B): ParameterDict()\n",
    "                (lora_magnitude_vector): ModuleDict()\n",
    "              )\n",
    "              (act_fn): SiLU()\n",
    "            )\n",
    "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
    "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
    "          )\n",
    "        )\n",
    "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
    "        (rotary_emb): LlamaRotaryEmbedding()\n",
    "      )\n",
    "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOLX05I97IzR"
   },
   "source": [
    "For training, we use the following nuances borrowed from the broader deep learning discipline.\n",
    "\n",
    "- Low learning rates for smooth parameter updates\n",
    "- Early stopping to monitor for validation loss (negative log likelihood in this case)\n",
    "- Checkpointing to enable resumption of training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMGd9Kdc5B-a"
   },
   "source": [
    "We are creating a **trainer** that will handle the fine-tuning of our model. The trainer takes care of feeding the data into the model, running the training loop, tracking progress, and saving results.\n",
    "\n",
    "Key points in this setup:\n",
    "\n",
    "* **Model & Tokenizer** - The language model and its tokenizer we are fine-tuning.\n",
    "* **Training & Validation Data** - Split datasets so the model can learn on one set and be tested on another.\n",
    "* **Max Sequence Length (2048)** - How much text the model can read at once.\n",
    "* **Data Collator** - Groups the data into batches in the right format.\n",
    "* **Batch Size & Gradient Accumulation** - Train on small pieces at a time (due to memory limits) and combine updates to act like a larger batch.\n",
    "* **Learning Rate & Optimizer** - Control how fast the model learns and how updates are applied.\n",
    "* **Epochs / Steps** - How long the model trains.\n",
    "* **FP16 / BF16** - Use lower precision for faster and more memory-efficient training.\n",
    "* **Output Directory** - Where trained model checkpoints and logs are saved.\n",
    "\n",
    "\n",
    "This trainer automates the whole training process from sending data into the model to adjusting weights, logging progress, and saving results, making fine-tuning efficient and manageable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EGcJ09z_exb"
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,  # LoRA-adapted model to fine-tune\n",
    "    tokenizer = tokenizer,  # Tokenizer corresponding to the model\n",
    "    train_dataset = formatted_training_dataset,  # Training dataset in prompt-ready format\n",
    "    eval_dataset = formatted_validation_dataset,  # Validation dataset for evaluation\n",
    "    dataset_text_field = \"text\",  # Field in dataset containing the input text\n",
    "    max_seq_length = 2048,  # Maximum sequence length for training\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer),  # Handles batching\n",
    "    dataset_num_proc = 2,  # Number of processes for dataset preprocessing\n",
    "    packing = False,  # Packing short sequences can make training faster (disabled here)\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,  # Batch size per GPU/CPU\n",
    "        gradient_accumulation_steps = 4,  # Accumulate gradients over steps to simulate larger batch\n",
    "        warmup_steps = 5,  # Learning rate warmup steps\n",
    "        max_steps = 30,  # Total training steps (used here for quick demonstration)\n",
    "        learning_rate = 2e-4,  # Learning rate for optimizer\n",
    "        fp16 = not is_bfloat16_supported(),  # Use 16-bit float if bfloat16 not supported\n",
    "        bf16 = is_bfloat16_supported(),  # Use bfloat16 if supported\n",
    "        logging_steps = 1,  # Log metrics every step\n",
    "        optim = \"adamw_8bit\",  # 8-bit AdamW optimizer for memory efficiency\n",
    "        weight_decay = 0.01,  # Regularization to prevent overfitting\n",
    "        lr_scheduler_type = \"linear\",  # Linear learning rate decay\n",
    "        seed = 3407,  # For reproducibility\n",
    "        output_dir = \"outputs\",  # Directory to save checkpoints and outputs\n",
    "        report_to = \"none\"  # No external logging (like WandB)\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xH8cGYJJ_0Wh",
    "outputId": "de4f4eb2-f2c9-442d-c2e5-a1df2adf4225"
   },
   "outputs": [],
   "source": [
    "training_history = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwO05JGo3Egh"
   },
   "source": [
    "## Saving the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C45mg7AjBMTr"
   },
   "source": [
    "\n",
    "We will be saving the **LoRA Parameters** of our fine-tuned model so that we can test/evaluate the model later. Since fine-tuning is an expensive process, it’s best to save these adapter files in case of crashes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9EDRk6Zh-b7"
   },
   "source": [
    "### Setup to enable bash commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyWEau0dneNw"
   },
   "source": [
    "This code ensures that all file names and metadata are encoded in UTF-8, preventing errors when writing model files to disk or Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsoPFnwi_0US"
   },
   "outputs": [],
   "source": [
    "# Setup to ensure Python uses UTF-8 encoding for shell/batch commands\n",
    "import locale\n",
    "\n",
    "# Override the system's preferred encoding to always return \"UTF-8\"\n",
    "def getpreferredencoding():\n",
    "    return \"UTF-8\"\n",
    "\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cmI1i5wm0UT"
   },
   "outputs": [],
   "source": [
    "lora_model_name = \"finetuned_mistral_llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBEche8koNV0"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(lora_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k55cdXfSoXNv"
   },
   "source": [
    "`ls -lh {folder}`\n",
    "\n",
    "* **ls** - Lists files and folders.\n",
    "* **-l** - Shows detailed information like permissions, owner, size, and modification date.\n",
    "* **-h** - Makes file sizes human-readable (KB, MB, GB instead of bytes).\n",
    "* `{folder}` - The folder whose contents you want to see.\n",
    "\n",
    "Shows the **contents and sizes** of a folder in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdMNXKQ5_0SA",
    "outputId": "87e43571-d756-43b1-a001-eaa7a3108ab2"
   },
   "outputs": [],
   "source": [
    "!ls -lh {lora_model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NoPpKOFoVgZ"
   },
   "source": [
    "`cp -r {source} {destination}`\n",
    "\n",
    "* **cp** - Stands for “copy”.\n",
    "* **-r** - Means “recursive”, which allows copying **folders and all their contents** (subfolders and files).\n",
    "* `{source}` - The folder you want to copy.\n",
    "* `{destination}` - Where you want to copy it to.\n",
    "\n",
    "Copies a folder and everything inside it to another location.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjPmezvToOeH",
    "outputId": "bbc47017-e8c7-417f-b8e9-4c5d2a2aef59"
   },
   "outputs": [],
   "source": [
    "# # Comment out this cell if you want to save the model to Google Drive\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# drive_model_path = \"/content/drive/MyDrive/finetuned_mistral_llm\"\n",
    "\n",
    "# !cp -r {lora_model_name} {drive_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Sq_u6Mpq441"
   },
   "source": [
    "# **3. Evaluation of LLM after FineTuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGV4lcz8FHa1"
   },
   "source": [
    "### Loading the Fine-tuned Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXDLid2b0zLu",
    "outputId": "2f1a67e7-9f85-4cd8-d24c-0e625bf22262"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=lora_model_name,                 # Replace the model name with \"drive_model_path\" if you are loading the model from Drive\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fb0DfllzFFmx"
   },
   "source": [
    "### Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8oJluSXGvNu"
   },
   "outputs": [],
   "source": [
    "alpaca_prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Write a concise summary of the following dialogue.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opvw9Yxp1lqR"
   },
   "outputs": [],
   "source": [
    "predicted_summaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiBUpkKHHGI8",
    "outputId": "e02d1d4e-f844-477d-b957-47acbf73f9e7"
   },
   "outputs": [],
   "source": [
    "# Loop through each dialogue in the test set and generate summaries\n",
    "for dialogue in tqdm(test_dialogues):\n",
    "    try:\n",
    "        # Format the dialogue into the Alpaca-style prompt\n",
    "        prompt = alpaca_prompt_template.format(dialogue, '')\n",
    "\n",
    "        # Tokenize the prompt and move to GPU\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Generate model output (summary)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,          # Limit summary length\n",
    "            use_cache=True,              # Reuse past key values for efficiency\n",
    "            temperature=0,               # Deterministic output\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode the generated tokens into text, skipping special tokens\n",
    "        prediction = tokenizer.decode(\n",
    "            outputs[0][inputs.input_ids.shape[-1]:],  # Remove input prompt tokens\n",
    "            skip_special_tokens=True,\n",
    "            cleanup_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "        # Store the generated summary\n",
    "        predicted_summaries.append(prediction)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)  # Log error if generation fails and continue\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WvP1kbtFIcF"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DNcZha8Vb7s",
    "outputId": "1d96c2de-57a3-484f-974c-7c4e82368e44"
   },
   "outputs": [],
   "source": [
    "predicted_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTPGlwJP1llz"
   },
   "outputs": [],
   "source": [
    "# Evaluate the quality of generated summaries using BERTScore\n",
    "score = bert_scorer.compute(\n",
    "    predictions=predicted_summaries,  # Summaries generated by the model\n",
    "    references=test_summaries,        # Ground-truth summaries from the dataset\n",
    "    lang='en',                        # Specify English language\n",
    "    rescale_with_baseline=True        # Normalize scores for easier interpretation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2-CFZ0X1ljj",
    "outputId": "e6d6284b-728d-45a7-bbfc-27011de0a9b7"
   },
   "outputs": [],
   "source": [
    "# Compute the average F1 score across all test examples\n",
    "avg_f1 = sum(score['f1']) / len(score['f1'])\n",
    "avg_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPgLSo07YBo3"
   },
   "source": [
    "**The BERT Score of Finetuned Mistral LLM is 0.53**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_vdbuZ781RI"
   },
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w_XdA-F8z7o"
   },
   "source": [
    "**We observed a significant improvement in the BERTScore after fine-tuning the Mistral model, also an observation can be made on the Predicted Summaries**\n",
    "\n",
    "- Previously, the generated summaries of client interactions were overly verbose and lacked alignment with user preferences and domain-specific needs.\n",
    "- By fine-tuning a language model on task-relevant and insurance-specific communication data, we significantly improved the model's ability to generate concise, actionable, and context-aware summaries.\n",
    "- The fine-tuned model now produces outputs that are not only more relevant and structured but also tailored to user expectations, enhancing sales productivity and ensuring better client engagement in the insurance domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfwgNZeOiRHT"
   },
   "source": [
    "<font size = 6 color=\"#4682B4\"><b> Power Ahead </font>\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Aa1yXUesL7Zy",
    "zFN8LrodMR7w",
    "qDG_248npouB",
    "IGxmR0XVwcAi",
    "Vp4LNGG30e8w",
    "mHaWozqI0hmW",
    "Km_LXL5DaGBO",
    "U6GqVK_ShZ2m",
    "n6rSZF3Qhdb2",
    "NIqdC93Rhi0g",
    "IwO05JGo3Egh",
    "qGV4lcz8FHa1",
    "Fb0DfllzFFmx",
    "3WvP1kbtFIcF",
    "E_vdbuZ781RI"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03aedb3387ef4182a7eaab849fc21123": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "182044c6a2574c4cbf97810a098484c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22b6ba504e194e1183c963417f128677": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "553208bce9ea4ef69921389d09f807c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8afa630e5bc342e59807f0ea8c493f95",
       "IPY_MODEL_80da8e51ccf64550ba5be2f58b28ac98",
       "IPY_MODEL_c6a40368c70c427da29a8f56181ed5fa"
      ],
      "layout": "IPY_MODEL_862b87382adf4c39b93e8f31e0ee6813"
     }
    },
    "5566162bc00d430b878336c23d24bdbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60a9f2753a4c447fa1cedc9d43ff5b90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6424175902e045d1b30d3fa12ff73f32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fb638164b754e538e38492a9aa08204": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "703d0fe63c26440b9f251f760c9ef4ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fb638164b754e538e38492a9aa08204",
      "max": 1355863,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc394f734d584b7d9315560b21640e5d",
      "value": 1355863
     }
    },
    "70af7c7881c445d79d4ac28cf196c379": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a733c57b43c04e749f6cb10ab3c5c647",
      "placeholder": "​",
      "style": "IPY_MODEL_e452aa326751490485ba8437dadcc361",
      "value": " 50/50 [00:00&lt;00:00, 1186.51 examples/s]"
     }
    },
    "73726c48f256499daf6a7e5ff7c8d05d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df5637d8d8e64c418e906500c4a925e6",
      "placeholder": "​",
      "style": "IPY_MODEL_f2ead2bea789443692b70420e747fab4",
      "value": "Map: 100%"
     }
    },
    "80da8e51ccf64550ba5be2f58b28ac98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f329740b7b6441519c5dea729dd7d1ab",
      "max": 1421700479,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_22b6ba504e194e1183c963417f128677",
      "value": 1421700479
     }
    },
    "82ed1f8f01ba41d58e60732f502e2606": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab6b2c906ff94da5a94201f409434574",
      "placeholder": "​",
      "style": "IPY_MODEL_6424175902e045d1b30d3fa12ff73f32",
      "value": "tokenizer.json: 100%"
     }
    },
    "8595e39cc76842029c15847345922cdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_182044c6a2574c4cbf97810a098484c9",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e196b6f6012c4591bec9956bd1b5607b",
      "value": 50
     }
    },
    "862b87382adf4c39b93e8f31e0ee6813": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8afa630e5bc342e59807f0ea8c493f95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9925280086c420a89726da2b0a3b6d4",
      "placeholder": "​",
      "style": "IPY_MODEL_e45a8ce6c93e4c069050076cfee78f09",
      "value": "model.safetensors: 100%"
     }
    },
    "9d50bfb3ceab4623875a8dc2045931f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc9aae332bad47faae3947721f8120d2",
      "placeholder": "​",
      "style": "IPY_MODEL_cbf6ccaa7ed2469e92323f82ffed9af3",
      "value": "Map: 100%"
     }
    },
    "a28e4972fad143c09cc7618bc543f696": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a547d8c63bb44f88a6522c7186b5e268": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a733c57b43c04e749f6cb10ab3c5c647": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9925280086c420a89726da2b0a3b6d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab6b2c906ff94da5a94201f409434574": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b00986e06e0e46e99419cc8db7de72e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a28e4972fad143c09cc7618bc543f696",
      "placeholder": "​",
      "style": "IPY_MODEL_c7746273b7624ecf8a8782eaf21582b8",
      "value": " 10/10 [00:00&lt;00:00, 210.90 examples/s]"
     }
    },
    "b3b3980bff4a47ecb25f6da45fbbf6fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60a9f2753a4c447fa1cedc9d43ff5b90",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c0337deda64f43eb9eef1c600a5e4270",
      "value": 10
     }
    },
    "bc9aae332bad47faae3947721f8120d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf846beed3634db78681d05271da7c01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0337deda64f43eb9eef1c600a5e4270": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c6a40368c70c427da29a8f56181ed5fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe29dde9a1674024bd7a7fb46f1359f8",
      "placeholder": "​",
      "style": "IPY_MODEL_fe5f10d9bd4d4db18c91be1612d7c4a8",
      "value": " 1.42G/1.42G [00:17&lt;00:00, 152MB/s]"
     }
    },
    "c7746273b7624ecf8a8782eaf21582b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cbf6ccaa7ed2469e92323f82ffed9af3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d08975faef354735a691d3bc98894693": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d50bfb3ceab4623875a8dc2045931f8",
       "IPY_MODEL_b3b3980bff4a47ecb25f6da45fbbf6fc",
       "IPY_MODEL_b00986e06e0e46e99419cc8db7de72e1"
      ],
      "layout": "IPY_MODEL_f2daf4a9808d4746a8c9fb774cf3baea"
     }
    },
    "df5637d8d8e64c418e906500c4a925e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e196b6f6012c4591bec9956bd1b5607b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e2021eea82144517b2a89d67402f6296": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_73726c48f256499daf6a7e5ff7c8d05d",
       "IPY_MODEL_8595e39cc76842029c15847345922cdc",
       "IPY_MODEL_70af7c7881c445d79d4ac28cf196c379"
      ],
      "layout": "IPY_MODEL_bf846beed3634db78681d05271da7c01"
     }
    },
    "e452aa326751490485ba8437dadcc361": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e45a8ce6c93e4c069050076cfee78f09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0af8c1d89704bd28d739a274869ad65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5566162bc00d430b878336c23d24bdbf",
      "placeholder": "​",
      "style": "IPY_MODEL_a547d8c63bb44f88a6522c7186b5e268",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 11.0MB/s]"
     }
    },
    "f2daf4a9808d4746a8c9fb774cf3baea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2ead2bea789443692b70420e747fab4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f329740b7b6441519c5dea729dd7d1ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f483532192744e2c9a4eefb2567b02b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_82ed1f8f01ba41d58e60732f502e2606",
       "IPY_MODEL_703d0fe63c26440b9f251f760c9ef4ab",
       "IPY_MODEL_f0af8c1d89704bd28d739a274869ad65"
      ],
      "layout": "IPY_MODEL_03aedb3387ef4182a7eaab849fc21123"
     }
    },
    "fc394f734d584b7d9315560b21640e5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe29dde9a1674024bd7a7fb46f1359f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe5f10d9bd4d4db18c91be1612d7c4a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
