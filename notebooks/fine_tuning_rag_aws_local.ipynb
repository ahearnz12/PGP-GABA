{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building, Training, and Testing a Local Model with Fine-Tuning and RAG in AWS Local Mode\n",
    "\n",
    "This notebook guides you through all steps needed to build, train, and test a language model using Parameter-Efficient Fine-Tuning (PEFT) with LoRA and Retrieval-Augmented Generation (RAG) in AWS local mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Up AWS Local Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AWS CLI\n",
    "!pip install awscli\n",
    "\n",
    "# Install SageMaker SDK\n",
    "!pip install sagemaker\n",
    "\n",
    "# Install additional dependencies\n",
    "!pip install torch transformers datasets peft faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure AWS credentials if not already done. You can run this in a terminal:\n",
    "```bash\n",
    "aws configure\n",
    "# Enter your AWS Access Key ID, Secret Access Key, region (e.g., us-east-1), and output format (json)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Your Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained model (e.g., a smaller model for local testing)\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # Or any other model you have access to\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up PEFT with LoRA for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank\n",
    "    lora_alpha=32,  # Alpha parameter for LoRA scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Target attention modules\n",
    "    lora_dropout=0.05,  # Dropout probability for LoRA layers\n",
    "    bias=\"none\",  # Bias type\n",
    "    task_type=\"CAUSAL_LM\"  # Task type\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters info\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset (example using a public dataset)\n",
    "# Replace with your dataset or use a sample dataset like this:\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")\n",
    "\n",
    "# Display sample data\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"quote\"],  # Adjust field name based on your dataset\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "print(f\"Dataset size: {len(tokenized_dataset['train'])} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Up RAG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages for RAG\n",
    "!pip install langchain chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "import os\n",
    "\n",
    "# Create a sample documents directory and files for demonstration\n",
    "os.makedirs(\"./sample_documents\", exist_ok=True)\n",
    "\n",
    "# Create a sample document\n",
    "with open(\"./sample_documents/sample1.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows you to fine-tune large language models \n",
    "    with significantly fewer resources. LoRA (Low-Rank Adaptation) is one of the most popular PEFT methods. \n",
    "    It works by freezing the original model weights and injecting trainable adapter layers.\"\"\")\n",
    "\n",
    "with open(\"./sample_documents/sample2.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"Retrieval-Augmented Generation (RAG) combines retrieval mechanisms with text generation. \n",
    "    It enhances the knowledge of language models by retrieving relevant information from external sources \n",
    "    before generating a response. This helps with factual accuracy and reduces hallucinations.\"\"\")\n",
    "\n",
    "# Load your knowledge base documents\n",
    "loader = DirectoryLoader('./sample_documents/', glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\"Vector store created and persisted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure SageMaker for Local Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import os\n",
    "\n",
    "# Create scripts directory if it doesn't exist\n",
    "os.makedirs(\"./scripts\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/train.py\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_from_disk\n",
    "\n",
    "def main():\n",
    "    # Load model and tokenizer\n",
    "    model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    # Prepare model for training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_from_disk(\"/opt/ml/input/data/training\")\n",
    "    \n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/opt/ml/model\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        warmup_steps=100,\n",
    "    )\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare and Save Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your processed dataset to disk\n",
    "tokenized_dataset.save_to_disk(\"./processed_dataset\")\n",
    "print(\"Dataset saved to ./processed_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize SageMaker and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.LocalSession()\n",
    "role = \"arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole\"  # Dummy role for local mode\n",
    "\n",
    "# Define the HuggingFace estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',  # Your training script\n",
    "    source_dir='./scripts',  # Directory containing your scripts\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='local',  # Use local mode\n",
    "    transformers_version='4.28.1',\n",
    "    pytorch_version='2.0.0',\n",
    "    py_version='py310',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training job\n",
    "huggingface_estimator.fit({\n",
    "    'training': 'file://' + os.path.abspath('./processed_dataset')\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create RAG Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile rag_inference.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load base model and tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load fine-tuned LoRA weights\n",
    "model_path = \"./model_output\"\n",
    "model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "# Load vector store\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_model)\n",
    "\n",
    "# Define RAG prompt template\n",
    "template = \"\"\"\n",
    "Answer the question based on the following context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "def generate_response(question, k=3):\n",
    "    # Retrieve relevant documents\n",
    "    docs = vectorstore.similarity_search(question, k=k)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Format prompt with retrieved context\n",
    "    formatted_prompt = prompt.format(context=context, question=question)\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the answer part (after the prompt)\n",
    "    answer = response.split(\"Answer:\")[-1].strip()\n",
    "    \n",
    "    return answer, docs\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is the capital of France?\"\n",
    "    answer, retrieved_docs = generate_response(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"\\nRetrieved Documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"Document {i+1}:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Your Model with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the inference module\n",
    "from rag_inference import generate_response\n",
    "\n",
    "# Test with sample questions\n",
    "questions = [\n",
    "    \"What is PEFT?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"What are the advantages of LoRA?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer, retrieved_docs = generate_response(question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"Document {i+1}: {doc.page_content[:100]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install evaluation packages\n",
    "!pip install evaluate rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from rag_inference import generate_response\n",
    "\n",
    "# Load evaluation dataset (example)\n",
    "eval_questions = [\n",
    "    {\"question\": \"What is PEFT?\", \"reference\": \"Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows you to fine-tune large language models with significantly fewer resources.\"},\n",
    "    {\"question\": \"How does RAG work?\", \"reference\": \"Retrieval-Augmented Generation (RAG) combines retrieval mechanisms with text generation. It enhances the knowledge of language models by retrieving relevant information from external sources.\"},\n",
    "    # Add more evaluation examples\n",
    "]\n",
    "\n",
    "# Initialize metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "# Evaluate\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in eval_questions:\n",
    "    question = example[\"question\"]\n",
    "    reference = example[\"reference\"]\n",
    "    \n",
    "    prediction, _ = generate_response(question)\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    references.append(reference)\n",
    "\n",
    "# Calculate metrics\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "exact_match_results = exact_match.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"ROUGE Results:\")\n",
    "for metric, score in rouge_results.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")\n",
    "    \n",
    "print(\"\\nExact Match Results:\")\n",
    "print(f\"  Score: {exact_match_results['exact_match']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Deploy Your Model Locally (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Create a HuggingFace Model\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data='file://' + os.path.abspath('./model_output'),\n",
    "    role=role,\n",
    "    transformers_version='4.28.1',\n",
    "    pytorch_version='2.0.0',\n",
    "    py_version='py310',\n",
    ")\n",
    "\n",
    "# Deploy the model locally\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='local'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the deployed model\n",
    "response = predictor.predict({\n",
    "    \"inputs\": \"What is the capital of France?\"\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Tips\n",
    "\n",
    "1. **Memory Management**: For large models, consider using techniques like:\n",
    "   - Gradient checkpointing\n",
    "   - Mixed precision training (fp16)\n",
    "   - DeepSpeed or other optimization libraries\n",
    "\n",
    "2. **Data Quality**: The quality of your RAG system heavily depends on your knowledge base. Ensure your documents are:\n",
    "   - Relevant to your domain\n",
    "   - Well-structured\n",
    "   - Properly chunked (not too large, not too small)\n",
    "\n",
    "3. **Hyperparameter Tuning**: Experiment with different values for:\n",
    "   - LoRA rank (r)\n",
    "   - Learning rate\n",
    "   - Number of training epochs\n",
    "   - Retrieval parameters (k, chunk size)\n",
    "\n",
    "4. **Monitoring**: Track training metrics to detect issues early:\n",
    "   - Loss curves\n",
    "   - GPU memory usage\n",
    "   - Training speed\n",
    "\n",
    "5. **Troubleshooting AWS Local Mode**:\n",
    "   - Ensure Docker is installed and running\n",
    "   - Check for sufficient disk space\n",
    "   - Monitor resource usage during training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
