{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtYI348gU7bH"
      },
      "source": [
        "<center><font size=6>Hands-on: Text Generation with Transformers</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdlO85FaVDai"
      },
      "source": [
        "# **Installing and Importing Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L5KMhwb44eMl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# installing the transformers library\n",
        "%pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z4ESP52j4gfB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alexanderhearnz/codebase/PGP-GABA/jupyter_env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# to load transformer models\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# to load a Deep Learning library\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndYbkuHuVpi3"
      },
      "source": [
        "# **Generating Text with Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u9ft_QW6Ddu"
      },
      "source": [
        "## **Step 1:** Load the pre-trained model and tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHJ_3nJtWRVl"
      },
      "source": [
        "First, we load the pre-trained model and its tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMwAef5aWsul"
      },
      "source": [
        "We'll be using the **Google FLAN-T5** model here.\n",
        "\n",
        "üí° **FLAN-T5, developed by Google Research**, is a **\"Fine-tuned LAnguage Net\" (FLAN) with \"Text-To-Text Transfer Transformer\" (T-5)** architecture.\n",
        "\n",
        "üìä **FLAN-T5 excels in various NLP tasks**, including translation and question answering, and it's known for its speed and efficiency.\n",
        "\n",
        "üìã **FLAN-T5 comes in different sizes** - small, base, large, XL, and XXL - offering customization options.\n",
        "\n",
        "üõ†Ô∏è Potential use cases include text generation, classification, summarization, sentiment analysis, question-answering, translation, and chatbots.\n",
        "\n",
        "FLAN-T5 is an Encoder-Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWcwL83W6BvZ",
        "outputId": "169bd7bd-c384-4bfe-89d1-664919e4c96e"
      },
      "outputs": [],
      "source": [
        "# defining the model name\n",
        "model_name = \"google/flan-t5-large\"\n",
        "\n",
        "# The 'from_pretrained' method allows us to load a pre-trained tokenizer from Hugging Face's model hub.\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the pre-trained model for text generation\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvoT-DmYWFqH"
      },
      "source": [
        "- **`T5Tokenizer`**: Converts text into numbers that the model can understand.\n",
        "- **`T5ForConditionalGeneration`**: Loads the pretrained model for generating text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouefCNfJ57Ag"
      },
      "source": [
        "## **Step 2:** Define the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1m2_43wXYLJ"
      },
      "source": [
        "Next, we define the input for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ts3v_PE64jhM"
      },
      "outputs": [],
      "source": [
        "input_text = \"List the steps to prepare lasagna.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ1lS7-P6YRg"
      },
      "source": [
        "## **Step 3:** Tokenize the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quSS4hEtYA7l"
      },
      "source": [
        "Next, we tokenize the input to make it ready to be passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CRCmenC5YPUm"
      },
      "outputs": [],
      "source": [
        "# tokenizing the input\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnorJ08W8ZkB"
      },
      "source": [
        "- **`tokenizer.encode()`**: It converts human-readable text into numeric tokens that the model understands. The tokens represent pieces of words or entire words ‚Äî it's how the model processes language.\n",
        "    - **`input_question`**: This is the text input for the model.\n",
        "    - **`return_tensors='pt'`**: This specifies that the output is to be formatted as a PyTorch tensor (`'pt'` stands for PyTorch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVQt9vm-YSL5"
      },
      "source": [
        "Let's check the output of tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx_LfGoo4jdz",
        "outputId": "c3011116-3a62-4a41-9751-d534f5f7e289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: tensor([[ 6792,     8,  2245,    12,  2967, 12031, 11260,     5,     1]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\", input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxbYGSLXYc3A"
      },
      "source": [
        "- As we can see, the input text is broken down into a list of tokens, and the IDs of each token are displayed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU5GuD8D6n4Y"
      },
      "source": [
        "## **Step 4:** Pass tokenized input to the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAW9kxc7ZJFy"
      },
      "source": [
        "Next, we pass the tokenized input to the model for generating outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Su2JMXUGZPgV"
      },
      "outputs": [],
      "source": [
        "# generating model output\n",
        "output_tokens = model.generate(input_ids, max_length=256, num_return_sequences=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XtpQVmd_G4M"
      },
      "source": [
        "- **`input_ids`**\n",
        "  - Tokenized input text\n",
        "\n",
        "- **`max_length=256`**\n",
        "  - Sets the maximum length (in tokens) of the generated output\n",
        "  - This number includes ***both*** the input and output tokens\n",
        "  - Range of the values depends on the required output size and the model's preset limits\n",
        "\n",
        "- **`num_return_sequences=1`**\n",
        "  - Tells the model how many different outputs to generate for the same input.\n",
        "  - Can be set to more than 1 if multiple variations of the output are required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTbpi87mbshx"
      },
      "source": [
        "Let's check the output from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXhgA1Ku4ja8",
        "outputId": "136466bb-8aa1-43bc-9fcf-6d9872513217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output Tokens: tensor([[    0,   304,  2967, 12031, 11260,     6,    25,   166,   174,    12,\n",
            "          2967,     8, 12031, 11260,  3837,     5,    86,     3,     9,   508,\n",
            "         22869,     6,  1678,     8,  7994,  1043,   147,  2768,  1678,     5,\n",
            "          2334,     8, 12031, 11260, 22200,    11,  3989,   552,    79,    33,\n",
            "           491,   177,    17,    15,     6,    81,   305,   676,     5,  2334,\n",
            "             8,  3285,    11,  3989,   552,    34,    19,     3, 19293,     6,\n",
            "            81,   204,   676,     5,  2334,     8,  3837,    11,  3989,   552,\n",
            "            34,    19,  4126,  4632,     6,    81,   204,   676,     5,  2334,\n",
            "             8,   260,  2687,   152,  3285,    11,  3989,   552,    34,    19,\n",
            "             3, 19293,     6,    81,   204,   676,     5,  2334,     8,   260,\n",
            "          8887,    11,  3989,   552,    34,    19,     3,   210,   173,  1054,\n",
            "             6,    81,   204,   676,     5,  2334,     8,     3,    52,    23,\n",
            "         10405,     9,    11,  3989,   552,    34,    19,     3, 19293,     6,\n",
            "            81,   204,   676,     5,  2334,     8,     3,    52,    23, 10405,\n",
            "             9,    11,  3989,   552,    34,    19,     3, 19293,     6,    81,\n",
            "           204,   676,     5,  2334,     8,   260,  2687,   152,  3285,    11,\n",
            "          3989,   552,    34,    19,     3, 19293,     6,    81,   204,   676,\n",
            "             5,  2334,     8,   260,  8887,    11,  3989,   552,    34,    19,\n",
            "             3, 19293,     6,    81,   204,   676,     5,  2334,     8,   260,\n",
            "          8887,    11,  3989,   552,    34,    19,     3, 19293,     6,    81,\n",
            "           204,   676,     5,  2334,     8,   260,  8887,    11,  3989,   552,\n",
            "            34,    19,     3, 19293,     6,    81,   204,   676,     5,  2334,\n",
            "             8,   260,  8887,    11,  3989,   552,    34,    19,     3, 19293,\n",
            "             6,    81,   204,   676,     5,  2334,     8,   260,  8887,    11,\n",
            "          3989,   552,    34,    19,     3, 19293,     6,    81,   204,   676,\n",
            "             5,     1]])\n"
          ]
        }
      ],
      "source": [
        "print(\"Output Tokens:\", output_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLn7w2AEbwcl"
      },
      "source": [
        "- As we can see, the model output is a list of token IDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GXLzGBs611p"
      },
      "source": [
        "## **Step 5:** Decode the generated output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbEoTuWGb52-"
      },
      "source": [
        "Finally, we decode the model's output token IDs to obtain a human-readable text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "afYawooXcGm3"
      },
      "outputs": [],
      "source": [
        "# decode the generated output from token IDs to human-readable text\n",
        "# 'skip_special_tokens=True' ensures that special tokens are omitted from the decoded output\n",
        "generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBQbIk3ecM2n",
        "outputId": "433c1272-5e96-43fd-814c-06cf019bb2c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text:  To prepare lasagna, you first need to prepare the lasagna sauce. In a large skillet, heat the olive oil over medium heat. Add the lasagna noodles and cook until they are al dente, about 5 minutes. Add the cheese and cook until it is melted, about 2 minutes. Add the sauce and cook until it is thickened, about 2 minutes. Add the parmesan cheese and cook until it is melted, about 2 minutes. Add the parsley and cook until it is wilted, about 2 minutes. Add the ricotta and cook until it is melted, about 2 minutes. Add the ricotta and cook until it is melted, about 2 minutes. Add the parmesan cheese and cook until it is melted, about 2 minutes. Add the parsley and cook until it is melted, about 2 minutes. Add the parsley and cook until it is melted, about 2 minutes. Add the parsley and cook until it is melted, about 2 minutes. Add the parsley and cook until it is melted, about 2 minutes. Add the parsley and cook until it is melted, about 2 minutes.\n"
          ]
        }
      ],
      "source": [
        "print(\"Generated Text: \", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGIqjji4c21w"
      },
      "source": [
        "- As we can see, the model has listed out the steps involved in preparing the specified dish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qICjLkV9o9Hc"
      },
      "source": [
        "**Note**: Given the probabilistic nature of transformers, the output might vary with multiple executions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMnMHfIxBuSb"
      },
      "source": [
        "## **Stitching Everything Together**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s92OMrgnXStU"
      },
      "source": [
        "To make the process reusable and efficient, we encapsulate the entire sequence of operations above into a function. This function will\n",
        "\n",
        "1. take an input text\n",
        "2. perform necessary processing, and\n",
        "3. output a human-readable response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uoDCSxeh7eiT"
      },
      "outputs": [],
      "source": [
        "def generate_response(input, max_length=256, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Generates a text response to a given question using a language model.\n",
        "\n",
        "    Parameters:\n",
        "    - question (str): The input text for the model.\n",
        "    - max_length (int): Maximum length of the model's generated response, including the input text.\n",
        "    - num_return_sequences (int): Number of different responses to generate for the same input.\n",
        "\n",
        "    Returns:\n",
        "    - str: The generated text response.\n",
        "    \"\"\"\n",
        "    # checking for GPU availability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Encode the input question into token IDs\n",
        "    input_ids = tokenizer(input, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "    # Generate the output tokens from the model\n",
        "    output_tokens = model.generate(input_ids, max_length=max_length, num_return_sequences=num_return_sequences)\n",
        "\n",
        "    # Decode the generated tokens into human-readable text\n",
        "    responses = []\n",
        "\n",
        "    # Iterate over all responses generated, decode them one at a time, and then add them to the response list\n",
        "    for generated_sequence in output_tokens:\n",
        "        generated_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
        "        responses.append(generated_text)\n",
        "\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttr-RXOeepTP"
      },
      "source": [
        "Let's test our function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u8hTEYAesTq",
        "outputId": "fdb12b95-c1bc-4216-85d6-eb7310e3c48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: List the steps to prepare lasagna.\n",
            "Response: To prepare lasagna, you first need to prepare the lasagna sauce. In a large skillet, heat the olive oil over medium heat. Add the lasagna noodles and cook until they are al dente, about 5 minutes. Add the cheese and cook until it is melted, about 2 minutes. Add the sauce and cook until it is thickened, about 2 minutes. Add the parmesan cheese and cook until it is melted, about 2 minutes. Add the parsley and cook until it is wilted, about 2 minutes. Add the ricotta and cook until it is melted, about 2 minutes. Add the ricotta and cook until it is melted, about 2 minutes. Add the parmesan cheese and cook until it is melted, about 2 minutes. Add the parsley and cook until it is melted, about 2 minutes. Add the parsley and cook until it is melted, about 2 minutes. Add the parsley and cook until it is melted, about 2 minutes. Add the parsley and cook until it is melted, about 2 minutes. Add the parsley and cook until it is melted, about 2 minutes.\n"
          ]
        }
      ],
      "source": [
        "input_text = \"List the steps to prepare lasagna.\"\n",
        "response = generate_response(input_text, max_length=300)\n",
        "\n",
        "print(f\"Question: {input_text}\")\n",
        "print(f\"Response: {response[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tazAGASRfXyy"
      },
      "source": [
        "# **More Examples**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CarnK_SDMDf"
      },
      "source": [
        "**Let's take a look at a few more examples.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDvmYpK77ee0",
        "outputId": "2da36c38-95df-4fb5-d563-6ba7e74830e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the capital of France?\n",
            "Response: paris\n"
          ]
        }
      ],
      "source": [
        "input_text = \"What is the capital of France?\"\n",
        "response = generate_response(input_text)\n",
        "\n",
        "print(f\"Question: {input_text}\")\n",
        "print(f\"Response: {response[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_rQEORqjmSX"
      },
      "source": [
        "- The model provided a crisp and correct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPKjmZ_QjsMk",
        "outputId": "bccca7e5-91ea-4dc9-fc5f-5561d0d2dccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Provide a brief overview of NLP.\n",
            "Response: NLP is a branch of computer science that uses machine learning to improve the performance of human decision-making processes.\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Provide a brief overview of NLP.\"\n",
        "response = generate_response(input_text)\n",
        "\n",
        "print(f\"Question: {input_text}\")\n",
        "print(f\"Response: {response[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE_W-8OZjww1"
      },
      "source": [
        "- Again, the model output is crisp and to the point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-IAr8_77eb9",
        "outputId": "8677bb6a-e32e-4239-f4ba-30bf2432ab69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Provide a brief overview of attention.\n",
            "Response: Attention is the ability of a person to focus attention on a particular stimulus.\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Provide a brief overview of attention.\"\n",
        "response = generate_response(input_text)\n",
        "\n",
        "print(f\"Question: {input_text}\")\n",
        "print(f\"Response: {response[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j5CLBeQkQ2Y"
      },
      "source": [
        "- The model's output is factually correct.\n",
        "- However, the generated text is not related to the attention mechanism in transformers.\n",
        "- This is because ***the input didn't specify*** that the output should be in the context of transformers.\n",
        "    - The model can generate text, but it ***doesn't know what we're expecting from it implicitly***.\n",
        "- We need to be ***clear and specific with the input***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNUU-3OHgMNR",
        "outputId": "69199d13-ea87-498a-876b-e37d036ee3d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Provide a brief overview of attention in the context of the transformer neural network architecture.\n",
            "Response: Attention is the ability to focus attention on a particular stimulus or object. The Transformer architecture uses a transformer architecture to achieve this goal.\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Provide a brief overview of attention in the context of the transformer neural network architecture.\"\n",
        "response = generate_response(input_text)\n",
        "\n",
        "print(f\"Question: {input_text}\")\n",
        "print(f\"Response: {response[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ90NvPyl6vM"
      },
      "source": [
        "- While the output provides a surface-level overview of the attention mechanism in transformers to some extent, it also contains parts that are unclear/incorrect.\n",
        "- This likely occurred because the ***model wasn't trained on data that the current input is based on***.\n",
        "- We would need to ***provide more context to the model***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS1kecci7eUF",
        "outputId": "793adbb7-c1a6-404c-9219-83f5df075efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: \n",
            "The attention mechanism in transformers is a technique that enables models to assign weights to the importance of different tokens in a sequence when embedding or generating text.\n",
            "Instead of processing the input sequentially one token at a time, attention allows the model to consider all tokens simultaneously by focusing on relevant parts of the input.\n",
            "It computes attention scores based on the relationships between tokens, which helps identify contextual dependencies. This results in an improved understanding of language context and meaning.\n",
            "Self-attention assesses connections within the input sequence and multi-head attention allows the model to capture diverse relationships by using multiple attention heads simultaneously.\n",
            "\n",
            "Based on the above information, provide a brief overview of attention in the context of the transformer neural network architecture.\n",
            "\n",
            "Response: The attention mechanism in transformers is a technique that enables models to assign weights to the importance of different tokens in a sequence when embedding or generating text. Instead of processing the input sequentially one token at a time, attention allows the model to consider all tokens simultaneously by focusing on relevant parts of the input. It computes attention scores based on the relationships between tokens, which helps identify contextual dependencies. This results in an improved understanding of language context and meaning.\n"
          ]
        }
      ],
      "source": [
        "input_text = \"\"\"\n",
        "The attention mechanism in transformers is a technique that enables models to assign weights to the importance of different tokens in a sequence when embedding or generating text.\n",
        "Instead of processing the input sequentially one token at a time, attention allows the model to consider all tokens simultaneously by focusing on relevant parts of the input.\n",
        "It computes attention scores based on the relationships between tokens, which helps identify contextual dependencies. This results in an improved understanding of language context and meaning.\n",
        "Self-attention assesses connections within the input sequence and multi-head attention allows the model to capture diverse relationships by using multiple attention heads simultaneously.\n",
        "\n",
        "Based on the above information, provide a brief overview of attention in the context of the transformer neural network architecture.\n",
        "\"\"\"\n",
        "response = generate_response(input_text)\n",
        "\n",
        "print(f\"Question: {input_text}\")\n",
        "print(f\"Response: {response[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4_miDC_oVYs"
      },
      "source": [
        "- Given more context, the model was able to generate a more accurate and desired output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgXxUEtPhij0"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlwBHS5Ahk-3"
      },
      "source": [
        "We learned how to utilize transformers to generate text.\n",
        "- An input text is first tokenized and then passed to the model.\n",
        "- The model's generated output is then decoded to obtain a human-readable output.\n",
        "- Model parameters like `max_length` allow us to specify the size of the generated output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrgZZkYhBDvS"
      },
      "source": [
        "**Remember!**\n",
        "\n",
        "While Generative AI models can help in generating text or asking questions, it's important to ask the question in the right manner and provide the right context for the model to build upon.\n",
        "\n",
        "In upcoming weeks, we'll learn more about:\n",
        "\n",
        "1. How to engineer the input to a language model to obtain outputs aligned with business goals?\n",
        "2. How to provide relevant context to a language model via documents to generate context-relevant output?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J49-X2WTBWAR"
      },
      "source": [
        "<font size=6 color='navyblue'>Power Ahead!</font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OdlO85FaVDai",
        "ndYbkuHuVpi3",
        "0u9ft_QW6Ddu",
        "ouefCNfJ57Ag",
        "dQ1lS7-P6YRg",
        "XU5GuD8D6n4Y",
        "9GXLzGBs611p",
        "FMnMHfIxBuSb",
        "tazAGASRfXyy",
        "NgXxUEtPhij0"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jupyter_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
